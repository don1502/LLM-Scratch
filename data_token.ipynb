{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba4034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()\n",
    "# data = input(\"Enter your prompt here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8778d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ea2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = re.split(r'([,.<>/!@#$%^&*():;]|--|\\s)',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2558b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This a for loop for separating the text. And below is list comprehension\n",
    "\n",
    "# result = []\n",
    "# for item in text:\n",
    "#     if item.strip():\n",
    "#         result.append(item)\n",
    "    \n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b021d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"The', 'height', 'of', 'his', 'glory\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication', '.', '\"Of', 'course', \"it's\", 'going', 'to', 'send', 'the', 'value', 'of', 'my', 'picture', \"'way\", 'up', ';', 'but', 'I', \"don't\", 'think', 'of', 'that', ',', 'Mr', '.', 'Rickham', '--', 'the', 'loss', 'to', 'Arrt', 'is', 'all', 'I', 'think', 'of', '.', '\"', 'The', 'word', ',', 'on', 'Mrs', '.', \"Thwing's\", 'lips', ',', 'multiplied', 'its', '_rs_', 'as', 'though', 'they', 'were', 'reflected', 'in', 'an', 'endless', 'vista', 'of', 'mirrors', '.', 'And', 'it', 'was', 'not', 'only', 'the', 'Mrs', '.', 'Thwings', 'who', 'mourned', '.', 'Had', 'not', 'the', 'exquisite', 'Hermia', 'Croft', ',', 'at', 'the', 'last', 'Grafton', 'Gallery', 'show', ',', 'stopped', 'me', 'before', \"Gisburn's\", '\"Moon-dancers\"', 'to', 'say', ',', 'with', 'tears', 'in', 'her', 'eyes', ':', '\"We', 'shall', 'not', 'look', 'upon', 'its', 'like', 'again\"?', 'Well', '!', '--', 'even', 'through', 'the', 'prism', 'of', \"Hermia's\", 'tears', 'I', 'felt', 'able', 'to', 'face', 'the', 'fact', 'with', 'equanimity', '.', 'Poor', 'Jack', 'Gisburn', '!', 'The', 'women', 'had', 'made', 'him', '--', 'it', 'was', 'fitting', 'that', 'they', 'should', 'mourn', 'him', '.', 'Among', 'his', 'own', 'sex', 'fewer', 'regrets', 'were', 'heard', ',', 'and', 'in', 'his', 'own', 'trade', 'hardly', 'a', 'murmur', '.', 'Professional', 'jealousy?', 'Perhaps', '.', 'If', 'it', 'were', ',', 'the', 'honour', 'of', 'the', 'craft', 'was', 'vindicated', 'by', 'little', 'Claude', 'Nutley', ',', 'who', ',', 'in', 'all', 'good', 'faith', ',', 'brought', 'out', 'in', 'the', 'Burlington', 'a', 'very', 'handsome', '\"obituary\"', 'on', 'Jack', '--', 'one', 'of', 'those', 'showy', 'articles', 'stocked', 'with', 'random', 'technicalities', 'that', 'I', 'have', 'heard', '(', 'I', \"won't\", 'say', 'by', 'whom', ')', 'compared', 'to', \"Gisburn's\", 'painting', '.', 'And', 'so', '--', 'his', 'resolve', 'being', 'apparently', 'irrevocable', '--', 'the', 'discussion', 'gradually', 'died', 'out', ',', 'and', ',', 'as', 'Mrs', '.', 'Thwing', 'had', 'predicted', ',', 'the', 'price', 'of', '\"Gisburns\"', 'went', 'up', '.', 'It', 'was', 'not', 'till', 'three', 'years', 'later', 'that', ',', 'in', 'the', 'course', 'of', 'a', 'few', \"weeks'\", 'idling', 'on', 'the', 'Riviera', ',', 'it', 'suddenly', 'occurred', 'to', 'me', 'to', 'wonder', 'why', 'Gisburn', 'had', 'given', 'up', 'his', 'painting', '.', 'On', 'reflection', ',', 'it', 'really', 'was', 'a', 'tempting', 'problem', '.', 'To', 'accuse', 'his', 'wife', 'would', 'have', 'been', 'too', 'easy', '--', 'his', 'fair', 'sitters', 'had', 'been', 'denied', 'the', 'solace', 'of', 'saying', 'that', 'Mrs', '.', 'Gisburn', 'had', '\"dragged', 'him', 'down', '.', '\"', 'For', 'Mrs', '.', 'Gisburn', '--', 'as', 'such', '--', 'had', 'not', 'existed', 'till', 'nearly', 'a', 'year', 'after', \"Jack's\", 'resolve', 'had', 'been', 'taken', '.', 'It', 'might', 'be', 'that', 'he', 'had', 'married', 'her', '--', 'since', 'he', 'liked', 'his', 'ease', '--', 'because', 'he', \"didn't\", 'want', 'to', 'go', 'on', 'painting', ';', 'but', 'it', 'would', 'have', 'been', 'hard', 'to', 'prove', 'that', 'he', 'had', 'given', 'up', 'his', 'painting', 'because', 'he', 'had', 'married', 'her', '.', 'Of', 'course', ',', 'if', 'she', 'had', 'not', 'dragged', 'him', 'down', ',', 'she', 'had', 'equally', ',', 'as', 'Miss', 'Croft', 'contended', ',', 'failed', 'to', '\"lift', 'him', 'up\"', '--', 'she', 'had', 'not', 'led', 'him', 'back', 'to', 'the', 'easel', '.', 'To', 'put', 'the', 'brush', 'into', 'his', 'hand', 'again', '--', 'what', 'a', 'vocation', 'for', 'a', 'wife', '!', 'But', 'Mrs', '.', 'Gisburn', 'appeared', 'to', 'have', 'disdained', 'it', '--', 'and', 'I', 'felt', 'it', 'might', 'be', 'interesting', 'to', 'find', 'out', 'why', '.', 'The', 'desultory', 'life', 'of', 'the', 'Riviera', 'lends', 'itself', 'to', 'such', 'purely', 'academic', 'speculations', ';', 'and', 'having', ',', 'on', 'my', 'way', 'to', 'Monte', 'Carlo', ',', 'caught', 'a', 'glimpse', 'of', \"Jack's\", 'balustraded', 'terraces', 'between', 'the', 'pines', ',', 'I', 'had', 'myself', 'borne', 'thither', 'the', 'next', 'day', '.', 'I', 'found', 'the', 'couple', 'at', 'tea', 'beneath', 'their', 'palm-trees', ';', 'and', 'Mrs', '.', \"Gisburn's\", 'welcome', 'was', 'so', 'genial', 'that', ',', 'in', 'the', 'ensuing', 'weeks', ',', 'I', 'claimed', 'it', 'frequently', '.', 'It', 'was', 'not', 'that', 'my', 'hostess', 'was', '\"interesting\"', ':', 'on', 'that', 'point', 'I', 'could', 'have', 'given', 'Miss', 'Croft', 'the', 'fullest', 'reassurance', '.', 'It', 'was', 'just', 'because', 'she', 'was', '_not_', 'interesting', '--', 'if', 'I', 'may', 'be', 'pardoned', 'the', 'bull', '--', 'that', 'I', 'found', 'her', 'so', '.', 'For', 'Jack', ',', 'all', 'his', 'life', ',', 'had', 'been', 'surrounded', 'by', 'interesting', 'women', ':', 'they', 'had', 'fostered', 'his', 'art', ',', 'it', 'had', 'been', 'reared', 'in', 'the', 'hot-house', 'of', 'their', 'adulation', '.', 'And', 'it', 'was', 'therefore', 'instructive', 'to', 'note', 'what', 'effect', 'the', '\"deadening', 'atmosphere', 'of', 'mediocrity\"', '(', 'I', 'quote', 'Miss', 'Croft', ')', 'was', 'having', 'on', 'him', '.', 'I', 'have', 'mentioned', 'that', 'Mrs', '.', 'Gisburn', 'was', 'rich', ';', 'and', 'it', 'was', 'immediately', 'perceptible', 'that', 'her', 'husband', 'was', 'extracting', 'from', 'this', 'circumstance', 'a', 'delicate', 'but', 'substantial', 'satisfaction', '.', 'It', 'is', ',', 'as', 'a', 'rule', ',', 'the', 'people', 'who', 'scorn', 'money', 'who', 'get', 'most', 'out', 'of', 'it', ';', 'and', \"Jack's\", 'elegant', 'disdain', 'of', 'his', \"wife's\", 'big', 'balance', 'enabled', 'him', ',', 'with', 'an', 'appearance', 'of', 'perfect', 'good-breeding', ',', 'to', 'transmute', 'it', 'into', 'objects', 'of', 'art', 'and', 'luxury', '.', 'To', 'the', 'latter', ',', 'I', 'must', 'add', ',', 'he', 'remained', 'relatively', 'indifferent', ';', 'but', 'he', 'was', 'buying', 'Renaissance', 'bronzes', 'and', 'eighteenth-century', 'pictures', 'with', 'a', 'discrimination', 'that', 'bespoke', 'the', 'amplest', 'resources', '.', '\"Money\\'s', 'only', 'excuse', 'is', 'to', 'put', 'beauty', 'into', 'circulation', ',', '\"', 'was', 'one', 'of', 'the', 'axioms', 'he', 'laid', 'down', 'across', 'the', 'Sevres', 'and', 'silver', 'of', 'an', 'exquisitely', 'appointed', 'luncheon-table', ',', 'when', ',', 'on', 'a', 'later', 'day', ',', 'I', 'had', 'again', 'run', 'over', 'from', 'Monte', 'Carlo', ';', 'and', 'Mrs', '.', 'Gisburn', ',', 'beaming', 'on', 'him', ',', 'added', 'for', 'my', 'enlightenment', ':', '\"Jack', 'is', 'so', 'morbidly', 'sensitive', 'to', 'every', 'form', 'of', 'beauty', '.', '\"', 'Poor', 'Jack', '!', 'It', 'had', 'always', 'been', 'his', 'fate', 'to', 'have', 'women', 'say', 'such', 'things', 'of', 'him', ':', 'the', 'fact', 'should', 'be', 'set', 'down', 'in', 'extenuation', '.', 'What', 'struck', 'me', 'now', 'was', 'that', ',', 'for', 'the', 'first', 'time', ',', 'he', 'resented', 'the', 'tone', '.', 'I', 'had', 'seen', 'him', ',', 'so', 'often', ',', 'basking', 'under', 'similar', 'tributes', '--', 'was', 'it', 'the', 'conjugal', 'note', 'that', 'robbed', 'them', 'of', 'their', 'savour?', 'No', '--', 'for', ',', 'oddly', 'enough', ',', 'it', 'became', 'apparent', 'that', 'he', 'was', 'fond', 'of', 'Mrs', '.', 'Gisburn', '--', 'fond', 'enough', 'not', 'to', 'see', 'her', 'absurdity', '.', 'It', 'was', 'his', 'own', 'absurdity', 'he', 'seemed', 'to', 'be', 'wincing', 'under', '--', 'his', 'own', 'attitude', 'as', 'an', 'object', 'for', 'garlands', 'and', 'incense', '.', '\"My', 'dear', ',', 'since', \"I've\", 'chucked', 'painting', 'people', \"don't\", 'say', 'that', 'stuff', 'about', 'me', '--', 'they', 'say', 'it', 'about', 'Victor', 'Grindle', ',', '\"', 'was', 'his', 'only', 'protest', ',', 'as', 'he', 'rose', 'from', 'the', 'table', 'and', 'strolled', 'out', 'onto', 'the', 'sunlit', 'terrace', '.', 'I', 'glanced', 'after', 'him', ',', 'struck', 'by', 'his', 'last', 'word', '.', 'Victor', 'Grindle', 'was', ',', 'in', 'fact', ',', 'becoming', 'the', 'man', 'of', 'the', 'moment', '--', 'as', 'Jack', 'himself', ',', 'one', 'might', 'put', 'it', ',', 'had', 'been', 'the', 'man', 'of', 'the', 'hour', '.', 'The', 'younger', 'artist', 'was', 'said', 'to', 'have', 'formed', 'himself', 'at', 'my', \"friend's\", 'feet', ',', 'and', 'I', 'wondered', 'if', 'a', 'tinge', 'of', 'jealousy', 'underlay', 'the', \"latter's\", 'mysterious', 'abdication', '.', 'But', 'no', '--', 'for', 'it', 'was', 'not', 'till', 'after', 'that', 'event', 'that', 'the', '_rose', 'Dubarry_', 'drawing-rooms', 'had', 'begun', 'to', 'display', 'their', '\"Grindles', '.', '\"', 'I', 'turned', 'to', 'Mrs', '.', 'Gisburn', ',', 'who', 'had', 'lingered', 'to', 'give', 'a', 'lump', 'of', 'sugar', 'to', 'her', 'spaniel', 'in', 'the', 'dining-room', '.', '\"Why', '_has_', 'he', 'chucked', 'painting?\"', 'I', 'asked', 'abruptly', '.', 'She', 'raised', 'her', 'eyebrows', 'with', 'a', 'hint', 'of', 'good-humoured', 'surprise', '.', '\"Oh', ',', 'he', \"doesn't\", '_have_', 'to', 'now', ',', 'you', 'know', ';', 'and', 'I', 'want', 'him', 'to', 'enjoy', 'himself', ',', '\"', 'she', 'said', 'quite', 'simply', '.', 'I', 'looked', 'about', 'the', 'spacious', 'white-panelled', 'room', ',', 'with', 'its', '_famille-verte_', 'vases', 'repeating', 'the', 'tones', 'of', 'the', 'pale', 'damask', 'curtains', ',', 'and', 'its', 'eighteenth-century', 'pastels', 'in', 'delicate', 'faded', 'frames', '.', '\"Has', 'he', 'chucked', 'his', 'pictures', 'too?', 'I', \"haven't\", 'seen', 'a', 'single', 'one', 'in', 'the', 'house', '.', '\"', 'A', 'slight', 'shade', 'of', 'constraint', 'crossed', 'Mrs', '.', \"Gisburn's\", 'open', 'countenance', '.', '\"It\\'s', 'his', 'ridiculous', 'modesty', ',', 'you', 'know', '.', 'He', 'says', \"they're\", 'not', 'fit', 'to', 'have', 'about', ';', \"he's\", 'sent', 'them', 'all', 'away', 'except', 'one', '--', 'my', 'portrait', '--', 'and', 'that', 'I', 'have', 'to', 'keep', 'upstairs', '.', '\"', 'His', 'ridiculous', 'modesty', '--', \"Jack's\", 'modesty', 'about', 'his', 'pictures?', 'My', 'curiosity', 'was', 'growing', 'like', 'the', 'bean-stalk', '.', 'I', 'said', 'persuasively', 'to', 'my', 'hostess', ':', '\"I', 'must', 'really', 'see', 'your', 'portrait', ',', 'you', 'know', '.', '\"', 'She', 'glanced', 'out', 'almost', 'timorously', 'at', 'the', 'terrace', 'where', 'her', 'husband', ',', 'lounging', 'in', 'a', 'hooded', 'chair', ',', 'had', 'lit', 'a', 'cigar', 'and', 'drawn', 'the', 'Russian', \"deerhound's\", 'head', 'between', 'his', 'knees', '.', '\"Well', ',', 'come', 'while', \"he's\", 'not', 'looking', ',', '\"', 'she', 'said', ',', 'with', 'a', 'laugh', 'that', 'tried', 'to', 'hide', 'her', 'nervousness', ';', 'and', 'I', 'followed', 'her', 'between', 'the', 'marble', 'Emperors', 'of', 'the', 'hall', ',', 'and', 'up', 'the', 'wide', 'stairs', 'with', 'terra-cotta', 'nymphs', 'poised', 'among', 'flowers', 'at', 'each', 'landing', '.', 'In', 'the', 'dimmest', 'corner', 'of', 'her', 'boudoir', ',', 'amid', 'a', 'profusion', 'of', 'delicate', 'and', 'distinguished', 'objects', ',', 'hung', 'one', 'of', 'the', 'familiar', 'oval', 'canvases', ',', 'in', 'the', 'inevitable', 'garlanded', 'frame', '.', 'The', 'mere', 'outline', 'of', 'the', 'frame', 'called', 'up', 'all', \"Gisburn's\", 'past', '!', 'Mrs', '.', 'Gisburn', 'drew', 'back', 'the', 'window-curtains', ',', 'moved', 'aside', 'a', '_jardiniere_', 'full', 'of', 'pink', 'azaleas', ',', 'pushed', 'an', 'arm-chair', 'away', ',', 'and', 'said', ':', '\"If', 'you', 'stand', 'here', 'you', 'can', 'just', 'manage', 'to', 'see', 'it', '.', 'I', 'had', 'it', 'over', 'the', 'mantel-piece', ',', 'but', 'he', \"wouldn't\", 'let', 'it', 'stay', '.', '\"', 'Yes', '--', 'I', 'could', 'just', 'manage', 'to', 'see', 'it', '--', 'the', 'first', 'portrait', 'of', \"Jack's\", 'I', 'had', 'ever', 'had', 'to', 'strain', 'my', 'eyes', 'over', '!', 'Usually', 'they', 'had', 'the', 'place', 'of', 'honour', '--', 'say', 'the', 'central', 'panel', 'in', 'a', 'pale', 'yellow', 'or', '_rose', 'Dubarry_', 'drawing-room', ',', 'or', 'a', 'monumental', 'easel', 'placed', 'so', 'that', 'it', 'took', 'the', 'light', 'through', 'curtains', 'of', 'old', 'Venetian', 'point', '.', 'The', 'more', 'modest', 'place', 'became', 'the', 'picture', 'better', ';', 'yet', ',', 'as', 'my', 'eyes', 'grew', 'accustomed', 'to', 'the', 'half-light', ',', 'all', 'the', 'characteristic', 'qualities', 'came', 'out', '--', 'all', 'the', 'hesitations', 'disguised', 'as', 'audacities', ',', 'the', 'tricks', 'of', 'prestidigitation', 'by', 'which', ',', 'with', 'such', 'consummate', 'skill', ',', 'he', 'managed', 'to', 'divert', 'attention', 'from', 'the', 'real', 'business', 'of', 'the', 'picture', 'to', 'some', 'pretty', 'irrelevance', 'of', 'detail', '.', 'Mrs', '.', 'Gisburn', ',', 'presenting', 'a', 'neutral', 'surface', 'to', 'work', 'on', '--', 'forming', ',', 'as', 'it', 'were', ',', 'so', 'inevitably', 'the', 'background', 'of', 'her', 'own', 'picture', '--', 'had', 'lent', 'herself', 'in', 'an', 'unusual', 'degree', 'to', 'the', 'display', 'of', 'this', 'false', 'virtuosity', '.', 'The', 'picture', 'was', 'one', 'of', \"Jack's\", '\"strongest', ',', '\"', 'as', 'his', 'admirers', 'would', 'have', 'put', 'it', '--', 'it', 'represented', ',', 'on', 'his', 'part', ',', 'a', 'swelling', 'of', 'muscles', ',', 'a', 'congesting', 'of', 'veins', ',', 'a', 'balancing', ',', 'straddling', 'and', 'straining', ',', 'that', 'reminded', 'one', 'of', 'the', \"circus-clown's\", 'ironic', 'efforts', 'to', 'lift', 'a', 'feather', '.', 'It', 'met', ',', 'in', 'short', ',', 'at', 'every', 'point', 'the', 'demand', 'of', 'lovely', 'woman', 'to', 'be', 'painted', '\"strongly\"', 'because', 'she', 'was', 'tired', 'of', 'being', 'painted', '\"sweetly\"', '--', 'and', 'yet', 'not', 'to', 'lose', 'an', 'atom', 'of', 'the', 'sweetness', '.', '\"It\\'s', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.', '\"The', 'last', 'but', 'one', ',', '\"', 'she', 'corrected', 'herself', '--', '\"but', 'the', 'other', \"doesn't\", 'count', ',', 'because', 'he', 'destroyed', 'it', '.', '\"', '\"Destroyed', 'it?\"', 'I', 'was', 'about', 'to', 'follow', 'up', 'this', 'clue', 'when', 'I', 'heard', 'a', 'footstep', 'and', 'saw', 'Jack', 'himself', 'on', 'the', 'threshold', '.', 'As', 'he', 'stood', 'there', ',', 'his', 'hands', 'in', 'the', 'pockets', 'of', 'his', 'velveteen', 'coat', ',', 'the', 'thin', 'brown', 'waves', 'of', 'hair', 'pushed', 'back', 'from', 'his', 'white', 'forehead', ',', 'his', 'lean', 'sunburnt', 'cheeks', 'furrowed', 'by', 'a', 'smile', 'that', 'lifted', 'the', 'tips', 'of', 'a', 'self-confident', 'moustache', ',', 'I', 'felt', 'to', 'what', 'a', 'degree', 'he', 'had', 'the', 'same', 'quality', 'as', 'his', 'pictures', '--', 'the', 'quality', 'of', 'looking', 'cleverer', 'than', 'he', 'was', '.', 'His', 'wife', 'glanced', 'at', 'him', 'deprecatingly', ',', 'but', 'his', 'eyes', 'travelled', 'past', 'her', 'to', 'the', 'portrait', '.', '\"Mr', '.', 'Rickham', 'wanted', 'to', 'see', 'it', ',', '\"', 'she', 'began', ',', 'as', 'if', 'excusing', 'herself', '.', 'He', 'shrugged', 'his', 'shoulders', ',', 'still', 'smiling', '.', '\"Oh', ',', 'Rickham', 'found', 'me', 'out', 'long', 'ago', ',', '\"', 'he', 'said', 'lightly', ';', 'then', ',', 'passing', 'his', 'arm', 'through', 'mine', ':', '\"Come', 'and', 'see', 'the', 'rest', 'of', 'the', 'house', '.', '\"', 'He', 'showed', 'it', 'to', 'me', 'with', 'a', 'kind', 'of', 'naive', 'suburban', 'pride', ':', 'the', 'bath-rooms', ',', 'the', 'speaking-tubes', ',', 'the', 'dress-closets', ',', 'the', 'trouser-presses', '--', 'all', 'the', 'complex', 'simplifications', 'of', 'the', \"millionaire's\", 'domestic', 'economy', '.', 'And', 'whenever', 'my', 'wonder', 'paid', 'the', 'expected', 'tribute', 'he', 'said', ',', 'throwing', 'out', 'his', 'chest', 'a', 'little', ':', '\"Yes', ',', 'I', 'really', \"don't\", 'see', 'how', 'people', 'manage', 'to', 'live', 'without', 'that', '.', '\"', 'Well', '--', 'it', 'was', 'just', 'the', 'end', 'one', 'might', 'have', 'foreseen', 'for', 'him', '.', 'Only', 'he', 'was', ',', 'through', 'it', 'all', 'and', 'in', 'spite', 'of', 'it', 'all', '--', 'as', 'he', 'had', 'been', 'through', ',', 'and', 'in', 'spite', 'of', ',', 'his', 'pictures', '--', 'so', 'handsome', ',', 'so', 'charming', ',', 'so', 'disarming', ',', 'that', 'one', 'longed', 'to', 'cry', 'out', ':', '\"Be', 'dissatisfied', 'with', 'your', 'leisure', '!', '\"', 'as', 'once', 'one', 'had', 'longed', 'to', 'say', ':', '\"Be', 'dissatisfied', 'with', 'your', 'work', '!', '\"', 'But', ',', 'with', 'the', 'cry', 'on', 'my', 'lips', ',', 'my', 'diagnosis', 'suffered', 'an', 'unexpected', 'check', '.', '\"This', 'is', 'my', 'own', 'lair', ',', '\"', 'he', 'said', ',', 'leading', 'me', 'into', 'a', 'dark', 'plain', 'room', 'at', 'the', 'end', 'of', 'the', 'florid', 'vista', '.', 'It', 'was', 'square', 'and', 'brown', 'and', 'leathery', ':', 'no', '\"effects\"', ';', 'no', 'bric-a-brac', ',', 'none', 'of', 'the', 'air', 'of', 'posing', 'for', 'reproduction', 'in', 'a', 'picture', 'weekly', '--', 'above', 'all', ',', 'no', 'least', 'sign', 'of', 'ever', 'having', 'been', 'used', 'as', 'a', 'studio', '.', 'The', 'fact', 'brought', 'home', 'to', 'me', 'the', 'absolute', 'finality', 'of', \"Jack's\", 'break', 'with', 'his', 'old', 'life', '.', '\"Don\\'t', 'you', 'ever', 'dabble', 'with', 'paint', 'any', 'more?\"', 'I', 'asked', ',', 'still', 'looking', 'about', 'for', 'a', 'trace', 'of', 'such', 'activity', '.', '\"Never', ',', '\"', 'he', 'said', 'briefly', '.', '\"Or', 'water-colour', '--', 'or', 'etching?\"', 'His', 'confident', 'eyes', 'grew', 'dim', ',', 'and', 'his', 'cheeks', 'paled', 'a', 'little', 'under', 'their', 'handsome', 'sunburn', '.', '\"Never', 'think', 'of', 'it', ',', 'my', 'dear', 'fellow', '--', 'any', 'more', 'than', 'if', \"I'd\", 'never', 'touched', 'a', 'brush', '.', '\"', 'And', 'his', 'tone', 'told', 'me', 'in', 'a', 'flash', 'that', 'he', 'never', 'thought', 'of', 'anything', 'else', '.', 'I', 'moved', 'away', ',', 'instinctively', 'embarrassed', 'by', 'my', 'unexpected', 'discovery', ';', 'and', 'as', 'I', 'turned', ',', 'my', 'eye', 'fell', 'on', 'a', 'small', 'picture', 'above', 'the', 'mantel-piece', '--', 'the', 'only', 'object', 'breaking', 'the', 'plain', 'oak', 'panelling', 'of', 'the', 'room', '.', '\"Oh', ',', 'by', 'Jove', '!', '\"', 'I', 'said', '.', 'It', 'was', 'a', 'sketch', 'of', 'a', 'donkey', '--', 'an', 'old', 'tired', 'donkey', ',', 'standing', 'in', 'the', 'rain', 'under', 'a', 'wall', '.', '\"By', 'Jove', '--', 'a', 'Stroud', '!', '\"', 'I', 'cried', '.', 'He', 'was', 'silent', ';', 'but', 'I', 'felt', 'him', 'close', 'behind', 'me', ',', 'breathing', 'a', 'little', 'quickly', '.', '\"What', 'a', 'wonder', '!', 'Made', 'with', 'a', 'dozen', 'lines', '--', 'but', 'on', 'everlasting', 'foundations', '.', 'You', 'lucky', 'chap', ',', 'where', 'did', 'you', 'get', 'it?\"', 'He', 'answered', 'slowly', ':', '\"Mrs', '.', 'Stroud', 'gave', 'it', 'to', 'me', '.', '\"', '\"Ah', '--', 'I', \"didn't\", 'know', 'you', 'even', 'knew', 'the', 'Strouds', '.', 'He', 'was', 'such', 'an', 'inflexible', 'hermit', '.', '\"', '\"I', \"didn't\", '--', 'till', 'after', '.', '.', '.', '.', 'She', 'sent', 'for', 'me', 'to', 'paint', 'him', 'when', 'he', 'was', 'dead', '.', '\"', '\"When', 'he', 'was', 'dead?', 'You?\"', 'I', 'must', 'have', 'let', 'a', 'little', 'too', 'much', 'amazement', 'escape', 'through', 'my', 'surprise', ',', 'for', 'he', 'answered', 'with', 'a', 'deprecating', 'laugh', ':', '\"Yes', '--', \"she's\", 'an', 'awful', 'simpleton', ',', 'you', 'know', ',', 'Mrs', '.', 'Stroud', '.', 'Her', 'only', 'idea', 'was', 'to', 'have', 'him', 'done', 'by', 'a', 'fashionable', 'painter', '--', 'ah', ',', 'poor', 'Stroud', '!', 'She', 'thought', 'it', 'the', 'surest', 'way', 'of', 'proclaiming', 'his', 'greatness', '--', 'of', 'forcing', 'it', 'on', 'a', 'purblind', 'public', '.', 'And', 'at', 'the', 'moment', 'I', 'was', '_the_', 'fashionable', 'painter', '.', '\"', '\"Ah', ',', 'poor', 'Stroud', '--', 'as', 'you', 'say', '.', 'Was', '_that_', 'his', 'history?\"', '\"That', 'was', 'his', 'history', '.', 'She', 'believed', 'in', 'him', ',', 'gloried', 'in', 'him', '--', 'or', 'thought', 'she', 'did', '.', 'But', 'she', \"couldn't\", 'bear', 'not', 'to', 'have', 'all', 'the', 'drawing-rooms', 'with', 'her', '.', 'She', \"couldn't\", 'bear', 'the', 'fact', 'that', ',', 'on', 'varnishing', 'days', ',', 'one', 'could', 'always', 'get', 'near', 'enough', 'to', 'see', 'his', 'pictures', '.', 'Poor', 'woman', '!', \"She's\", 'just', 'a', 'fragment', 'groping', 'for', 'other', 'fragments', '.', 'Stroud', 'is', 'the', 'only', 'whole', 'I', 'ever', 'knew', '.', '\"', '\"You', 'ever', 'knew?', 'But', 'you', 'just', 'said', '--', '\"', 'Gisburn', 'had', 'a', 'curious', 'smile', 'in', 'his', 'eyes', '.', '\"Oh', ',', 'I', 'knew', 'him', ',', 'and', 'he', 'knew', 'me', '--', 'only', 'it', 'happened', 'after', 'he', 'was', 'dead', '.', '\"', 'I', 'dropped', 'my', 'voice', 'instinctively', '.', '\"When', 'she', 'sent', 'for', 'you?\"', '\"Yes', '--', 'quite', 'insensible', 'to', 'the', 'irony', '.', 'She', 'wanted', 'him', 'vindicated', '--', 'and', 'by', 'me', '!', '\"', 'He', 'laughed', 'again', ',', 'and', 'threw', 'back', 'his', 'head', 'to', 'look', 'up', 'at', 'the', 'sketch', 'of', 'the', 'donkey', '.', '\"There', 'were', 'days', 'when', 'I', \"couldn't\", 'look', 'at', 'that', 'thing', '--', \"couldn't\", 'face', 'it', '.', 'But', 'I', 'forced', 'myself', 'to', 'put', 'it', 'here', ';', 'and', 'now', \"it's\", 'cured', 'me', '--', 'cured', 'me', '.', \"That's\", 'the', 'reason', 'why', 'I', \"don't\", 'dabble', 'any', 'more', ',', 'my', 'dear', 'Rickham', ';', 'or', 'rather', 'Stroud', 'himself', 'is', 'the', 'reason', '.', '\"', 'For', 'the', 'first', 'time', 'my', 'idle', 'curiosity', 'about', 'my', 'companion', 'turned', 'into', 'a', 'serious', 'desire', 'to', 'understand', 'him', 'better', '.', '\"I', 'wish', \"you'd\", 'tell', 'me', 'how', 'it', 'happened', ',', '\"', 'I', 'said', '.', 'He', 'stood', 'looking', 'up', 'at', 'the', 'sketch', ',', 'and', 'twirling', 'between', 'his', 'fingers', 'a', 'cigarette', 'he', 'had', 'forgotten', 'to', 'light', '.', 'Suddenly', 'he', 'turned', 'toward', 'me', '.', '\"I\\'d', 'rather', 'like', 'to', 'tell', 'you', '--', 'because', \"I've\", 'always', 'suspected', 'you', 'of', 'loathing', 'my', 'work', '.', '\"', 'I', 'made', 'a', 'deprecating', 'gesture', ',', 'which', 'he', 'negatived', 'with', 'a', 'good-humoured', 'shrug', '.', '\"Oh', ',', 'I', \"didn't\", 'care', 'a', 'straw', 'when', 'I', 'believed', 'in', 'myself', '--', 'and', 'now', \"it's\", 'an', 'added', 'tie', 'between', 'us', '!', '\"', 'He', 'laughed', 'slightly', ',', 'without', 'bitterness', ',', 'and', 'pushed', 'one', 'of', 'the', 'deep', 'arm-chairs', 'forward', '.', '\"There', ':', 'make', 'yourself', 'comfortable', '--', 'and', 'here', 'are', 'the', 'cigars', 'you', 'like', '.', '\"', 'He', 'placed', 'them', 'at', 'my', 'elbow', 'and', 'continued', 'to', 'wander', 'up', 'and', 'down', 'the', 'room', ',', 'stopping', 'now', 'and', 'then', 'beneath', 'the', 'picture', '.', '\"How', 'it', 'happened?', 'I', 'can', 'tell', 'you', 'in', 'five', 'minutes', '--', 'and', 'it', \"didn't\", 'take', 'much', 'longer', 'to', 'happen', '.', '.', '.', '.', 'I', 'can', 'remember', 'now', 'how', 'surprised', 'and', 'pleased', 'I', 'was', 'when', 'I', 'got', 'Mrs', '.', \"Stroud's\", 'note', '.', 'Of', 'course', ',', 'deep', 'down', ',', 'I', 'had', 'always', '_felt_', 'there', 'was', 'no', 'one', 'like', 'him', '--', 'only', 'I', 'had', 'gone', 'with', 'the', 'stream', ',', 'echoed', 'the', 'usual', 'platitudes', 'about', 'him', ',', 'till', 'I', 'half', 'got', 'to', 'think', 'he', 'was', 'a', 'failure', ',', 'one', 'of', 'the', 'kind', 'that', 'are', 'left', 'behind', '.', 'By', 'Jove', ',', 'and', 'he', '_was_', 'left', 'behind', '--', 'because', 'he', 'had', 'come', 'to', 'stay', '!', 'The', 'rest', 'of', 'us', 'had', 'to', 'let', 'ourselves', 'be', 'swept', 'along', 'or', 'go', 'under', ',', 'but', 'he', 'was', 'high', 'above', 'the', 'current', '--', 'on', 'everlasting', 'foundations', ',', 'as', 'you', 'say', '.', '\"Well', ',', 'I', 'went', 'off', 'to', 'the', 'house', 'in', 'my', 'most', 'egregious', 'mood', '--', 'rather', 'moved', ',', 'Lord', 'forgive', 'me', ',', 'at', 'the', 'pathos', 'of', 'poor', \"Stroud's\", 'career', 'of', 'failure', 'being', 'crowned', 'by', 'the', 'glory', 'of', 'my', 'painting', 'him', '!', 'Of', 'course', 'I', 'meant', 'to', 'do', 'the', 'picture', 'for', 'nothing', '--', 'I', 'told', 'Mrs', '.', 'Stroud', 'so', 'when', 'she', 'began', 'to', 'stammer', 'something', 'about', 'her', 'poverty', '.', 'I', 'remember', 'getting', 'off', 'a', 'prodigious', 'phrase', 'about', 'the', 'honour', 'being', '_mine_', '--', 'oh', ',', 'I', 'was', 'princely', ',', 'my', 'dear', 'Rickham', '!', 'I', 'was', 'posing', 'to', 'myself', 'like', 'one', 'of', 'my', 'own', 'sitters', '.', '\"Then', 'I', 'was', 'taken', 'up', 'and', 'left', 'alone', 'with', 'him', '.', 'I', 'had', 'sent', 'all', 'my', 'traps', 'in', 'advance', ',', 'and', 'I', 'had', 'only', 'to', 'set', 'up', 'the', 'easel', 'and', 'get', 'to', 'work', '.', 'He', 'had', 'been', 'dead', 'only', 'twenty-four', 'hours', ',', 'and', 'he', 'died', 'suddenly', ',', 'of', 'heart', 'disease', ',', 'so', 'that', 'there', 'had', 'been', 'no', 'preliminary', 'work', 'of', 'destruction', '--', 'his', 'face', 'was', 'clear', 'and', 'untouched', '.', 'I', 'had', 'met', 'him', 'once', 'or', 'twice', ',', 'years', 'before', ',', 'and', 'thought', 'him', 'insignificant', 'and', 'dingy', '.', 'Now', 'I', 'saw', 'that', 'he', 'was', 'superb', '.', '\"I', 'was', 'glad', 'at', 'first', ',', 'with', 'a', 'merely', 'aesthetic', 'satisfaction', ':', 'glad', 'to', 'have', 'my', 'hand', 'on', 'such', 'a', \"'subject\", '.', \"'\", 'Then', 'his', 'strange', 'life-likeness', 'began', 'to', 'affect', 'me', 'queerly', '--', 'as', 'I', 'blocked', 'the', 'head', 'in', 'I', 'felt', 'as', 'if', 'he', 'were', 'watching', 'me', 'do', 'it', '.', 'The', 'sensation', 'was', 'followed', 'by', 'the', 'thought', ':', 'if', 'he', '_were_', 'watching', 'me', ',', 'what', 'would', 'he', 'say', 'to', 'my', 'way', 'of', 'working?', 'My', 'strokes', 'began', 'to', 'go', 'a', 'little', 'wild', '--', 'I', 'felt', 'nervous', 'and', 'uncertain', '.', '\"Once', ',', 'when', 'I', 'looked', 'up', ',', 'I', 'seemed', 'to', 'see', 'a', 'smile', 'behind', 'his', 'close', 'grayish', 'beard', '--', 'as', 'if', 'he', 'had', 'the', 'secret', ',', 'and', 'were', 'amusing', 'himself', 'by', 'holding', 'it', 'back', 'from', 'me', '.', 'That', 'exasperated', 'me', 'still', 'more', '.', 'The', 'secret?', 'Why', ',', 'I', 'had', 'a', 'secret', 'worth', 'twenty', 'of', 'his', '!', 'I', 'dashed', 'at', 'the', 'canvas', 'furiously', ',', 'and', 'tried', 'some', 'of', 'my', 'bravura', 'tricks', '.', 'But', 'they', 'failed', 'me', ',', 'they', 'crumbled', '.', 'I', 'saw', 'that', 'he', \"wasn't\", 'watching', 'the', 'showy', 'bits', '--', 'I', \"couldn't\", 'distract', 'his', 'attention', ';', 'he', 'just', 'kept', 'his', 'eyes', 'on', 'the', 'hard', 'passages', 'between', '.', 'Those', 'were', 'the', 'ones', 'I', 'had', 'always', 'shirked', ',', 'or', 'covered', 'up', 'with', 'some', 'lying', 'paint', '.', 'And', 'how', 'he', 'saw', 'through', 'my', 'lies', '!', '\"I', 'looked', 'up', 'again', ',', 'and', 'caught', 'sight', 'of', 'that', 'sketch', 'of', 'the', 'donkey', 'hanging', 'on', 'the', 'wall', 'near', 'his', 'bed', '.', 'His', 'wife', 'told', 'me', 'afterward', 'it', 'was', 'the', 'last', 'thing', 'he', 'had', 'done', '--', 'just', 'a', 'note', 'taken', 'with', 'a', 'shaking', 'hand', ',', 'when', 'he', 'was', 'down', 'in', 'Devonshire', 'recovering', 'from', 'a', 'previous', 'heart', 'attack', '.', 'Just', 'a', 'note', '!', 'But', 'it', 'tells', 'his', 'whole', 'history', '.', 'There', 'are', 'years', 'of', 'patient', 'scornful', 'persistence', 'in', 'every', 'line', '.', 'A', 'man', 'who', 'had', 'swum', 'with', 'the', 'current', 'could', 'never', 'have', 'learned', 'that', 'mighty', 'up-stream', 'stroke', '.', '.', '.', '.', '\"I', 'turned', 'back', 'to', 'my', 'work', ',', 'and', 'went', 'on', 'groping', 'and', 'muddling', ';', 'then', 'I', 'looked', 'at', 'the', 'donkey', 'again', '.', 'I', 'saw', 'that', ',', 'when', 'Stroud', 'laid', 'in', 'the', 'first', 'stroke', ',', 'he', 'knew', 'just', 'what', 'the', 'end', 'would', 'be', '.', 'He', 'had', 'possessed', 'his', 'subject', ',', 'absorbed', 'it', ',', 'recreated', 'it', '.', 'When', 'had', 'I', 'done', 'that', 'with', 'any', 'of', 'my', 'things?', 'They', \"hadn't\", 'been', 'born', 'of', 'me', '--', 'I', 'had', 'just', 'adopted', 'them', '.', '.', '.', '.', '\"Hang', 'it', ',', 'Rickham', ',', 'with', 'that', 'face', 'watching', 'me', 'I', \"couldn't\", 'do', 'another', 'stroke', '.', 'The', 'plain', 'truth', 'was', ',', 'I', \"didn't\", 'know', 'where', 'to', 'put', 'it', '--', '_I', 'had', 'never', 'known_', '.', 'Only', ',', 'with', 'my', 'sitters', 'and', 'my', 'public', ',', 'a', 'showy', 'splash', 'of', 'colour', 'covered', 'up', 'the', 'fact', '--', 'I', 'just', 'threw', 'paint', 'into', 'their', 'faces', '.', '.', '.', '.', 'Well', ',', 'paint', 'was', 'the', 'one', 'medium', 'those', 'dead', 'eyes', 'could', 'see', 'through', '--', 'see', 'straight', 'to', 'the', 'tottering', 'foundations', 'underneath', '.', \"Don't\", 'you', 'know', 'how', ',', 'in', 'talking', 'a', 'foreign', 'language', ',', 'even', 'fluently', ',', 'one', 'says', 'half', 'the', 'time', 'not', 'what', 'one', 'wants', 'to', 'but', 'what', 'one', 'can?', 'Well', '--', 'that', 'was', 'the', 'way', 'I', 'painted', ';', 'and', 'as', 'he', 'lay', 'there', 'and', 'watched', 'me', ',', 'the', 'thing', 'they', 'called', 'my', \"'technique'\", 'collapsed', 'like', 'a', 'house', 'of', 'cards', '.', 'He', \"didn't\", 'sneer', ',', 'you', 'understand', ',', 'poor', 'Stroud', '--', 'he', 'just', 'lay', 'there', 'quietly', 'watching', ',', 'and', 'on', 'his', 'lips', ',', 'through', 'the', 'gray', 'beard', ',', 'I', 'seemed', 'to', 'hear', 'the', 'question', ':', \"'Are\", 'you', 'sure', 'you', 'know', 'where', \"you're\", 'coming', \"out?'\", '\"If', 'I', 'could', 'have', 'painted', 'that', 'face', ',', 'with', 'that', 'question', 'on', 'it', ',', 'I', 'should', 'have', 'done', 'a', 'great', 'thing', '.', 'The', 'next', 'greatest', 'thing', 'was', 'to', 'see', 'that', 'I', \"couldn't\", '--', 'and', 'that', 'grace', 'was', 'given', 'me', '.', 'But', ',', 'oh', ',', 'at', 'that', 'minute', ',', 'Rickham', ',', 'was', 'there', 'anything', 'on', 'earth', 'I', \"wouldn't\", 'have', 'given', 'to', 'have', 'Stroud', 'alive', 'before', 'me', ',', 'and', 'to', 'hear', 'him', 'say', ':', \"'It's\", 'not', 'too', 'late', '--', \"I'll\", 'show', 'you', \"how'?\", '\"It', '_was_', 'too', 'late', '--', 'it', 'would', 'have', 'been', ',', 'even', 'if', \"he'd\", 'been', 'alive', '.', 'I', 'packed', 'up', 'my', 'traps', ',', 'and', 'went', 'down', 'and', 'told', 'Mrs', '.', 'Stroud', '.', 'Of', 'course', 'I', \"didn't\", 'tell', 'her', '_that_', '--', 'it', 'would', 'have', 'been', 'Greek', 'to', 'her', '.', 'I', 'simply', 'said', 'I', \"couldn't\", 'paint', 'him', ',', 'that', 'I', 'was', 'too', 'moved', '.', 'She', 'rather', 'liked', 'the', 'idea', '--', \"she's\", 'so', 'romantic', '!', 'It', 'was', 'that', 'that', 'made', 'her', 'give', 'me', 'the', 'donkey', '.', 'But', 'she', 'was', 'terribly', 'upset', 'at', 'not', 'getting', 'the', 'portrait', '--', 'she', 'did', 'so', 'want', 'him', \"'done'\", 'by', 'some', 'one', 'showy', '!', 'At', 'first', 'I', 'was', 'afraid', 'she', \"wouldn't\", 'let', 'me', 'off', '--', 'and', 'at', 'my', \"wits'\", 'end', 'I', 'suggested', 'Grindle', '.', 'Yes', ',', 'it', 'was', 'I', 'who', 'started', 'Grindle', ':', 'I', 'told', 'Mrs', '.', 'Stroud', 'he', 'was', 'the', \"'coming'\", 'man', ',', 'and', 'she', 'told', 'somebody', 'else', ',', 'and', 'so', 'it', 'got', 'to', 'be', 'true', '.', '.', '.', '.', 'And', 'he', 'painted', 'Stroud', 'without', 'wincing', ';', 'and', 'she', 'hung', 'the', 'picture', 'among', 'her', \"husband's\", 'things', '.', '.', '.', '.', '\"', 'He', 'flung', 'himself', 'down', 'in', 'the', 'arm-chair', 'near', 'mine', ',', 'laid', 'back', 'his', 'head', ',', 'and', 'clasping', 'his', 'arms', 'beneath', 'it', ',', 'looked', 'up', 'at', 'the', 'picture', 'above', 'the', 'chimney-piece', '.', '\"I', 'like', 'to', 'fancy', 'that', 'Stroud', 'himself', 'would', 'have', 'given', 'it', 'to', 'me', ',', 'if', \"he'd\", 'been', 'able', 'to', 'say', 'what', 'he', 'thought', 'that', 'day', '.', '\"', 'And', ',', 'in', 'answer', 'to', 'a', 'question', 'I', 'put', 'half-mechanically', '--', '\"Begin', 'again?\"', 'he', 'flashed', 'out', '.', '\"When', 'the', 'one', 'thing', 'that', 'brings', 'me', 'anywhere', 'near', 'him', 'is', 'that', 'I', 'knew', 'enough', 'to', 'leave', 'off?\"', 'He', 'stood', 'up', 'and', 'laid', 'his', 'hand', 'on', 'my', 'shoulder', 'with', 'a', 'laugh', '.', '\"Only', 'the', 'irony', 'of', 'it', 'is', 'that', 'I', '_am_', 'still', 'painting', '--', 'since', \"Grindle's\", 'doing', 'it', 'for', 'me', '!', 'The', 'Strouds', 'stand', 'alone', ',', 'and', 'happen', 'once', '--', 'but', \"there's\", 'no', 'exterminating', 'our', 'kind', 'of', 'art', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is a word based tokenization algorithm. This tokenize for each word.....\n",
    "\n",
    "result = [item.strip() for item in split_text if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec956c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4364"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ca9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n"
     ]
    }
   ],
   "source": [
    "words = sorted(set(result))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96156f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, '\"Ah': 2, '\"Be': 3, '\"Begin': 4, '\"By': 5, '\"Come': 6, '\"Destroyed': 7, '\"Don\\'t': 8, '\"Gisburns\"': 9, '\"Grindles': 10, '\"Hang': 11, '\"Has': 12, '\"How': 13, '\"I': 14, '\"I\\'d': 15, '\"If': 16, '\"It': 17, '\"It\\'s': 18, '\"Jack': 19, '\"Money\\'s': 20, '\"Moon-dancers\"': 21, '\"Mr': 22, '\"Mrs': 23, '\"My': 24, '\"Never': 25, '\"Of': 26, '\"Oh': 27, '\"Once': 28, '\"Only': 29, '\"Or': 30, '\"That': 31, '\"The': 32, '\"Then': 33, '\"There': 34, '\"This': 35, '\"We': 36, '\"Well': 37, '\"What': 38, '\"When': 39, '\"Why': 40, '\"Yes': 41, '\"You': 42, '\"but': 43, '\"deadening': 44, '\"dragged': 45, '\"effects\"': 46, '\"interesting\"': 47, '\"lift': 48, '\"obituary\"': 49, '\"strongest': 50, '\"strongly\"': 51, '\"sweetly\"': 52, \"'\": 53, \"'Are\": 54, \"'It's\": 55, \"'coming'\": 56, \"'done'\": 57, \"'subject\": 58, \"'technique'\": 59, \"'way\": 60, '(': 61, ')': 62, ',': 63, '--': 64, '.': 65, ':': 66, ';': 67, 'A': 68, 'Among': 69, 'And': 70, 'Arrt': 71, 'As': 72, 'At': 73, 'Burlington': 74, 'But': 75, 'By': 76, 'Carlo': 77, 'Chicago': 78, 'Claude': 79, 'Croft': 80, 'Devonshire': 81, \"Don't\": 82, 'Dubarry_': 83, 'Emperors': 84, 'Florence': 85, 'For': 86, 'Gallery': 87, 'Gideon': 88, 'Gisburn': 89, \"Gisburn's\": 90, 'Grafton': 91, 'Greek': 92, 'Grindle': 93, \"Grindle's\": 94, 'HAD': 95, 'Had': 96, 'He': 97, 'Her': 98, 'Hermia': 99, \"Hermia's\": 100, 'His': 101, 'I': 102, \"I'd\": 103, \"I'll\": 104, \"I've\": 105, 'If': 106, 'In': 107, 'It': 108, 'Jack': 109, \"Jack's\": 110, 'Jove': 111, 'Just': 112, 'Lord': 113, 'Made': 114, 'Miss': 115, 'Monte': 116, 'Mr': 117, 'Mrs': 118, 'My': 119, 'No': 120, 'Now': 121, 'Nutley': 122, 'Of': 123, 'On': 124, 'Only': 125, 'Perhaps': 126, 'Poor': 127, 'Professional': 128, 'Renaissance': 129, 'Rickham': 130, 'Riviera': 131, 'Rome': 132, 'Russian': 133, 'Sevres': 134, 'She': 135, \"She's\": 136, 'Stroud': 137, \"Stroud's\": 138, 'Strouds': 139, 'Suddenly': 140, 'That': 141, \"That's\": 142, 'The': 143, 'Then': 144, 'There': 145, 'They': 146, 'Those': 147, 'Though': 148, 'Thwing': 149, \"Thwing's\": 150, 'Thwings': 151, 'To': 152, 'Usually': 153, 'Venetian': 154, 'Victor': 155, 'Was': 156, 'Well': 157, 'What': 158, 'When': 159, 'Why': 160, 'Yes': 161, 'You': 162, 'You?\"': 163, '_I': 164, '_am_': 165, '_famille-verte_': 166, '_felt_': 167, '_has_': 168, '_have_': 169, '_jardiniere_': 170, '_mine_': 171, '_not_': 172, '_rose': 173, '_rs_': 174, '_that_': 175, '_the_': 176, '_was_': 177, '_were_': 178, 'a': 179, 'abdication': 180, 'able': 181, 'about': 182, 'above': 183, 'abruptly': 184, 'absolute': 185, 'absorbed': 186, 'absurdity': 187, 'academic': 188, 'accuse': 189, 'accustomed': 190, 'across': 191, 'activity': 192, 'add': 193, 'added': 194, 'admirers': 195, 'adopted': 196, 'adulation': 197, 'advance': 198, 'aesthetic': 199, 'affect': 200, 'afraid': 201, 'after': 202, 'afterward': 203, 'again': 204, 'again\"?': 205, 'again?\"': 206, 'ago': 207, 'ah': 208, 'air': 209, 'alive': 210, 'all': 211, 'almost': 212, 'alone': 213, 'along': 214, 'always': 215, 'amazement': 216, 'amid': 217, 'among': 218, 'amplest': 219, 'amusing': 220, 'an': 221, 'and': 222, 'another': 223, 'answer': 224, 'answered': 225, 'any': 226, 'anything': 227, 'anywhere': 228, 'apparent': 229, 'apparently': 230, 'appearance': 231, 'appeared': 232, 'appointed': 233, 'are': 234, 'arm': 235, 'arm-chair': 236, 'arm-chairs': 237, 'arms': 238, 'art': 239, 'articles': 240, 'artist': 241, 'as': 242, 'aside': 243, 'asked': 244, 'at': 245, 'atmosphere': 246, 'atom': 247, 'attack': 248, 'attention': 249, 'attitude': 250, 'audacities': 251, 'away': 252, 'awful': 253, 'axioms': 254, 'azaleas': 255, 'back': 256, 'background': 257, 'balance': 258, 'balancing': 259, 'balustraded': 260, 'basking': 261, 'bath-rooms': 262, 'be': 263, 'beaming': 264, 'bean-stalk': 265, 'bear': 266, 'beard': 267, 'beauty': 268, 'became': 269, 'because': 270, 'becoming': 271, 'bed': 272, 'been': 273, 'before': 274, 'began': 275, 'begun': 276, 'behind': 277, 'being': 278, 'believed': 279, 'beneath': 280, 'bespoke': 281, 'better': 282, 'between': 283, 'big': 284, 'bits': 285, 'bitterness': 286, 'blocked': 287, 'born': 288, 'borne': 289, 'boudoir': 290, 'bravura': 291, 'break': 292, 'breaking': 293, 'breathing': 294, 'bric-a-brac': 295, 'briefly': 296, 'brings': 297, 'bronzes': 298, 'brought': 299, 'brown': 300, 'brush': 301, 'bull': 302, 'business': 303, 'but': 304, 'buying': 305, 'by': 306, 'called': 307, 'came': 308, 'can': 309, 'can?': 310, 'canvas': 311, 'canvases': 312, 'cards': 313, 'care': 314, 'career': 315, 'caught': 316, 'central': 317, 'chair': 318, 'chap': 319, 'characteristic': 320, 'charming': 321, 'cheap': 322, 'check': 323, 'cheeks': 324, 'chest': 325, 'chimney-piece': 326, 'chucked': 327, 'cigar': 328, 'cigarette': 329, 'cigars': 330, 'circulation': 331, 'circumstance': 332, \"circus-clown's\": 333, 'claimed': 334, 'clasping': 335, 'clear': 336, 'cleverer': 337, 'close': 338, 'clue': 339, 'coat': 340, 'collapsed': 341, 'colour': 342, 'come': 343, 'comfortable': 344, 'coming': 345, 'companion': 346, 'compared': 347, 'complex': 348, 'confident': 349, 'congesting': 350, 'conjugal': 351, 'constraint': 352, 'consummate': 353, 'contended': 354, 'continued': 355, 'corner': 356, 'corrected': 357, 'could': 358, \"couldn't\": 359, 'count': 360, 'countenance': 361, 'couple': 362, 'course': 363, 'covered': 364, 'craft': 365, 'cried': 366, 'crossed': 367, 'crowned': 368, 'crumbled': 369, 'cry': 370, 'cured': 371, 'curiosity': 372, 'curious': 373, 'current': 374, 'curtains': 375, 'dabble': 376, 'damask': 377, 'dark': 378, 'dashed': 379, 'day': 380, 'days': 381, 'dead': 382, 'dead?': 383, 'dear': 384, 'deep': 385, \"deerhound's\": 386, 'degree': 387, 'delicate': 388, 'demand': 389, 'denied': 390, 'deploring': 391, 'deprecating': 392, 'deprecatingly': 393, 'desire': 394, 'destroyed': 395, 'destruction': 396, 'desultory': 397, 'detail': 398, 'diagnosis': 399, 'did': 400, \"didn't\": 401, 'died': 402, 'dim': 403, 'dimmest': 404, 'dingy': 405, 'dining-room': 406, 'disarming': 407, 'discovery': 408, 'discrimination': 409, 'discussion': 410, 'disdain': 411, 'disdained': 412, 'disease': 413, 'disguised': 414, 'display': 415, 'dissatisfied': 416, 'distinguished': 417, 'distract': 418, 'divert': 419, 'do': 420, \"doesn't\": 421, 'doing': 422, 'domestic': 423, \"don't\": 424, 'done': 425, 'donkey': 426, 'down': 427, 'dozen': 428, 'dragged': 429, 'drawing-room': 430, 'drawing-rooms': 431, 'drawn': 432, 'dress-closets': 433, 'drew': 434, 'dropped': 435, 'each': 436, 'earth': 437, 'ease': 438, 'easel': 439, 'easy': 440, 'echoed': 441, 'economy': 442, 'effect': 443, 'efforts': 444, 'egregious': 445, 'eighteenth-century': 446, 'elbow': 447, 'elegant': 448, 'else': 449, 'embarrassed': 450, 'enabled': 451, 'end': 452, 'endless': 453, 'enjoy': 454, 'enlightenment': 455, 'enough': 456, 'ensuing': 457, 'equally': 458, 'equanimity': 459, 'escape': 460, 'established': 461, 'etching?\"': 462, 'even': 463, 'event': 464, 'ever': 465, 'everlasting': 466, 'every': 467, 'exasperated': 468, 'except': 469, 'excuse': 470, 'excusing': 471, 'existed': 472, 'expected': 473, 'exquisite': 474, 'exquisitely': 475, 'extenuation': 476, 'exterminating': 477, 'extracting': 478, 'eye': 479, 'eyebrows': 480, 'eyes': 481, 'face': 482, 'faces': 483, 'fact': 484, 'faded': 485, 'failed': 486, 'failure': 487, 'fair': 488, 'faith': 489, 'false': 490, 'familiar': 491, 'fancy': 492, 'fashionable': 493, 'fate': 494, 'feather': 495, 'feet': 496, 'fell': 497, 'fellow': 498, 'felt': 499, 'few': 500, 'fewer': 501, 'finality': 502, 'find': 503, 'fingers': 504, 'first': 505, 'fit': 506, 'fitting': 507, 'five': 508, 'flash': 509, 'flashed': 510, 'florid': 511, 'flowers': 512, 'fluently': 513, 'flung': 514, 'follow': 515, 'followed': 516, 'fond': 517, 'footstep': 518, 'for': 519, 'forced': 520, 'forcing': 521, 'forehead': 522, 'foreign': 523, 'foreseen': 524, 'forgive': 525, 'forgotten': 526, 'form': 527, 'formed': 528, 'forming': 529, 'forward': 530, 'fostered': 531, 'found': 532, 'foundations': 533, 'fragment': 534, 'fragments': 535, 'frame': 536, 'frames': 537, 'frequently': 538, \"friend's\": 539, 'from': 540, 'full': 541, 'fullest': 542, 'furiously': 543, 'furrowed': 544, 'garlanded': 545, 'garlands': 546, 'gave': 547, 'genial': 548, 'genius': 549, 'gesture': 550, 'get': 551, 'getting': 552, 'give': 553, 'given': 554, 'glad': 555, 'glanced': 556, 'glimpse': 557, 'gloried': 558, 'glory': 559, 'glory\"': 560, 'go': 561, 'going': 562, 'gone': 563, 'good': 564, 'good-breeding': 565, 'good-humoured': 566, 'got': 567, 'grace': 568, 'gradually': 569, 'gray': 570, 'grayish': 571, 'great': 572, 'greatest': 573, 'greatness': 574, 'grew': 575, 'groping': 576, 'growing': 577, 'had': 578, \"hadn't\": 579, 'hair': 580, 'half': 581, 'half-light': 582, 'half-mechanically': 583, 'hall': 584, 'hand': 585, 'hands': 586, 'handsome': 587, 'hanging': 588, 'happen': 589, 'happened': 590, 'happened?': 591, 'hard': 592, 'hardly': 593, 'have': 594, \"haven't\": 595, 'having': 596, 'he': 597, \"he'd\": 598, \"he's\": 599, 'head': 600, 'hear': 601, 'heard': 602, 'heart': 603, 'height': 604, 'her': 605, 'here': 606, 'hermit': 607, 'herself': 608, 'hesitations': 609, 'hide': 610, 'high': 611, 'him': 612, 'himself': 613, 'hint': 614, 'his': 615, 'history': 616, 'history?\"': 617, 'holding': 618, 'home': 619, 'honour': 620, 'hooded': 621, 'hostess': 622, 'hot-house': 623, 'hour': 624, 'hours': 625, 'house': 626, 'how': 627, \"how'?\": 628, 'hung': 629, 'husband': 630, \"husband's\": 631, 'idea': 632, 'idle': 633, 'idling': 634, 'if': 635, 'immediately': 636, 'in': 637, 'incense': 638, 'indifferent': 639, 'inevitable': 640, 'inevitably': 641, 'inflexible': 642, 'insensible': 643, 'insignificant': 644, 'instinctively': 645, 'instructive': 646, 'interesting': 647, 'into': 648, 'ironic': 649, 'irony': 650, 'irrelevance': 651, 'irrevocable': 652, 'is': 653, 'it': 654, \"it's\": 655, 'it?\"': 656, 'its': 657, 'itself': 658, 'jealousy': 659, 'jealousy?': 660, 'just': 661, 'keep': 662, 'kept': 663, 'kind': 664, 'knees': 665, 'knew': 666, 'knew?': 667, 'know': 668, 'known_': 669, 'laid': 670, 'lair': 671, 'landing': 672, 'language': 673, 'last': 674, 'late': 675, 'later': 676, 'latter': 677, \"latter's\": 678, 'laugh': 679, 'laughed': 680, 'lay': 681, 'leading': 682, 'lean': 683, 'learned': 684, 'least': 685, 'leathery': 686, 'leave': 687, 'led': 688, 'left': 689, 'leisure': 690, 'lends': 691, 'lent': 692, 'let': 693, 'lies': 694, 'life': 695, 'life-likeness': 696, 'lift': 697, 'lifted': 698, 'light': 699, 'lightly': 700, 'like': 701, 'liked': 702, 'line': 703, 'lines': 704, 'lingered': 705, 'lips': 706, 'lit': 707, 'little': 708, 'live': 709, 'loathing': 710, 'long': 711, 'longed': 712, 'longer': 713, 'look': 714, 'looked': 715, 'looking': 716, 'lose': 717, 'loss': 718, 'lounging': 719, 'lovely': 720, 'lucky': 721, 'lump': 722, 'luncheon-table': 723, 'luxury': 724, 'lying': 725, 'made': 726, 'make': 727, 'man': 728, 'manage': 729, 'managed': 730, 'mantel-piece': 731, 'marble': 732, 'married': 733, 'may': 734, 'me': 735, 'meant': 736, 'mediocrity\"': 737, 'medium': 738, 'mentioned': 739, 'mere': 740, 'merely': 741, 'met': 742, 'might': 743, 'mighty': 744, \"millionaire's\": 745, 'mine': 746, 'minute': 747, 'minutes': 748, 'mirrors': 749, 'modest': 750, 'modesty': 751, 'moment': 752, 'money': 753, 'monumental': 754, 'mood': 755, 'morbidly': 756, 'more': 757, 'more?\"': 758, 'most': 759, 'mourn': 760, 'mourned': 761, 'moustache': 762, 'moved': 763, 'much': 764, 'muddling': 765, 'multiplied': 766, 'murmur': 767, 'muscles': 768, 'must': 769, 'my': 770, 'myself': 771, 'mysterious': 772, 'naive': 773, 'near': 774, 'nearly': 775, 'negatived': 776, 'nervous': 777, 'nervousness': 778, 'neutral': 779, 'never': 780, 'next': 781, 'no': 782, 'none': 783, 'not': 784, 'note': 785, 'nothing': 786, 'now': 787, 'nymphs': 788, 'oak': 789, 'object': 790, 'objects': 791, 'occurred': 792, 'oddly': 793, 'of': 794, 'off': 795, 'off?\"': 796, 'often': 797, 'oh': 798, 'old': 799, 'on': 800, 'once': 801, 'one': 802, 'ones': 803, 'only': 804, 'onto': 805, 'open': 806, 'or': 807, 'other': 808, 'our': 809, 'ourselves': 810, 'out': 811, \"out?'\": 812, 'outline': 813, 'oval': 814, 'over': 815, 'own': 816, 'packed': 817, 'paid': 818, 'paint': 819, 'painted': 820, 'painter': 821, 'painting': 822, 'painting?\"': 823, 'pale': 824, 'paled': 825, 'palm-trees': 826, 'panel': 827, 'panelling': 828, 'pardonable': 829, 'pardoned': 830, 'part': 831, 'passages': 832, 'passing': 833, 'past': 834, 'pastels': 835, 'pathos': 836, 'patient': 837, 'people': 838, 'perceptible': 839, 'perfect': 840, 'persistence': 841, 'persuasively': 842, 'phrase': 843, 'picture': 844, 'pictures': 845, 'pictures?': 846, 'pines': 847, 'pink': 848, 'place': 849, 'placed': 850, 'plain': 851, 'platitudes': 852, 'pleased': 853, 'pockets': 854, 'point': 855, 'poised': 856, 'poor': 857, 'portrait': 858, 'posing': 859, 'possessed': 860, 'poverty': 861, 'predicted': 862, 'preliminary': 863, 'presenting': 864, 'prestidigitation': 865, 'pretty': 866, 'previous': 867, 'price': 868, 'pride': 869, 'princely': 870, 'prism': 871, 'problem': 872, 'proclaiming': 873, 'prodigious': 874, 'profusion': 875, 'protest': 876, 'prove': 877, 'public': 878, 'purblind': 879, 'purely': 880, 'pushed': 881, 'put': 882, 'qualities': 883, 'quality': 884, 'queerly': 885, 'question': 886, 'quickly': 887, 'quietly': 888, 'quite': 889, 'quote': 890, 'rain': 891, 'raised': 892, 'random': 893, 'rather': 894, 'real': 895, 'really': 896, 'reared': 897, 'reason': 898, 'reassurance': 899, 'recovering': 900, 'recreated': 901, 'reflected': 902, 'reflection': 903, 'regrets': 904, 'relatively': 905, 'remained': 906, 'remember': 907, 'reminded': 908, 'repeating': 909, 'represented': 910, 'reproduction': 911, 'resented': 912, 'resolve': 913, 'resources': 914, 'rest': 915, 'rich': 916, 'ridiculous': 917, 'robbed': 918, 'romantic': 919, 'room': 920, 'rose': 921, 'rule': 922, 'run': 923, 'said': 924, 'same': 925, 'satisfaction': 926, 'savour?': 927, 'saw': 928, 'say': 929, 'saying': 930, 'says': 931, 'scorn': 932, 'scornful': 933, 'secret': 934, 'secret?': 935, 'see': 936, 'seemed': 937, 'seen': 938, 'self-confident': 939, 'send': 940, 'sensation': 941, 'sensitive': 942, 'sent': 943, 'serious': 944, 'set': 945, 'sex': 946, 'shade': 947, 'shaking': 948, 'shall': 949, 'she': 950, \"she's\": 951, 'shirked': 952, 'short': 953, 'should': 954, 'shoulder': 955, 'shoulders': 956, 'show': 957, 'showed': 958, 'showy': 959, 'shrug': 960, 'shrugged': 961, 'sight': 962, 'sign': 963, 'silent': 964, 'silver': 965, 'similar': 966, 'simpleton': 967, 'simplifications': 968, 'simply': 969, 'since': 970, 'single': 971, 'sitter': 972, 'sitters': 973, 'sketch': 974, 'skill': 975, 'slight': 976, 'slightly': 977, 'slowly': 978, 'small': 979, 'smile': 980, 'smiling': 981, 'sneer': 982, 'so': 983, 'solace': 984, 'some': 985, 'somebody': 986, 'something': 987, 'spacious': 988, 'spaniel': 989, 'speaking-tubes': 990, 'speculations': 991, 'spite': 992, 'splash': 993, 'square': 994, 'stairs': 995, 'stammer': 996, 'stand': 997, 'standing': 998, 'started': 999, 'stay': 1000, 'still': 1001, 'stocked': 1002, 'stood': 1003, 'stopped': 1004, 'stopping': 1005, 'straddling': 1006, 'straight': 1007, 'strain': 1008, 'straining': 1009, 'strange': 1010, 'straw': 1011, 'stream': 1012, 'stroke': 1013, 'strokes': 1014, 'strolled': 1015, 'struck': 1016, 'studio': 1017, 'stuff': 1018, 'subject': 1019, 'substantial': 1020, 'suburban': 1021, 'such': 1022, 'suddenly': 1023, 'suffered': 1024, 'sugar': 1025, 'suggested': 1026, 'sunburn': 1027, 'sunburnt': 1028, 'sunlit': 1029, 'superb': 1030, 'sure': 1031, 'surest': 1032, 'surface': 1033, 'surprise': 1034, 'surprised': 1035, 'surrounded': 1036, 'suspected': 1037, 'sweetness': 1038, 'swelling': 1039, 'swept': 1040, 'swum': 1041, 'table': 1042, 'take': 1043, 'taken': 1044, 'talking': 1045, 'tea': 1046, 'tears': 1047, 'technicalities': 1048, 'tell': 1049, 'tells': 1050, 'tempting': 1051, 'terra-cotta': 1052, 'terrace': 1053, 'terraces': 1054, 'terribly': 1055, 'than': 1056, 'that': 1057, 'the': 1058, 'their': 1059, 'them': 1060, 'then': 1061, 'there': 1062, \"there's\": 1063, 'therefore': 1064, 'they': 1065, \"they're\": 1066, 'thin': 1067, 'thing': 1068, 'things': 1069, 'things?': 1070, 'think': 1071, 'this': 1072, 'thither': 1073, 'those': 1074, 'though': 1075, 'thought': 1076, 'three': 1077, 'threshold': 1078, 'threw': 1079, 'through': 1080, 'throwing': 1081, 'tie': 1082, 'till': 1083, 'time': 1084, 'timorously': 1085, 'tinge': 1086, 'tips': 1087, 'tired': 1088, 'to': 1089, 'told': 1090, 'tone': 1091, 'tones': 1092, 'too': 1093, 'too?': 1094, 'took': 1095, 'tottering': 1096, 'touched': 1097, 'toward': 1098, 'trace': 1099, 'trade': 1100, 'transmute': 1101, 'traps': 1102, 'travelled': 1103, 'tribute': 1104, 'tributes': 1105, 'tricks': 1106, 'tried': 1107, 'trouser-presses': 1108, 'true': 1109, 'truth': 1110, 'turned': 1111, 'twenty': 1112, 'twenty-four': 1113, 'twice': 1114, 'twirling': 1115, 'unaccountable': 1116, 'uncertain': 1117, 'under': 1118, 'underlay': 1119, 'underneath': 1120, 'understand': 1121, 'unexpected': 1122, 'untouched': 1123, 'unusual': 1124, 'up': 1125, 'up\"': 1126, 'up-stream': 1127, 'upon': 1128, 'upset': 1129, 'upstairs': 1130, 'us': 1131, 'used': 1132, 'usual': 1133, 'value': 1134, 'varnishing': 1135, 'vases': 1136, 'veins': 1137, 'velveteen': 1138, 'very': 1139, 'villa': 1140, 'vindicated': 1141, 'virtuosity': 1142, 'vista': 1143, 'vocation': 1144, 'voice': 1145, 'wall': 1146, 'wander': 1147, 'want': 1148, 'wanted': 1149, 'wants': 1150, 'was': 1151, \"wasn't\": 1152, 'watched': 1153, 'watching': 1154, 'water-colour': 1155, 'waves': 1156, 'way': 1157, 'weekly': 1158, 'weeks': 1159, \"weeks'\": 1160, 'welcome': 1161, 'went': 1162, 'were': 1163, 'what': 1164, 'when': 1165, 'whenever': 1166, 'where': 1167, 'which': 1168, 'while': 1169, 'white': 1170, 'white-panelled': 1171, 'who': 1172, 'whole': 1173, 'whom': 1174, 'why': 1175, 'wide': 1176, 'widow': 1177, 'wife': 1178, \"wife's\": 1179, 'wild': 1180, 'wincing': 1181, 'window-curtains': 1182, 'wish': 1183, 'with': 1184, 'without': 1185, \"wits'\": 1186, 'woman': 1187, 'women': 1188, \"won't\": 1189, 'wonder': 1190, 'wondered': 1191, 'word': 1192, 'work': 1193, 'working?': 1194, 'worth': 1195, 'would': 1196, \"wouldn't\": 1197, 'year': 1198, 'years': 1199, 'yellow': 1200, 'yet': 1201, 'you': 1202, \"you'd\": 1203, \"you're\": 1204, 'you?\"': 1205, 'younger': 1206, 'your': 1207, 'yourself': 1208}\n"
     ]
    }
   ],
   "source": [
    "token_id = {token:i for i,token in enumerate(words)}\n",
    "print(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e367cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tik version =  0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tik version = \", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c8d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# Byte Pair Encoding ---------- BPE is done using tiktoken library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74482610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28254, 5439, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 19606, 8812, 2114, 1659, 617, 20035, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Helllo, do you like tea? <|endoftext|> In the sunlight terraces\"\n",
    "        \"of someUnknownPlace.\")\n",
    "\n",
    "interger = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(interger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04549de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo, do you like tea? <|endoftext|> In the sunlight terracesof someUnknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(interger)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9173daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input-target pair\n",
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee031f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(data)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2362e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ------->  H\n",
      "I H -------> AD\n",
      "I HAD ------->  always\n",
      "I HAD always ------->  thought\n",
      "I HAD always thought ------->  Jack\n",
      "I HAD always thought Jack ------->  G\n",
      "I HAD always thought Jack G -------> is\n",
      "I HAD always thought Jack Gis -------> burn\n",
      "I HAD always thought Jack Gisburn ------->  rather\n",
      "I HAD always thought Jack Gisburn rather ------->  a\n",
      "I HAD always thought Jack Gisburn rather a ------->  cheap\n",
      "I HAD always thought Jack Gisburn rather a cheap ------->  genius\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -------> --\n",
      "I HAD always thought Jack Gisburn rather a cheap genius-- -------> though\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though ------->  a\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a ------->  good\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good ------->  fellow\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow ------->  enough\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough -------> --\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough-- -------> so\n"
     ]
    }
   ],
   "source": [
    "sample_context = 20\n",
    "for i in range(1, sample_context+1):\n",
    "    context = enc_text[:i]\n",
    "    desire = enc_text[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"------->\", tokenizer.decode([desire]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14950503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Creating a input output pair\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "    # Using sliding window to chuck block into overlapping sequence of max_len \n",
    "        for i in range (0, len(token_ids)-max_len, stride):\n",
    "            input_chuck = token_ids[i : i + max_len]\n",
    "            target_chuck = token_ids[i + 1 : i + max_len + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chuck))\n",
    "            self.target_ids.append(torch.tensor(target_chuck))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloader(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride) #create dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = shuffle, drop_last = drop_last, num_workers = num_workers)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#with open(\"the-verdict.txt\" , 'r') as file:\n",
    "#    data = file.read()      Run this to get the data\n",
    "\n",
    "\n",
    "dataloader = create_dataloader(data, batch_size=1, max_len=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "# [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])] ----->> it gives input_tensor and output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d1da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatiing Token Embedding\n",
    "\n",
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d694ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  1.3337,  0.0771, -0.0522],\n",
      "        [ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [-1.3152, -0.0677, -0.1350,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        ...,\n",
      "        [ 0.5871, -0.0572, -1.1628,  ..., -0.6887, -0.7364,  0.4479],\n",
      "        [ 0.4438,  0.7411,  1.1263,  ...,  1.2091,  0.6781,  0.3331],\n",
      "        [-0.2537,  0.1446,  0.7203,  ..., -0.2134,  0.2144,  0.3006]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # Here number of rows is equal to vocab_size\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb8a61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Id : \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input Shape :  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader(data, batch_size=8, max_len=max_len, stride=max_len, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token Id : \\n\", inputs)\n",
    "print(\"Input Shape : \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d571b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding = embedding_layer(inputs)\n",
    "print(token_embedding.shape) # This will give 3 dimensional matrix which formed by embedding vector for all the 8*4 so output is ------>> 8*4*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f40a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create another embedding layer for positional encoding\n",
    "# Here the number of row is context_length\n",
    "context_length = max_len\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) # This creates the embedding layer for positional  encoding\n",
    "\n",
    "pos_embedding  = pos_embedding_layer(torch.arange(max_len))\n",
    "print(pos_embedding.shape)\n",
    "\n",
    "# This encodes the data to positional encoding to form size of 4*256\n",
    "# Why we need to encode to get 4*256? ----  Because we are encoding positional encode and we want to know which position does input present\n",
    "# So we get 4*256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a3983eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embedding = token_embedding + pos_embedding\n",
    "print(input_embedding.shape)\n",
    "\n",
    "# As total embedding of positional embedding is addidtion of token embedding and unquie positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07b2ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing simple attention mechanism\n",
    "import torch\n",
    "input_sam = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],   #your   (x^1)\n",
    "    [0.55, 0.87, 0.66],    #journey (x^2)\n",
    "    [0.57, 0.85, 0.64],    #starts  (x^3)\n",
    "    [0.22, 0.58, 0.33],    #with  (x^4)\n",
    "    [0.77, 0.25, 0.10],    #one (x^5)\n",
    "    [0.05, 0.80, 0.55]])   #step (x^6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9967a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for Visualizing the attention of words in 3D graph\n",
    "# not necessary to use. \n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # corresponding words\n",
    "\n",
    "# words_sam = ['Your','journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "# # Extract x, y, z coordinates\n",
    "# x_coords = input_sam[: , 0].numpy()\n",
    "# y_coords = input_sam[: , 1].numpy()\n",
    "# z_coords = input_sam[: , 2].numpy()\n",
    "\n",
    "# # creating 3D plot\n",
    "# fig = plt.Figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for x,y,z,word in zip(x_coords,y_coords, z_coords, words_sam):\n",
    "#     ax.scatter(x,y,z)\n",
    "#     ax.text(x,y,z,word, fontsize = 10)\n",
    "\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "# ax.set_zlabel(\"z\")\n",
    "\n",
    "# plt.title(\"3D plot of word Embedding\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a31659a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Attention Score with only 2nd element\n",
    "query = input_sam[1]\n",
    "attn_score_2 = torch.empty(input_sam.shape[0])\n",
    "for i,x_i in enumerate(input_sam):\n",
    "    attn_score_2[i] = torch.dot(x_i,query) # dot product (transpose not necessary)\n",
    "\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90012c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights :  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum :  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# normalization of attention score\n",
    "\n",
    "attn_weight_2_tmp = attn_score_2/attn_score_2.sum()\n",
    "\n",
    "print(\"Attention weights : \", attn_weight_2_tmp)\n",
    "print(\"Sum : \", attn_weight_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23bc629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights :  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Using Navie Softmax for normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weight_2_naive = softmax_naive(attn_score_2)\n",
    "\n",
    "print(\"Attention weights : \", attn_weight_2_naive)\n",
    "print(\"Sum : \", attn_weight_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c632acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights :  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch implementation of softmax\n",
    "\n",
    "attn_weight_2 = torch.softmax(attn_score_2, dim=0) # Attention weights\n",
    "print(\"Attention weights : \", attn_weight_2)\n",
    "print(\"Sum : \", attn_weight_2.sum())\n",
    "\n",
    "# Naive softmax may encounter numerical instability problems such as overflow and underflow\n",
    "# It is always advisable to use Pytorch softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d19f55e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# context vector implementation\n",
    "query = input_sam[1]\n",
    "\n",
    "context_vector = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(input_sam):\n",
    "    context_vector += attn_weight_2[i]*x_i\n",
    "\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Efficient way for attention score\n",
    "\n",
    "attn_score = input_sam @ input_sam.T # It does dot product by multiplying input*input(transpose)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weight = torch.softmax(attn_score, dim=-1) # Attention weight for all the elements\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vectors = attn_weight @ input_sam # All elements context vectors\n",
    "print(all_context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02407b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# self attention mechanism with query key and value\n",
    "\n",
    "x_2 = input_sam[1]\n",
    "d_in = input_sam.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "w_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "w_key = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "w_value = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "\n",
    "print(w_query,\"\\n\")\n",
    "print(w_key,\"\\n\")\n",
    "print(w_value,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c743e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551]) \n",
      "\n",
      "tensor([0.4433, 1.1419]) \n",
      "\n",
      "tensor([0.3951, 1.0037]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Key Value for input_sam[1]\n",
    "\n",
    "query_2 = x_2 @ w_query\n",
    "key_2 = x_2 @ w_key\n",
    "value_2 = x_2 @ w_value\n",
    "\n",
    "print(query_2,\"\\n\")\n",
    "print(key_2,\"\\n\")\n",
    "print(value_2,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2401725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]]) \n",
      "\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]]) \n",
      "\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Key Value using input_sam\n",
    "\n",
    "query= input_sam @ w_query\n",
    "key = input_sam @ w_key\n",
    "value = input_sam @ w_value\n",
    "\n",
    "print(query,\"\\n\")\n",
    "print(key,\"\\n\")\n",
    "print(value,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7869964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score 22 \n",
      " tensor(1.8524) \n",
      "\n",
      "Attention score 2 \n",
      " tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440]) \n",
      "\n",
      "Attention score \n",
      " tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "key_2 = key[1]\n",
    "attn_score_22 = query_2.dot(key_2)\n",
    "print(\"Attention score 22 \\n\",attn_score_22,\"\\n\")\n",
    "\n",
    "attn_score_2 = query_2 @ key.T # All attention score for given query   Here query is input_sam[2]\n",
    "print(\"Attention score 2 \\n\",attn_score_2,\"\\n\")\n",
    "\n",
    "attn_score = query @ key.T # omega\n",
    "print(\"Attention score \\n\", attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46cd736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = key.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2/ d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d03f288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3069, 0.8188])\n"
     ]
    }
   ],
   "source": [
    "# sample context vector with weights, values and keys\n",
    "context_vec_2 = attn_weight_2 @ value \n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bd3290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Modular self attention implementation for full input\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self,x):\n",
    "        key = x @ self.w_key    \n",
    "        query = x @ self.w_query\n",
    "        value = x @ self.w_value\n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ value\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)       # This is the context vector of input_sam\n",
    "print(sa_v1(input_sam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9abbd2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced version of self attention \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ value\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)       # This is the context vector of input_sam in an enhanced way\n",
    "print(sa_v2(input_sam)) \n",
    "\n",
    "# version 1 and version 2 are completely different because they both use different initialization\n",
    "# version 1 ---> nn.Parameter        version 2 ---> nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c0eec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.w_query(input_sam)\n",
    "keys = sa_v2.w_key(input_sam)\n",
    "attention_score = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_score/keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5cb7907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_score.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) # tril gives lower triangular matrix    triu gives upper triangular matrix\n",
    "print(mask_simple,\"\\n\")\n",
    "\n",
    "masked_simple = attention_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "66900df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# normalizing masked_simple to make the sum of rows is equal to 1\n",
    "row_sum = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_normalize = masked_simple/row_sum\n",
    "print(masked_simple_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "846e1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiently Masked \n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>) \n",
      "\n",
      "Efficiennt attention weight after masked \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Efficient masking\n",
    "mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
    "masked = attention_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Efficiently Masked \\n\",masked, \"\\n\")  # It gives masked matrix\n",
    "\n",
    "attention_weights_eff = torch.softmax(masked/key.shape[-1]**0.5,dim=1)\n",
    "print(\"Efficiennt attention weight after masked \\n\", attention_weights_eff) # Softmax after masked will give us normalized attention weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92286f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dropout for reducing thw lazy neurons \n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "# example = torch.ones(6,6)  example\n",
    "# print(dropout(example))\n",
    "print(dropout(attention_weights_eff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3476be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of context vector for causal attention \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Implementation of causal mechanism \n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self,d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        attn_scores = query @ key.transpose(1,2)\n",
    "        attn_score.masked_fill(self.mask.bool() [:num_token, :num_token], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vector_causal = attn_weights @ value\n",
    "        return context_vector_causal\n",
    "\n",
    "batch = torch.stack((input_sam, input_sam), dim=0) # shape of batch is [2,6,3]\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "causalAttention = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vec_causal = causalAttention(batch)\n",
    "print(\"Shape of context vector for causal attention \\n\", context_vec_causal.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73eb9fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector of Multi head Attention \n",
      " tensor([[[-0.5337, -0.1051,  0.5085,  0.3508],\n",
      "         [-0.5323, -0.1080,  0.5084,  0.3508],\n",
      "         [-0.5323, -0.1079,  0.5084,  0.3506],\n",
      "         [-0.5297, -0.1076,  0.5074,  0.3471],\n",
      "         [-0.5311, -0.1066,  0.5076,  0.3446],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.5337, -0.1051,  0.5085,  0.3508],\n",
      "         [-0.5323, -0.1080,  0.5084,  0.3508],\n",
      "         [-0.5323, -0.1079,  0.5084,  0.3506],\n",
      "         [-0.5297, -0.1076,  0.5074,  0.3471],\n",
      "         [-0.5311, -0.1066,  0.5076,  0.3446],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>) \n",
      "\n",
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Implementation Multi head attentioon mechanism\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_head, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in,d_out,context_length,dropout,qkv_bias)\n",
    "             for _ in range (num_head)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((input_sam, input_sam), dim=0) \n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "multihead = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_head=2)\n",
    "context_vec_multihead = multihead(batch)\n",
    "print(\"Context Vector of Multi head Attention \\n\", context_vec_multihead, \"\\n\")\n",
    "print(context_vec_multihead.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "380f7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector for Multihead Attention \n",
      " tensor([[[ 0.1195, -0.0484,  0.0306, -0.0639, -0.2782, -0.2564],\n",
      "         [ 0.1208, -0.0497,  0.0319, -0.0638, -0.2779, -0.2566],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1195, -0.0484,  0.0306, -0.0639, -0.2782, -0.2564],\n",
      "         [ 0.1208, -0.0497,  0.0319, -0.0638, -0.2779, -0.2566],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>) \n",
      "\n",
      "Context Vector shape \n",
      " torch.Size([2, 3, 6]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Multi head attention mechanism with weight split\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_head, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_head == 0),\\\n",
    "            \"d_out must be divisible by num_head\"\n",
    "        self.d_out = d_out\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_out//num_head # Reduce the production dim to match the desire output dim\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_in,d_out) # linear projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        # We implicity split the matrix by adding a num_head dimension \n",
    "        # unroll last dim : (b, num_token, d_out) -> (b, num_token, num_head, head_dim)\n",
    "        key = key.view(b, num_token, self.num_head, self.head_dim)\n",
    "        value = value.view(b, num_token, self.num_head, self.head_dim)\n",
    "        query = query.view(b, num_token, self.num_head, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_token, num_head, head_dim) -> (b, num_head, num_token, head_dim)\n",
    "        key = key.transpose(1,2)\n",
    "        query = query.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        # Compute scaled dot product attention aka self attention with causal mask\n",
    "        attention_score = query @ key.transpose(2,3)\n",
    "\n",
    "        # original mask truncated to the number of tokens and convert to boolean \n",
    "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
    "\n",
    "        attention_score.masked_fill(mask_bool, -torch.inf) # masking the attention score\n",
    "\n",
    "        attention_weight = torch.softmax(attention_score/key.shape[-1]**0.5, dim = -1)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        context_vector_multihead = (attention_weight @ value).transpose(1,2)\n",
    "\n",
    "        # Combine head, where self.d_out = self.num_head * self.head_dim\n",
    "        context_vector_multihead = context_vector_multihead.contiguous().view(b, num_token, self.d_out)\n",
    "        context_vector_multihead = self.out_proj(context_vector_multihead) # Optional projection\n",
    "\n",
    "        return context_vector_multihead\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "    [0.43, 0.15, 0.89, 0.55, 0.87, 0.66],    \n",
    "    [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],    \n",
    "    [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]\n",
    "    ])\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "multiheadAttentionComplete = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_head=2)\n",
    "context_vector = multiheadAttentionComplete(batch)\n",
    "print(\"Context Vector for Multihead Attention \\n\", context_vector,\"\\n\")\n",
    "print(\"Context Vector shape \\n\", context_vector.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0762078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT model from scratch to generate text\n",
    "\n",
    "GPT_Config_124m = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,    # Embedding Dimension\n",
    "    \"n_heads\" : 12,  # Numbers of attention head\n",
    "    \"n_layers\" : 12,    # Number of layers\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "90937982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size : \n",
      " tensor([[6109, 4040, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]]) \n",
      "\n",
      "Output shape : \n",
      " torch.Size([2, 4, 50257]) \n",
      "\n",
      "Logits : \n",
      " tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.9125,  1.5573, -0.5135,  ..., -0.3851,  0.6555,  1.5411],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6755, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementing Dummy GPT model\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DummyGPT(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        # Using placeholder for transformer block\n",
    "        self.trf_blocks = nn.Sequential(*[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        # Use placeholder for layer Normalisation\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_index):\n",
    "        batch_size, seq_len = in_index.shape\n",
    "        tok_embeds = self.tok_emb(in_index)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_index.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x) \n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "    \n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # A simple placeholder\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x # this block does nothing and returns its input\n",
    "    \n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps = 1e-5):\n",
    "        super().__init__()\n",
    "        # parameter here are just to mimic the Layernorm interface\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x  # this layer does nothing and returns its input\n",
    "    \n",
    "\n",
    "\n",
    "    # *Both DummyLayerNorm and DummyTransformerBlock doesn't have any parameters or code as it is a dummy GPT*\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "text1  = \"Every efforts moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"batch size : \\n\", batch, \"\\n\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = DummyGPT(GPT_Config_124m)\n",
    "logits = model(batch)\n",
    "print(\"Output shape : \\n\", logits.shape,\"\\n\")\n",
    "print(\"Logits : \\n\", logits, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c79925f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>) \n",
      "\n",
      "mean \n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "variance \n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>) \n",
      "\n",
      "norm mean \n",
      " tensor([[9.9341e-09],\n",
      "        [0.0000e+00]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "norm variance \n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>) \n",
      "\n",
      "normalize \n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# layer normalization\n",
    "torch.manual_seed(123)\n",
    "batch_example = torch.randn(2,5)\n",
    "layer = nn.Sequential(nn.Linear(5,6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out, \"\\n\")\n",
    "\n",
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"mean \\n\", mean,\"\\n\")\n",
    "print(\"variance \\n\", var,\"\\n\")\n",
    "\n",
    "\n",
    "out_norm = (out - mean)/ torch.sqrt(var)\n",
    "new_mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "new_var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"norm mean \\n\", new_mean,\"\\n\")\n",
    "print(\"norm variance \\n\", new_var,\"\\n\")\n",
    "print(\"normalize \\n\", out_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fd46a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.9802e-08],\n",
      "        [ 0.0000e+00]], grad_fn=<MeanBackward1>) \n",
      "\n",
      "tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# class for layer normalization\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased = False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "    \n",
    "ln = LayerNormalization(emb_dim=5)\n",
    "out_ln = ln(batch_example)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, keepdim=True, unbiased = False)\n",
    "print(mean,\"\\n\")\n",
    "print(var,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bf6cc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GELU activation function demonstrationn and comparision with RELU activation function.\n",
    "\n",
    "# We use GeLU activation function than ReLU in LLM because ReLU ignore negative values and take only maximum value\n",
    "# GeLU take both negative and positive values and give output. This is demonstrated below using a graph.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))* (x + 0.044715 * torch.pow(x, 3))))\n",
    "    \n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"RELU\"]),1):\n",
    "    plt.subplot(1,2,i)\n",
    "    plt.plot(x,y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a0e40ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output : \n",
      " torch.Size([2, 3, 768]) \n",
      "\n",
      "Output : \n",
      " tensor([[[-0.3731, -0.2161,  0.1972,  ..., -0.2462,  0.0535,  0.2413],\n",
      "         [ 0.0069,  0.0609,  0.3952,  ...,  0.1626, -0.0415, -0.1237],\n",
      "         [ 0.1569, -0.1565, -0.0789,  ..., -0.3007,  0.2389, -0.1702]],\n",
      "\n",
      "        [[ 0.2887,  0.0783,  0.1038,  ..., -0.2605, -0.0504, -0.2268],\n",
      "         [-0.0889,  0.2274,  0.0563,  ..., -0.2062,  0.0148, -0.2420],\n",
      "         [ 0.2520, -0.0005, -0.2848,  ..., -0.0739, -0.0354,  0.0410]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Feed forward neural network\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(  # Structure of feedforward neural network is linear -> GeLU -> Linear\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]), # Using GPT configuration for FeedForward neural network\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "ff = FeedForward(GPT_Config_124m)\n",
    "x = torch.randn(2,3,768)\n",
    "out_ff = ff(x)\n",
    "print(\"Shape of output : \\n\",out_ff.shape,\"\\n\")\n",
    "print(\"Output : \\n\", out_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "622dbf70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173587836325169 \n",
      "layers.1.0.weight has gradient mean of 0.0001201116101583466 \n",
      "layers.2.0.weight has gradient mean of 0.0007152041653171182 \n",
      "layers.3.0.weight has gradient mean of 0.001398873864673078 \n",
      "layers.4.0.weight has gradient mean of 0.005049646366387606 \n",
      "None \n",
      "\n",
      "layers.0.0.weight has gradient mean of 0.2216978669166565 \n",
      "layers.1.0.weight has gradient mean of 0.20694100856781006 \n",
      "layers.2.0.weight has gradient mean of 0.3289698660373688 \n",
      "layers.3.0.weight has gradient mean of 0.2665731906890869 \n",
      "layers.4.0.weight has gradient mean of 1.3258538246154785 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Shortcut Connection implementation\n",
    "\n",
    "class ExamplDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_size, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_size[0], layer_size[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[1], layer_size[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[2], layer_size[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[3], layer_size[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_size[4], layer_size[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            layer_out = layer(x) # Compute output of the current layer\n",
    "            if self.use_shortcut and x.shape == layer_out.shape: # Checks whether shortcut can be applied\n",
    "                x = x + layer_out\n",
    "            else:\n",
    "                x = layer_out\n",
    "        return x\n",
    "    \n",
    "layer_size = [3,3,3,3,3,1]\n",
    "sample_input = torch.tensor([[1.,0.,-1.]])\n",
    "torch.manual_seed(123)\n",
    "model_without_shorcut = ExamplDeepNeuralNetwork(layer_size, use_shortcut=False)\n",
    "torch.manual_seed(123) # It is used for every class instance\n",
    "model_with_shorcut = ExamplDeepNeuralNetwork(layer_size, use_shortcut=True)\n",
    "\n",
    "def print_gradient(model, x):\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # Calculate loss to find how close the output and the target\n",
    "    \n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()} \")\n",
    "print(print_gradient(model_without_shorcut,sample_input),\"\\n\")\n",
    "print(print_gradient(model_with_shorcut, sample_input))\n",
    "\n",
    "# This the demonstratipon for shortcut connection which keeps the gradient to flow without gradient vanishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0f15e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Shape : \n",
      " torch.Size([2, 4, 768]) \n",
      "\n",
      "Output : \n",
      " tensor([[[ 0.1698,  0.1784,  0.2471,  ...,  0.8822,  0.4130,  0.5059],\n",
      "         [-0.0217, -0.0910,  0.1543,  ...,  0.5084,  0.5222,  0.5250],\n",
      "         [ 0.4345,  0.3351, -0.0045,  ...,  1.0532,  0.3839,  0.7335],\n",
      "         [ 0.0626,  0.7235,  0.8617,  ...,  0.4536,  0.7321,  0.6908]],\n",
      "\n",
      "        [[ 0.6442,  0.8567,  0.7715,  ...,  0.2118,  0.1566,  0.0600],\n",
      "         [-0.0574,  0.4658,  0.2350,  ...,  0.1011,  0.5168,  0.1547],\n",
      "         [ 0.8730,  0.4320,  0.0860,  ...,  0.4330,  0.5711,  0.1511],\n",
      "         [ 0.4679,  0.5837,  0.4073,  ...,  1.1564,  1.2291,  0.1967]]],\n",
      "       grad_fn=<AddBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Complete Transformer Block Implementation\n",
    "\n",
    "GPT_Config_124m = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,    # Embedding Dimension\n",
    "    \"n_heads\" : 12,  # Numbers of attention head\n",
    "    \"n_layers\" : 12,    # Number of layers\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased = False)\n",
    "        norm_x = (x-mean)/torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))* (x + 0.044715 * torch.pow(x, 3))))\n",
    " \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layer = nn.Sequential(  # Structure of feedforward neural network is linear -> GeLU -> Linear\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]), # Using GPT configuration for FeedForward NN  This line is Expansion layer of feedforward NN\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"],cfg[\"emb_dim\"]), # This line is contraction layer of FeedForward NN\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "# Implementation of Multi head attention mechanism with weight split\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_head, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_head == 0),\\\n",
    "            \"d_out must be divisible by num_head\"\n",
    "        self.d_out = d_out\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_out//num_head # Reduce the production dim to match the desire output dim\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_in,d_out) # linear projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        # We implicity split the matrix by adding a num_head dimension \n",
    "        # unroll last dim : (b, num_token, d_out) -> (b, num_token, num_head, head_dim)\n",
    "        key = key.view(b, num_token, self.num_head, self.head_dim)\n",
    "        value = value.view(b, num_token, self.num_head, self.head_dim)\n",
    "        query = query.view(b, num_token, self.num_head, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_token, num_head, head_dim) -> (b, num_head, num_token, head_dim)\n",
    "        key = key.transpose(1,2)\n",
    "        query = query.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        # Compute scaled dot product attention aka self attention with causal mask\n",
    "        attention_score = query @ key.transpose(2,3)\n",
    "\n",
    "        # original mask truncated to the number of tokens and convert to boolean \n",
    "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
    "\n",
    "        attention_score.masked_fill(mask_bool, -torch.inf) # masking the attention score\n",
    "\n",
    "        attention_weight = torch.softmax(attention_score/key.shape[-1]**0.5, dim = -1)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        context_vector_multihead = (attention_weight @ value).transpose(1,2)\n",
    "\n",
    "        # Combine head, where self.d_out = self.num_head * self.head_dim\n",
    "        context_vector_multihead = context_vector_multihead.contiguous().view(b, num_token, self.d_out)\n",
    "        context_vector_multihead = self.out_proj(context_vector_multihead) # Optional projection\n",
    "\n",
    "        return context_vector_multihead\n",
    "\n",
    "# Now let us implement the entire transformer block.....\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_head=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Shortcut Connection for attention block\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.attention(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        # Shortcut connection for FeedForward NN\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "        return x\n",
    "\n",
    "torch.manual_seed(123)\n",
    "x = torch.rand(2, 4, 768)\n",
    "block = TransformerBlock(GPT_Config_124m)\n",
    "output = block(x)\n",
    "print(\"Output Shape : \\n\", output.shape,\"\\n\")\n",
    "print(\"Output : \\n\", output, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6443ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size : \n",
      " tensor([[6109, 4040, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]]) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape : \n",
      " torch.Size([2, 4, 50257]) \n",
      "\n",
      "Logits : \n",
      " tensor([[[ 0.5167, -0.0350, -0.2424,  ...,  0.1139,  0.1244, -0.2865],\n",
      "         [ 1.3569, -0.5268,  0.0245,  ..., -0.1971,  0.2882,  0.1781],\n",
      "         [ 1.2166, -0.1139, -0.2749,  ...,  0.0964, -0.5610, -0.2167],\n",
      "         [-0.8211,  0.3125, -0.2892,  ...,  0.5673,  0.3373,  0.2216]],\n",
      "\n",
      "        [[-0.4464, -0.1647, -0.1758,  ...,  0.3376,  0.1688, -0.2383],\n",
      "         [ 0.2094,  0.3176, -0.1318,  ...,  0.6261,  0.0143,  0.5881],\n",
      "         [ 1.1166,  0.6801, -0.3057,  ...,  0.8567,  0.2679, -0.3500],\n",
      "         [ 0.0117,  0.3627,  0.2767,  ...,  1.2365, -0.0898, -0.0242]]],\n",
      "       grad_fn=<UnsafeViewBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of 124M version of GPT-2\n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "    \n",
    "    \n",
    "    def forward(self, in_index):\n",
    "        batch_size, seq_len = in_index.shape\n",
    "        tok_embeds = self.tok_emb(in_index)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_index.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x) \n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "GPT_Config_124m = {\n",
    "    \"vocab_size\" : 50257,\n",
    "    \"context_length\" : 1024,\n",
    "    \"emb_dim\" : 768,    # Embedding Dimension\n",
    "    \"n_heads\" : 12,  # Numbers of attention head\n",
    "    \"n_layers\" : 12,    # Number of layers\n",
    "    \"drop_rate\" : 0.1,\n",
    "    \"qkv_bias\" : False\n",
    "}\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "text1  = \"Every efforts moves you\"\n",
    "text2 = \"Every day holds a\"\n",
    "batch.append(torch.tensor(tokenizer.encode(text1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(text2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(\"batch size : \\n\", batch, \"\\n\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "gptmodel = GPTModel(GPT_Config_124m)\n",
    "out = gptmodel(batch)\n",
    "print(\"Output shape : \\n\", out.shape,\"\\n\")\n",
    "print(\"Logits : \\n\", out, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "95ee1246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded :  [15496, 11, 314, 716]\n",
      "Ecoded tensor shape :  torch.Size([1, 4]) \n",
      "\n",
      "Encoded Tensor :\n",
      " tensor([[15496,    11,   314,   716]]) \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output : tensor([[15496,    11,   314,   716, 12170, 44251, 25952, 49216, 30322,  6868]])\n",
      "Output Length :  10 \n",
      "\n",
      "Hello, I am drone Omni SSLmyra muc native\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, idx, max_new_token, context_size):\n",
    "    # idx is (batch, n_tokens)\n",
    "\n",
    "    for _ in range(max_new_token):\n",
    "\n",
    "        # Crop current context if it exceeds the supported context size\n",
    "\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "\n",
    "        # Get predictions\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # Focus only on last time step (batch, n_tokens, vocab_size) ---> (batch, vocab_size)\n",
    "\n",
    "        logits = logits[:, -1,:]\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "\n",
    "        probab = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Get the idx of vocab entry with highest probability\n",
    "\n",
    "        idx_next = torch.argmax(probab, dim=-1, keepdim=True)\n",
    "\n",
    "        # Append sampled index to the running sequence\n",
    "\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "    return idx\n",
    "\n",
    "start_context = \"Hello, I am\"\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded : \", encoded)\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"Ecoded tensor shape : \", encoded_tensor.shape,\"\\n\")\n",
    "print(\"Encoded Tensor :\\n\", encoded_tensor,\"\\n\")\n",
    "\n",
    "model.eval()\n",
    "out = generate_text(model=model, idx=encoded_tensor, max_new_token=6, context_size=GPT_Config_124m[\"context_length\"])\n",
    "print(\"Output :\", out)\n",
    "print(\"Output Length : \", len(out[0]),\"\\n\")\n",
    "\n",
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0d6df801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output text:\n",
      " Every efforts moves you horizontRoomki laserutfucks Levels Nice Sir dentist\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "\n",
    "def text_to_token(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)  # FIXED: add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_to_text(token_ids, tokenizer):\n",
    "    return tokenizer.decode(token_ids.squeeze(0).tolist())  # FIXED: flatten properly\n",
    "\n",
    "start_context = \"Every efforts moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text(\n",
    "    model=model, \n",
    "    idx=text_to_token(start_context, tokenizer), \n",
    "    max_new_token=10, \n",
    "    context_size=GPT_Config_124m[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"output text:\\n\", token_to_text(token_ids, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab864df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n",
      "Token ID: \n",
      " tensor([[[25388],\n",
      "         [41068],\n",
      "         [49272]],\n",
      "\n",
      "        [[ 2109],\n",
      "         [ 3957],\n",
      "         [30269]]]) \n",
      "\n",
      "Text 1: tensor([4.8134e-06, 5.9264e-06, 3.4793e-06]) \n",
      "\n",
      "Text 2: tensor([1.2831e-05, 2.4681e-05, 1.0381e-05])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-12.2441, -12.0361, -12.5687, -11.2636, -10.6095, -11.4755])\n",
      "tensor(-11.6996)\n",
      "tensor(11.6996)\n",
      "Logits shape : torch.Size([2, 3, 50257])\n",
      "target shape : torch.Size([2, 3])\n",
      "Flattened logits : torch.Size([6, 50257])\n",
      "Flattened target : torch.Size([6])\n",
      "loss :  tensor(11.6996)\n",
      "perplexity : tensor(120521.5703)\n"
     ]
    }
   ],
   "source": [
    "input = torch.tensor([[16833, 3626, 6100],\n",
    "                      [40, 1107, 588]])\n",
    "\n",
    "target = torch.tensor([[3626, 6100, 345],\n",
    "                      [1107, 588, 11311]])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(input)\n",
    "\n",
    "probab = torch.softmax(logits, dim=-1)\n",
    "print(probab.shape)\n",
    "\n",
    "token_ids = torch.argmax(probab, dim=-1, keepdim=True)\n",
    "print(\"Token ID: \\n\", token_ids, \"\\n\")\n",
    "\n",
    "# Cross ENtropy Loss\n",
    "\n",
    "text_idx = 0\n",
    "target_probab_1 = probab[text_idx, [0,1,2], target[text_idx]]\n",
    "print(\"Text 1:\", target_probab_1,\"\\n\")\n",
    "\n",
    "text_idx = 1\n",
    "target_probab_2 = probab[text_idx, [0,1,2], target[text_idx]]\n",
    "print(\"Text 2:\", target_probab_2)\n",
    "\n",
    "log_probab = torch.log(torch.cat((target_probab_1,target_probab_2))) # Taking log\n",
    "print(log_probab)\n",
    "\n",
    "avg_log_probab = torch.mean(log_probab) # taking Average\n",
    "print(avg_log_probab)\n",
    "\n",
    "neg_avg_log_probab = avg_log_probab * -1\n",
    "print(neg_avg_log_probab)\n",
    "\n",
    "print(\"Logits shape :\", logits.shape)\n",
    "print(\"target shape :\", target.shape)\n",
    "\n",
    "logit_flatten = logits.flatten(0,1)\n",
    "target_flat = target.flatten()\n",
    "\n",
    "print(\"Flattened logits :\", logit_flatten.shape)\n",
    "print(\"Flattened target :\", target_flat.shape)\n",
    "\n",
    "# Main cross entropy \n",
    "\n",
    "loss = torch.nn.functional.cross_entropy(logit_flatten, target_flat)\n",
    "print(\"loss : \", loss)\n",
    "\n",
    "perplexity = torch.exp(loss)\n",
    "print(\"perplexity :\", perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f62f10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n",
      "5145\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()\n",
    "# print(data)\n",
    "total_character = len(data)\n",
    "total_token = len(tokenizer.encode(data))\n",
    "print(total_character)\n",
    "print(total_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "27dab0c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss : 10.998909950256348 \n",
      "\n",
      "Validation loss : 11.013081550598145\n"
     ]
    }
   ],
   "source": [
    "# Creating a input output pair\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "    # Using sliding window to chuck block into overlapping sequence of max_len \n",
    "        for i in range (0, len(token_ids)-max_len, stride):\n",
    "            input_chuck = token_ids[i : i + max_len]\n",
    "            target_chuck = token_ids[i + 1 : i + max_len + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chuck))\n",
    "            self.target_ids.append(torch.tensor(target_chuck))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloader(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride) #create dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = shuffle, drop_last = drop_last, num_workers = num_workers)\n",
    "    return dataloader\n",
    "\n",
    "train_ratio = 0.80\n",
    "split_text = int(train_ratio * len(data))\n",
    "train_data = data[: split_text]\n",
    "val_data = data[split_text :]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataloader = create_dataloader(\n",
    "    data, batch_size=2, \n",
    "    max_len=GPT_Config_124m[\"context_length\"], \n",
    "    stride=GPT_Config_124m[\"context_length\"],\n",
    "    drop_last = True,\n",
    "    shuffle = True, \n",
    "    num_workers=0\n",
    "    )\n",
    "\n",
    "val_data_loader = create_dataloader(\n",
    "    data, batch_size=2, \n",
    "    max_len=GPT_Config_124m[\"context_length\"], \n",
    "    stride=GPT_Config_124m[\"context_length\"],\n",
    "    drop_last = False,\n",
    "    shuffle = False, \n",
    "    num_workers=0\n",
    "    )\n",
    "\n",
    "if total_token * (train_ratio) < GPT_Config_124m[\"context_length\"]:\n",
    "    print(\"Try to increase the training_ratio \")\n",
    "if total_token * (1 - train_ratio) < GPT_Config_124m[\"context_length\"]:\n",
    "    print(\"Try to decrease the training_ratio \")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_Config_124m)\n",
    "\n",
    "def cal_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0,1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def cal_loss_loader(data_loader, model, device, num_batches = None):\n",
    "    total_loss = 0.\n",
    "    if len(train_dataloader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(train_dataloader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(train_dataloader))\n",
    "    \n",
    "    for i, (input_batch, target_batch) in enumerate(train_dataloader):\n",
    "        if i < num_batches:\n",
    "            loss = cal_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss/num_batches\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "with torch.no_grad():\n",
    "    train_loss = cal_loss_loader(train_dataloader, model, device)\n",
    "    val_loss = cal_loss_loader(val_data_loader, model, device)\n",
    "\n",
    "print(\"Training loss :\", train_loss, \"\\n\")\n",
    "print(\"Validation loss :\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423c6c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_simple(model, train_dataloader, val_data_loader, optimizer, device, num_epoch, \n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen , global_step = 0, -1\n",
    "\n",
    "    # Main training loop\n",
    "    for epoch in range(num_epoch):\n",
    "        model.train()\n",
    "\n",
    "        for input_batch, target_batch in train_dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = cal_loss_batch(input_batch, target_batch, model, device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "\n",
    "            # Optional Evalution step\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_dataloader, val_data_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Ep {epoch+1} (step{global_step:06d}): \"\n",
    "                      f\"Train loss {train_loss:.3f}, Validation loss {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def evaluate_model(model, train_dataloader, val_data_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = cal_loss_loader(train_dataloader, model, device, num_batches=eval_iter)\n",
    "        val_loss = cal_loss_loader(val_data_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "def generate_and_print(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0]\n",
    "    encoded = text_to_token(start_context, tokenizer).to(device)\n",
    "    with torch.no_grad():\n",
    "        token_id = generate_text(model=model, idx=encoded, max_new_token=50, context_size=context_size)\n",
    "    decoded_text = token_to_text(token_id,tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \")) \n",
    "\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_Config_124m)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
    "\n",
    "num_epoch = 10\n",
    "train_losses, val_losses, token_seen = train_model_simple(\n",
    "    model, train_dataloader, \n",
    "    val_data_loader, optimizer, \n",
    "    device, num_epoch=num_epoch, \n",
    "    eval_freq=5, \n",
    "    eval_iter=5, \n",
    "    start_context=\"Every efforts moves you\", tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b78705d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8206b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c4ea38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3f7c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
