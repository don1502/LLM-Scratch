{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba4034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa9755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()\n",
    "# data = input(\"Enter your prompt here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8778d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ea2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = re.split(r'([,.<>/!@#$%^&*():;]|--|\\s)',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2558b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This a for loop for separating the text. And below is list comprehension\n",
    "\n",
    "# result = []\n",
    "# for item in text:\n",
    "#     if item.strip():\n",
    "#         result.append(item)\n",
    "    \n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b021d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'I', 'am', 'Don', 'Christ']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is a word based tokenization algorithm. This tokenize for each word.....\n",
    "\n",
    "result = [item.strip() for item in split_text if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec956c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ca9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "words = sorted(set(result))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96156f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Christ': 0, 'Don': 1, 'Hi': 2, 'I': 3, 'am': 4}\n"
     ]
    }
   ],
   "source": [
    "token_id = {token:i for i,token in enumerate(words)}\n",
    "print(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6e367cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tik version =  0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tik version = \", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c8d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74482610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28254, 5439, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 19606, 8812, 2114, 1659, 617, 20035, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Helllo, do you like tea? <|endoftext|> In the sunlight terraces\"\n",
    "        \"of someUnknownPlace.\")\n",
    "\n",
    "interger = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(interger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b04549de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo, do you like tea? <|endoftext|> In the sunlight terracesof someUnknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(interger)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9173daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input-target pair\n",
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ee031f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(data)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2362e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ------->  H\n",
      "I H -------> AD\n",
      "I HAD ------->  always\n",
      "I HAD always ------->  thought\n",
      "I HAD always thought ------->  Jack\n",
      "I HAD always thought Jack ------->  G\n",
      "I HAD always thought Jack G -------> is\n",
      "I HAD always thought Jack Gis -------> burn\n",
      "I HAD always thought Jack Gisburn ------->  rather\n",
      "I HAD always thought Jack Gisburn rather ------->  a\n",
      "I HAD always thought Jack Gisburn rather a ------->  cheap\n",
      "I HAD always thought Jack Gisburn rather a cheap ------->  genius\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -------> --\n",
      "I HAD always thought Jack Gisburn rather a cheap genius-- -------> though\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though ------->  a\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a ------->  good\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good ------->  fellow\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow ------->  enough\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough -------> --\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough-- -------> so\n"
     ]
    }
   ],
   "source": [
    "sample_context = 20\n",
    "for i in range(1, sample_context+1):\n",
    "    context = enc_text[:i]\n",
    "    desire = enc_text[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"------->\", tokenizer.decode([desire]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14950503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "    # Using sliding window to chuck book into overlapping sequence of max_len \n",
    "        for i in range (0, len(token_ids)-max_len, stride):\n",
    "            input_chuck = token_ids[i : i + max_len]\n",
    "            target_chuck = token_ids[i + 1 : i + max_len + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chuck))\n",
    "            self.target_ids.append(torch.tensor(target_chuck))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloader(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride) #create dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = shuffle, drop_last = drop_last, num_workers = num_workers)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#with open(\"the-verdict.txt\" , 'r') as file:\n",
    "#    data = file.read()      Run this to get the data\n",
    "\n",
    "\n",
    "dataloader = create_dataloader(data, batch_size=1, max_len=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "# [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])] ----->> it gives input_tensor and output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1da52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d694ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8a61fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d571b43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f40a6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31659a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90012c47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
