{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eba4034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa9755a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()\n",
    "# data = input(\"Enter your prompt here: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8778d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20479\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10ea2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_text = re.split(r'([,.<>/!@#$%^&*():;]|--|\\s)',data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2558b2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This a for loop for separating the text. And below is list comprehension\n",
    "\n",
    "# result = []\n",
    "# for item in text:\n",
    "#     if item.strip():\n",
    "#         result.append(item)\n",
    "    \n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b021d18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in', 'the', 'height', 'of', 'his', 'glory', ',', 'he', 'had', 'dropped', 'his', 'painting', ',', 'married', 'a', 'rich', 'widow', ',', 'and', 'established', 'himself', 'in', 'a', 'villa', 'on', 'the', 'Riviera', '.', '(', 'Though', 'I', 'rather', 'thought', 'it', 'would', 'have', 'been', 'Rome', 'or', 'Florence', '.', ')', '\"The', 'height', 'of', 'his', 'glory\"', '--', 'that', 'was', 'what', 'the', 'women', 'called', 'it', '.', 'I', 'can', 'hear', 'Mrs', '.', 'Gideon', 'Thwing', '--', 'his', 'last', 'Chicago', 'sitter', '--', 'deploring', 'his', 'unaccountable', 'abdication', '.', '\"Of', 'course', \"it's\", 'going', 'to', 'send', 'the', 'value', 'of', 'my', 'picture', \"'way\", 'up', ';', 'but', 'I', \"don't\", 'think', 'of', 'that', ',', 'Mr', '.', 'Rickham', '--', 'the', 'loss', 'to', 'Arrt', 'is', 'all', 'I', 'think', 'of', '.', '\"', 'The', 'word', ',', 'on', 'Mrs', '.', \"Thwing's\", 'lips', ',', 'multiplied', 'its', '_rs_', 'as', 'though', 'they', 'were', 'reflected', 'in', 'an', 'endless', 'vista', 'of', 'mirrors', '.', 'And', 'it', 'was', 'not', 'only', 'the', 'Mrs', '.', 'Thwings', 'who', 'mourned', '.', 'Had', 'not', 'the', 'exquisite', 'Hermia', 'Croft', ',', 'at', 'the', 'last', 'Grafton', 'Gallery', 'show', ',', 'stopped', 'me', 'before', \"Gisburn's\", '\"Moon-dancers\"', 'to', 'say', ',', 'with', 'tears', 'in', 'her', 'eyes', ':', '\"We', 'shall', 'not', 'look', 'upon', 'its', 'like', 'again\"?', 'Well', '!', '--', 'even', 'through', 'the', 'prism', 'of', \"Hermia's\", 'tears', 'I', 'felt', 'able', 'to', 'face', 'the', 'fact', 'with', 'equanimity', '.', 'Poor', 'Jack', 'Gisburn', '!', 'The', 'women', 'had', 'made', 'him', '--', 'it', 'was', 'fitting', 'that', 'they', 'should', 'mourn', 'him', '.', 'Among', 'his', 'own', 'sex', 'fewer', 'regrets', 'were', 'heard', ',', 'and', 'in', 'his', 'own', 'trade', 'hardly', 'a', 'murmur', '.', 'Professional', 'jealousy?', 'Perhaps', '.', 'If', 'it', 'were', ',', 'the', 'honour', 'of', 'the', 'craft', 'was', 'vindicated', 'by', 'little', 'Claude', 'Nutley', ',', 'who', ',', 'in', 'all', 'good', 'faith', ',', 'brought', 'out', 'in', 'the', 'Burlington', 'a', 'very', 'handsome', '\"obituary\"', 'on', 'Jack', '--', 'one', 'of', 'those', 'showy', 'articles', 'stocked', 'with', 'random', 'technicalities', 'that', 'I', 'have', 'heard', '(', 'I', \"won't\", 'say', 'by', 'whom', ')', 'compared', 'to', \"Gisburn's\", 'painting', '.', 'And', 'so', '--', 'his', 'resolve', 'being', 'apparently', 'irrevocable', '--', 'the', 'discussion', 'gradually', 'died', 'out', ',', 'and', ',', 'as', 'Mrs', '.', 'Thwing', 'had', 'predicted', ',', 'the', 'price', 'of', '\"Gisburns\"', 'went', 'up', '.', 'It', 'was', 'not', 'till', 'three', 'years', 'later', 'that', ',', 'in', 'the', 'course', 'of', 'a', 'few', \"weeks'\", 'idling', 'on', 'the', 'Riviera', ',', 'it', 'suddenly', 'occurred', 'to', 'me', 'to', 'wonder', 'why', 'Gisburn', 'had', 'given', 'up', 'his', 'painting', '.', 'On', 'reflection', ',', 'it', 'really', 'was', 'a', 'tempting', 'problem', '.', 'To', 'accuse', 'his', 'wife', 'would', 'have', 'been', 'too', 'easy', '--', 'his', 'fair', 'sitters', 'had', 'been', 'denied', 'the', 'solace', 'of', 'saying', 'that', 'Mrs', '.', 'Gisburn', 'had', '\"dragged', 'him', 'down', '.', '\"', 'For', 'Mrs', '.', 'Gisburn', '--', 'as', 'such', '--', 'had', 'not', 'existed', 'till', 'nearly', 'a', 'year', 'after', \"Jack's\", 'resolve', 'had', 'been', 'taken', '.', 'It', 'might', 'be', 'that', 'he', 'had', 'married', 'her', '--', 'since', 'he', 'liked', 'his', 'ease', '--', 'because', 'he', \"didn't\", 'want', 'to', 'go', 'on', 'painting', ';', 'but', 'it', 'would', 'have', 'been', 'hard', 'to', 'prove', 'that', 'he', 'had', 'given', 'up', 'his', 'painting', 'because', 'he', 'had', 'married', 'her', '.', 'Of', 'course', ',', 'if', 'she', 'had', 'not', 'dragged', 'him', 'down', ',', 'she', 'had', 'equally', ',', 'as', 'Miss', 'Croft', 'contended', ',', 'failed', 'to', '\"lift', 'him', 'up\"', '--', 'she', 'had', 'not', 'led', 'him', 'back', 'to', 'the', 'easel', '.', 'To', 'put', 'the', 'brush', 'into', 'his', 'hand', 'again', '--', 'what', 'a', 'vocation', 'for', 'a', 'wife', '!', 'But', 'Mrs', '.', 'Gisburn', 'appeared', 'to', 'have', 'disdained', 'it', '--', 'and', 'I', 'felt', 'it', 'might', 'be', 'interesting', 'to', 'find', 'out', 'why', '.', 'The', 'desultory', 'life', 'of', 'the', 'Riviera', 'lends', 'itself', 'to', 'such', 'purely', 'academic', 'speculations', ';', 'and', 'having', ',', 'on', 'my', 'way', 'to', 'Monte', 'Carlo', ',', 'caught', 'a', 'glimpse', 'of', \"Jack's\", 'balustraded', 'terraces', 'between', 'the', 'pines', ',', 'I', 'had', 'myself', 'borne', 'thither', 'the', 'next', 'day', '.', 'I', 'found', 'the', 'couple', 'at', 'tea', 'beneath', 'their', 'palm-trees', ';', 'and', 'Mrs', '.', \"Gisburn's\", 'welcome', 'was', 'so', 'genial', 'that', ',', 'in', 'the', 'ensuing', 'weeks', ',', 'I', 'claimed', 'it', 'frequently', '.', 'It', 'was', 'not', 'that', 'my', 'hostess', 'was', '\"interesting\"', ':', 'on', 'that', 'point', 'I', 'could', 'have', 'given', 'Miss', 'Croft', 'the', 'fullest', 'reassurance', '.', 'It', 'was', 'just', 'because', 'she', 'was', '_not_', 'interesting', '--', 'if', 'I', 'may', 'be', 'pardoned', 'the', 'bull', '--', 'that', 'I', 'found', 'her', 'so', '.', 'For', 'Jack', ',', 'all', 'his', 'life', ',', 'had', 'been', 'surrounded', 'by', 'interesting', 'women', ':', 'they', 'had', 'fostered', 'his', 'art', ',', 'it', 'had', 'been', 'reared', 'in', 'the', 'hot-house', 'of', 'their', 'adulation', '.', 'And', 'it', 'was', 'therefore', 'instructive', 'to', 'note', 'what', 'effect', 'the', '\"deadening', 'atmosphere', 'of', 'mediocrity\"', '(', 'I', 'quote', 'Miss', 'Croft', ')', 'was', 'having', 'on', 'him', '.', 'I', 'have', 'mentioned', 'that', 'Mrs', '.', 'Gisburn', 'was', 'rich', ';', 'and', 'it', 'was', 'immediately', 'perceptible', 'that', 'her', 'husband', 'was', 'extracting', 'from', 'this', 'circumstance', 'a', 'delicate', 'but', 'substantial', 'satisfaction', '.', 'It', 'is', ',', 'as', 'a', 'rule', ',', 'the', 'people', 'who', 'scorn', 'money', 'who', 'get', 'most', 'out', 'of', 'it', ';', 'and', \"Jack's\", 'elegant', 'disdain', 'of', 'his', \"wife's\", 'big', 'balance', 'enabled', 'him', ',', 'with', 'an', 'appearance', 'of', 'perfect', 'good-breeding', ',', 'to', 'transmute', 'it', 'into', 'objects', 'of', 'art', 'and', 'luxury', '.', 'To', 'the', 'latter', ',', 'I', 'must', 'add', ',', 'he', 'remained', 'relatively', 'indifferent', ';', 'but', 'he', 'was', 'buying', 'Renaissance', 'bronzes', 'and', 'eighteenth-century', 'pictures', 'with', 'a', 'discrimination', 'that', 'bespoke', 'the', 'amplest', 'resources', '.', '\"Money\\'s', 'only', 'excuse', 'is', 'to', 'put', 'beauty', 'into', 'circulation', ',', '\"', 'was', 'one', 'of', 'the', 'axioms', 'he', 'laid', 'down', 'across', 'the', 'Sevres', 'and', 'silver', 'of', 'an', 'exquisitely', 'appointed', 'luncheon-table', ',', 'when', ',', 'on', 'a', 'later', 'day', ',', 'I', 'had', 'again', 'run', 'over', 'from', 'Monte', 'Carlo', ';', 'and', 'Mrs', '.', 'Gisburn', ',', 'beaming', 'on', 'him', ',', 'added', 'for', 'my', 'enlightenment', ':', '\"Jack', 'is', 'so', 'morbidly', 'sensitive', 'to', 'every', 'form', 'of', 'beauty', '.', '\"', 'Poor', 'Jack', '!', 'It', 'had', 'always', 'been', 'his', 'fate', 'to', 'have', 'women', 'say', 'such', 'things', 'of', 'him', ':', 'the', 'fact', 'should', 'be', 'set', 'down', 'in', 'extenuation', '.', 'What', 'struck', 'me', 'now', 'was', 'that', ',', 'for', 'the', 'first', 'time', ',', 'he', 'resented', 'the', 'tone', '.', 'I', 'had', 'seen', 'him', ',', 'so', 'often', ',', 'basking', 'under', 'similar', 'tributes', '--', 'was', 'it', 'the', 'conjugal', 'note', 'that', 'robbed', 'them', 'of', 'their', 'savour?', 'No', '--', 'for', ',', 'oddly', 'enough', ',', 'it', 'became', 'apparent', 'that', 'he', 'was', 'fond', 'of', 'Mrs', '.', 'Gisburn', '--', 'fond', 'enough', 'not', 'to', 'see', 'her', 'absurdity', '.', 'It', 'was', 'his', 'own', 'absurdity', 'he', 'seemed', 'to', 'be', 'wincing', 'under', '--', 'his', 'own', 'attitude', 'as', 'an', 'object', 'for', 'garlands', 'and', 'incense', '.', '\"My', 'dear', ',', 'since', \"I've\", 'chucked', 'painting', 'people', \"don't\", 'say', 'that', 'stuff', 'about', 'me', '--', 'they', 'say', 'it', 'about', 'Victor', 'Grindle', ',', '\"', 'was', 'his', 'only', 'protest', ',', 'as', 'he', 'rose', 'from', 'the', 'table', 'and', 'strolled', 'out', 'onto', 'the', 'sunlit', 'terrace', '.', 'I', 'glanced', 'after', 'him', ',', 'struck', 'by', 'his', 'last', 'word', '.', 'Victor', 'Grindle', 'was', ',', 'in', 'fact', ',', 'becoming', 'the', 'man', 'of', 'the', 'moment', '--', 'as', 'Jack', 'himself', ',', 'one', 'might', 'put', 'it', ',', 'had', 'been', 'the', 'man', 'of', 'the', 'hour', '.', 'The', 'younger', 'artist', 'was', 'said', 'to', 'have', 'formed', 'himself', 'at', 'my', \"friend's\", 'feet', ',', 'and', 'I', 'wondered', 'if', 'a', 'tinge', 'of', 'jealousy', 'underlay', 'the', \"latter's\", 'mysterious', 'abdication', '.', 'But', 'no', '--', 'for', 'it', 'was', 'not', 'till', 'after', 'that', 'event', 'that', 'the', '_rose', 'Dubarry_', 'drawing-rooms', 'had', 'begun', 'to', 'display', 'their', '\"Grindles', '.', '\"', 'I', 'turned', 'to', 'Mrs', '.', 'Gisburn', ',', 'who', 'had', 'lingered', 'to', 'give', 'a', 'lump', 'of', 'sugar', 'to', 'her', 'spaniel', 'in', 'the', 'dining-room', '.', '\"Why', '_has_', 'he', 'chucked', 'painting?\"', 'I', 'asked', 'abruptly', '.', 'She', 'raised', 'her', 'eyebrows', 'with', 'a', 'hint', 'of', 'good-humoured', 'surprise', '.', '\"Oh', ',', 'he', \"doesn't\", '_have_', 'to', 'now', ',', 'you', 'know', ';', 'and', 'I', 'want', 'him', 'to', 'enjoy', 'himself', ',', '\"', 'she', 'said', 'quite', 'simply', '.', 'I', 'looked', 'about', 'the', 'spacious', 'white-panelled', 'room', ',', 'with', 'its', '_famille-verte_', 'vases', 'repeating', 'the', 'tones', 'of', 'the', 'pale', 'damask', 'curtains', ',', 'and', 'its', 'eighteenth-century', 'pastels', 'in', 'delicate', 'faded', 'frames', '.', '\"Has', 'he', 'chucked', 'his', 'pictures', 'too?', 'I', \"haven't\", 'seen', 'a', 'single', 'one', 'in', 'the', 'house', '.', '\"', 'A', 'slight', 'shade', 'of', 'constraint', 'crossed', 'Mrs', '.', \"Gisburn's\", 'open', 'countenance', '.', '\"It\\'s', 'his', 'ridiculous', 'modesty', ',', 'you', 'know', '.', 'He', 'says', \"they're\", 'not', 'fit', 'to', 'have', 'about', ';', \"he's\", 'sent', 'them', 'all', 'away', 'except', 'one', '--', 'my', 'portrait', '--', 'and', 'that', 'I', 'have', 'to', 'keep', 'upstairs', '.', '\"', 'His', 'ridiculous', 'modesty', '--', \"Jack's\", 'modesty', 'about', 'his', 'pictures?', 'My', 'curiosity', 'was', 'growing', 'like', 'the', 'bean-stalk', '.', 'I', 'said', 'persuasively', 'to', 'my', 'hostess', ':', '\"I', 'must', 'really', 'see', 'your', 'portrait', ',', 'you', 'know', '.', '\"', 'She', 'glanced', 'out', 'almost', 'timorously', 'at', 'the', 'terrace', 'where', 'her', 'husband', ',', 'lounging', 'in', 'a', 'hooded', 'chair', ',', 'had', 'lit', 'a', 'cigar', 'and', 'drawn', 'the', 'Russian', \"deerhound's\", 'head', 'between', 'his', 'knees', '.', '\"Well', ',', 'come', 'while', \"he's\", 'not', 'looking', ',', '\"', 'she', 'said', ',', 'with', 'a', 'laugh', 'that', 'tried', 'to', 'hide', 'her', 'nervousness', ';', 'and', 'I', 'followed', 'her', 'between', 'the', 'marble', 'Emperors', 'of', 'the', 'hall', ',', 'and', 'up', 'the', 'wide', 'stairs', 'with', 'terra-cotta', 'nymphs', 'poised', 'among', 'flowers', 'at', 'each', 'landing', '.', 'In', 'the', 'dimmest', 'corner', 'of', 'her', 'boudoir', ',', 'amid', 'a', 'profusion', 'of', 'delicate', 'and', 'distinguished', 'objects', ',', 'hung', 'one', 'of', 'the', 'familiar', 'oval', 'canvases', ',', 'in', 'the', 'inevitable', 'garlanded', 'frame', '.', 'The', 'mere', 'outline', 'of', 'the', 'frame', 'called', 'up', 'all', \"Gisburn's\", 'past', '!', 'Mrs', '.', 'Gisburn', 'drew', 'back', 'the', 'window-curtains', ',', 'moved', 'aside', 'a', '_jardiniere_', 'full', 'of', 'pink', 'azaleas', ',', 'pushed', 'an', 'arm-chair', 'away', ',', 'and', 'said', ':', '\"If', 'you', 'stand', 'here', 'you', 'can', 'just', 'manage', 'to', 'see', 'it', '.', 'I', 'had', 'it', 'over', 'the', 'mantel-piece', ',', 'but', 'he', \"wouldn't\", 'let', 'it', 'stay', '.', '\"', 'Yes', '--', 'I', 'could', 'just', 'manage', 'to', 'see', 'it', '--', 'the', 'first', 'portrait', 'of', \"Jack's\", 'I', 'had', 'ever', 'had', 'to', 'strain', 'my', 'eyes', 'over', '!', 'Usually', 'they', 'had', 'the', 'place', 'of', 'honour', '--', 'say', 'the', 'central', 'panel', 'in', 'a', 'pale', 'yellow', 'or', '_rose', 'Dubarry_', 'drawing-room', ',', 'or', 'a', 'monumental', 'easel', 'placed', 'so', 'that', 'it', 'took', 'the', 'light', 'through', 'curtains', 'of', 'old', 'Venetian', 'point', '.', 'The', 'more', 'modest', 'place', 'became', 'the', 'picture', 'better', ';', 'yet', ',', 'as', 'my', 'eyes', 'grew', 'accustomed', 'to', 'the', 'half-light', ',', 'all', 'the', 'characteristic', 'qualities', 'came', 'out', '--', 'all', 'the', 'hesitations', 'disguised', 'as', 'audacities', ',', 'the', 'tricks', 'of', 'prestidigitation', 'by', 'which', ',', 'with', 'such', 'consummate', 'skill', ',', 'he', 'managed', 'to', 'divert', 'attention', 'from', 'the', 'real', 'business', 'of', 'the', 'picture', 'to', 'some', 'pretty', 'irrelevance', 'of', 'detail', '.', 'Mrs', '.', 'Gisburn', ',', 'presenting', 'a', 'neutral', 'surface', 'to', 'work', 'on', '--', 'forming', ',', 'as', 'it', 'were', ',', 'so', 'inevitably', 'the', 'background', 'of', 'her', 'own', 'picture', '--', 'had', 'lent', 'herself', 'in', 'an', 'unusual', 'degree', 'to', 'the', 'display', 'of', 'this', 'false', 'virtuosity', '.', 'The', 'picture', 'was', 'one', 'of', \"Jack's\", '\"strongest', ',', '\"', 'as', 'his', 'admirers', 'would', 'have', 'put', 'it', '--', 'it', 'represented', ',', 'on', 'his', 'part', ',', 'a', 'swelling', 'of', 'muscles', ',', 'a', 'congesting', 'of', 'veins', ',', 'a', 'balancing', ',', 'straddling', 'and', 'straining', ',', 'that', 'reminded', 'one', 'of', 'the', \"circus-clown's\", 'ironic', 'efforts', 'to', 'lift', 'a', 'feather', '.', 'It', 'met', ',', 'in', 'short', ',', 'at', 'every', 'point', 'the', 'demand', 'of', 'lovely', 'woman', 'to', 'be', 'painted', '\"strongly\"', 'because', 'she', 'was', 'tired', 'of', 'being', 'painted', '\"sweetly\"', '--', 'and', 'yet', 'not', 'to', 'lose', 'an', 'atom', 'of', 'the', 'sweetness', '.', '\"It\\'s', 'the', 'last', 'he', 'painted', ',', 'you', 'know', ',', '\"', 'Mrs', '.', 'Gisburn', 'said', 'with', 'pardonable', 'pride', '.', '\"The', 'last', 'but', 'one', ',', '\"', 'she', 'corrected', 'herself', '--', '\"but', 'the', 'other', \"doesn't\", 'count', ',', 'because', 'he', 'destroyed', 'it', '.', '\"', '\"Destroyed', 'it?\"', 'I', 'was', 'about', 'to', 'follow', 'up', 'this', 'clue', 'when', 'I', 'heard', 'a', 'footstep', 'and', 'saw', 'Jack', 'himself', 'on', 'the', 'threshold', '.', 'As', 'he', 'stood', 'there', ',', 'his', 'hands', 'in', 'the', 'pockets', 'of', 'his', 'velveteen', 'coat', ',', 'the', 'thin', 'brown', 'waves', 'of', 'hair', 'pushed', 'back', 'from', 'his', 'white', 'forehead', ',', 'his', 'lean', 'sunburnt', 'cheeks', 'furrowed', 'by', 'a', 'smile', 'that', 'lifted', 'the', 'tips', 'of', 'a', 'self-confident', 'moustache', ',', 'I', 'felt', 'to', 'what', 'a', 'degree', 'he', 'had', 'the', 'same', 'quality', 'as', 'his', 'pictures', '--', 'the', 'quality', 'of', 'looking', 'cleverer', 'than', 'he', 'was', '.', 'His', 'wife', 'glanced', 'at', 'him', 'deprecatingly', ',', 'but', 'his', 'eyes', 'travelled', 'past', 'her', 'to', 'the', 'portrait', '.', '\"Mr', '.', 'Rickham', 'wanted', 'to', 'see', 'it', ',', '\"', 'she', 'began', ',', 'as', 'if', 'excusing', 'herself', '.', 'He', 'shrugged', 'his', 'shoulders', ',', 'still', 'smiling', '.', '\"Oh', ',', 'Rickham', 'found', 'me', 'out', 'long', 'ago', ',', '\"', 'he', 'said', 'lightly', ';', 'then', ',', 'passing', 'his', 'arm', 'through', 'mine', ':', '\"Come', 'and', 'see', 'the', 'rest', 'of', 'the', 'house', '.', '\"', 'He', 'showed', 'it', 'to', 'me', 'with', 'a', 'kind', 'of', 'naive', 'suburban', 'pride', ':', 'the', 'bath-rooms', ',', 'the', 'speaking-tubes', ',', 'the', 'dress-closets', ',', 'the', 'trouser-presses', '--', 'all', 'the', 'complex', 'simplifications', 'of', 'the', \"millionaire's\", 'domestic', 'economy', '.', 'And', 'whenever', 'my', 'wonder', 'paid', 'the', 'expected', 'tribute', 'he', 'said', ',', 'throwing', 'out', 'his', 'chest', 'a', 'little', ':', '\"Yes', ',', 'I', 'really', \"don't\", 'see', 'how', 'people', 'manage', 'to', 'live', 'without', 'that', '.', '\"', 'Well', '--', 'it', 'was', 'just', 'the', 'end', 'one', 'might', 'have', 'foreseen', 'for', 'him', '.', 'Only', 'he', 'was', ',', 'through', 'it', 'all', 'and', 'in', 'spite', 'of', 'it', 'all', '--', 'as', 'he', 'had', 'been', 'through', ',', 'and', 'in', 'spite', 'of', ',', 'his', 'pictures', '--', 'so', 'handsome', ',', 'so', 'charming', ',', 'so', 'disarming', ',', 'that', 'one', 'longed', 'to', 'cry', 'out', ':', '\"Be', 'dissatisfied', 'with', 'your', 'leisure', '!', '\"', 'as', 'once', 'one', 'had', 'longed', 'to', 'say', ':', '\"Be', 'dissatisfied', 'with', 'your', 'work', '!', '\"', 'But', ',', 'with', 'the', 'cry', 'on', 'my', 'lips', ',', 'my', 'diagnosis', 'suffered', 'an', 'unexpected', 'check', '.', '\"This', 'is', 'my', 'own', 'lair', ',', '\"', 'he', 'said', ',', 'leading', 'me', 'into', 'a', 'dark', 'plain', 'room', 'at', 'the', 'end', 'of', 'the', 'florid', 'vista', '.', 'It', 'was', 'square', 'and', 'brown', 'and', 'leathery', ':', 'no', '\"effects\"', ';', 'no', 'bric-a-brac', ',', 'none', 'of', 'the', 'air', 'of', 'posing', 'for', 'reproduction', 'in', 'a', 'picture', 'weekly', '--', 'above', 'all', ',', 'no', 'least', 'sign', 'of', 'ever', 'having', 'been', 'used', 'as', 'a', 'studio', '.', 'The', 'fact', 'brought', 'home', 'to', 'me', 'the', 'absolute', 'finality', 'of', \"Jack's\", 'break', 'with', 'his', 'old', 'life', '.', '\"Don\\'t', 'you', 'ever', 'dabble', 'with', 'paint', 'any', 'more?\"', 'I', 'asked', ',', 'still', 'looking', 'about', 'for', 'a', 'trace', 'of', 'such', 'activity', '.', '\"Never', ',', '\"', 'he', 'said', 'briefly', '.', '\"Or', 'water-colour', '--', 'or', 'etching?\"', 'His', 'confident', 'eyes', 'grew', 'dim', ',', 'and', 'his', 'cheeks', 'paled', 'a', 'little', 'under', 'their', 'handsome', 'sunburn', '.', '\"Never', 'think', 'of', 'it', ',', 'my', 'dear', 'fellow', '--', 'any', 'more', 'than', 'if', \"I'd\", 'never', 'touched', 'a', 'brush', '.', '\"', 'And', 'his', 'tone', 'told', 'me', 'in', 'a', 'flash', 'that', 'he', 'never', 'thought', 'of', 'anything', 'else', '.', 'I', 'moved', 'away', ',', 'instinctively', 'embarrassed', 'by', 'my', 'unexpected', 'discovery', ';', 'and', 'as', 'I', 'turned', ',', 'my', 'eye', 'fell', 'on', 'a', 'small', 'picture', 'above', 'the', 'mantel-piece', '--', 'the', 'only', 'object', 'breaking', 'the', 'plain', 'oak', 'panelling', 'of', 'the', 'room', '.', '\"Oh', ',', 'by', 'Jove', '!', '\"', 'I', 'said', '.', 'It', 'was', 'a', 'sketch', 'of', 'a', 'donkey', '--', 'an', 'old', 'tired', 'donkey', ',', 'standing', 'in', 'the', 'rain', 'under', 'a', 'wall', '.', '\"By', 'Jove', '--', 'a', 'Stroud', '!', '\"', 'I', 'cried', '.', 'He', 'was', 'silent', ';', 'but', 'I', 'felt', 'him', 'close', 'behind', 'me', ',', 'breathing', 'a', 'little', 'quickly', '.', '\"What', 'a', 'wonder', '!', 'Made', 'with', 'a', 'dozen', 'lines', '--', 'but', 'on', 'everlasting', 'foundations', '.', 'You', 'lucky', 'chap', ',', 'where', 'did', 'you', 'get', 'it?\"', 'He', 'answered', 'slowly', ':', '\"Mrs', '.', 'Stroud', 'gave', 'it', 'to', 'me', '.', '\"', '\"Ah', '--', 'I', \"didn't\", 'know', 'you', 'even', 'knew', 'the', 'Strouds', '.', 'He', 'was', 'such', 'an', 'inflexible', 'hermit', '.', '\"', '\"I', \"didn't\", '--', 'till', 'after', '.', '.', '.', '.', 'She', 'sent', 'for', 'me', 'to', 'paint', 'him', 'when', 'he', 'was', 'dead', '.', '\"', '\"When', 'he', 'was', 'dead?', 'You?\"', 'I', 'must', 'have', 'let', 'a', 'little', 'too', 'much', 'amazement', 'escape', 'through', 'my', 'surprise', ',', 'for', 'he', 'answered', 'with', 'a', 'deprecating', 'laugh', ':', '\"Yes', '--', \"she's\", 'an', 'awful', 'simpleton', ',', 'you', 'know', ',', 'Mrs', '.', 'Stroud', '.', 'Her', 'only', 'idea', 'was', 'to', 'have', 'him', 'done', 'by', 'a', 'fashionable', 'painter', '--', 'ah', ',', 'poor', 'Stroud', '!', 'She', 'thought', 'it', 'the', 'surest', 'way', 'of', 'proclaiming', 'his', 'greatness', '--', 'of', 'forcing', 'it', 'on', 'a', 'purblind', 'public', '.', 'And', 'at', 'the', 'moment', 'I', 'was', '_the_', 'fashionable', 'painter', '.', '\"', '\"Ah', ',', 'poor', 'Stroud', '--', 'as', 'you', 'say', '.', 'Was', '_that_', 'his', 'history?\"', '\"That', 'was', 'his', 'history', '.', 'She', 'believed', 'in', 'him', ',', 'gloried', 'in', 'him', '--', 'or', 'thought', 'she', 'did', '.', 'But', 'she', \"couldn't\", 'bear', 'not', 'to', 'have', 'all', 'the', 'drawing-rooms', 'with', 'her', '.', 'She', \"couldn't\", 'bear', 'the', 'fact', 'that', ',', 'on', 'varnishing', 'days', ',', 'one', 'could', 'always', 'get', 'near', 'enough', 'to', 'see', 'his', 'pictures', '.', 'Poor', 'woman', '!', \"She's\", 'just', 'a', 'fragment', 'groping', 'for', 'other', 'fragments', '.', 'Stroud', 'is', 'the', 'only', 'whole', 'I', 'ever', 'knew', '.', '\"', '\"You', 'ever', 'knew?', 'But', 'you', 'just', 'said', '--', '\"', 'Gisburn', 'had', 'a', 'curious', 'smile', 'in', 'his', 'eyes', '.', '\"Oh', ',', 'I', 'knew', 'him', ',', 'and', 'he', 'knew', 'me', '--', 'only', 'it', 'happened', 'after', 'he', 'was', 'dead', '.', '\"', 'I', 'dropped', 'my', 'voice', 'instinctively', '.', '\"When', 'she', 'sent', 'for', 'you?\"', '\"Yes', '--', 'quite', 'insensible', 'to', 'the', 'irony', '.', 'She', 'wanted', 'him', 'vindicated', '--', 'and', 'by', 'me', '!', '\"', 'He', 'laughed', 'again', ',', 'and', 'threw', 'back', 'his', 'head', 'to', 'look', 'up', 'at', 'the', 'sketch', 'of', 'the', 'donkey', '.', '\"There', 'were', 'days', 'when', 'I', \"couldn't\", 'look', 'at', 'that', 'thing', '--', \"couldn't\", 'face', 'it', '.', 'But', 'I', 'forced', 'myself', 'to', 'put', 'it', 'here', ';', 'and', 'now', \"it's\", 'cured', 'me', '--', 'cured', 'me', '.', \"That's\", 'the', 'reason', 'why', 'I', \"don't\", 'dabble', 'any', 'more', ',', 'my', 'dear', 'Rickham', ';', 'or', 'rather', 'Stroud', 'himself', 'is', 'the', 'reason', '.', '\"', 'For', 'the', 'first', 'time', 'my', 'idle', 'curiosity', 'about', 'my', 'companion', 'turned', 'into', 'a', 'serious', 'desire', 'to', 'understand', 'him', 'better', '.', '\"I', 'wish', \"you'd\", 'tell', 'me', 'how', 'it', 'happened', ',', '\"', 'I', 'said', '.', 'He', 'stood', 'looking', 'up', 'at', 'the', 'sketch', ',', 'and', 'twirling', 'between', 'his', 'fingers', 'a', 'cigarette', 'he', 'had', 'forgotten', 'to', 'light', '.', 'Suddenly', 'he', 'turned', 'toward', 'me', '.', '\"I\\'d', 'rather', 'like', 'to', 'tell', 'you', '--', 'because', \"I've\", 'always', 'suspected', 'you', 'of', 'loathing', 'my', 'work', '.', '\"', 'I', 'made', 'a', 'deprecating', 'gesture', ',', 'which', 'he', 'negatived', 'with', 'a', 'good-humoured', 'shrug', '.', '\"Oh', ',', 'I', \"didn't\", 'care', 'a', 'straw', 'when', 'I', 'believed', 'in', 'myself', '--', 'and', 'now', \"it's\", 'an', 'added', 'tie', 'between', 'us', '!', '\"', 'He', 'laughed', 'slightly', ',', 'without', 'bitterness', ',', 'and', 'pushed', 'one', 'of', 'the', 'deep', 'arm-chairs', 'forward', '.', '\"There', ':', 'make', 'yourself', 'comfortable', '--', 'and', 'here', 'are', 'the', 'cigars', 'you', 'like', '.', '\"', 'He', 'placed', 'them', 'at', 'my', 'elbow', 'and', 'continued', 'to', 'wander', 'up', 'and', 'down', 'the', 'room', ',', 'stopping', 'now', 'and', 'then', 'beneath', 'the', 'picture', '.', '\"How', 'it', 'happened?', 'I', 'can', 'tell', 'you', 'in', 'five', 'minutes', '--', 'and', 'it', \"didn't\", 'take', 'much', 'longer', 'to', 'happen', '.', '.', '.', '.', 'I', 'can', 'remember', 'now', 'how', 'surprised', 'and', 'pleased', 'I', 'was', 'when', 'I', 'got', 'Mrs', '.', \"Stroud's\", 'note', '.', 'Of', 'course', ',', 'deep', 'down', ',', 'I', 'had', 'always', '_felt_', 'there', 'was', 'no', 'one', 'like', 'him', '--', 'only', 'I', 'had', 'gone', 'with', 'the', 'stream', ',', 'echoed', 'the', 'usual', 'platitudes', 'about', 'him', ',', 'till', 'I', 'half', 'got', 'to', 'think', 'he', 'was', 'a', 'failure', ',', 'one', 'of', 'the', 'kind', 'that', 'are', 'left', 'behind', '.', 'By', 'Jove', ',', 'and', 'he', '_was_', 'left', 'behind', '--', 'because', 'he', 'had', 'come', 'to', 'stay', '!', 'The', 'rest', 'of', 'us', 'had', 'to', 'let', 'ourselves', 'be', 'swept', 'along', 'or', 'go', 'under', ',', 'but', 'he', 'was', 'high', 'above', 'the', 'current', '--', 'on', 'everlasting', 'foundations', ',', 'as', 'you', 'say', '.', '\"Well', ',', 'I', 'went', 'off', 'to', 'the', 'house', 'in', 'my', 'most', 'egregious', 'mood', '--', 'rather', 'moved', ',', 'Lord', 'forgive', 'me', ',', 'at', 'the', 'pathos', 'of', 'poor', \"Stroud's\", 'career', 'of', 'failure', 'being', 'crowned', 'by', 'the', 'glory', 'of', 'my', 'painting', 'him', '!', 'Of', 'course', 'I', 'meant', 'to', 'do', 'the', 'picture', 'for', 'nothing', '--', 'I', 'told', 'Mrs', '.', 'Stroud', 'so', 'when', 'she', 'began', 'to', 'stammer', 'something', 'about', 'her', 'poverty', '.', 'I', 'remember', 'getting', 'off', 'a', 'prodigious', 'phrase', 'about', 'the', 'honour', 'being', '_mine_', '--', 'oh', ',', 'I', 'was', 'princely', ',', 'my', 'dear', 'Rickham', '!', 'I', 'was', 'posing', 'to', 'myself', 'like', 'one', 'of', 'my', 'own', 'sitters', '.', '\"Then', 'I', 'was', 'taken', 'up', 'and', 'left', 'alone', 'with', 'him', '.', 'I', 'had', 'sent', 'all', 'my', 'traps', 'in', 'advance', ',', 'and', 'I', 'had', 'only', 'to', 'set', 'up', 'the', 'easel', 'and', 'get', 'to', 'work', '.', 'He', 'had', 'been', 'dead', 'only', 'twenty-four', 'hours', ',', 'and', 'he', 'died', 'suddenly', ',', 'of', 'heart', 'disease', ',', 'so', 'that', 'there', 'had', 'been', 'no', 'preliminary', 'work', 'of', 'destruction', '--', 'his', 'face', 'was', 'clear', 'and', 'untouched', '.', 'I', 'had', 'met', 'him', 'once', 'or', 'twice', ',', 'years', 'before', ',', 'and', 'thought', 'him', 'insignificant', 'and', 'dingy', '.', 'Now', 'I', 'saw', 'that', 'he', 'was', 'superb', '.', '\"I', 'was', 'glad', 'at', 'first', ',', 'with', 'a', 'merely', 'aesthetic', 'satisfaction', ':', 'glad', 'to', 'have', 'my', 'hand', 'on', 'such', 'a', \"'subject\", '.', \"'\", 'Then', 'his', 'strange', 'life-likeness', 'began', 'to', 'affect', 'me', 'queerly', '--', 'as', 'I', 'blocked', 'the', 'head', 'in', 'I', 'felt', 'as', 'if', 'he', 'were', 'watching', 'me', 'do', 'it', '.', 'The', 'sensation', 'was', 'followed', 'by', 'the', 'thought', ':', 'if', 'he', '_were_', 'watching', 'me', ',', 'what', 'would', 'he', 'say', 'to', 'my', 'way', 'of', 'working?', 'My', 'strokes', 'began', 'to', 'go', 'a', 'little', 'wild', '--', 'I', 'felt', 'nervous', 'and', 'uncertain', '.', '\"Once', ',', 'when', 'I', 'looked', 'up', ',', 'I', 'seemed', 'to', 'see', 'a', 'smile', 'behind', 'his', 'close', 'grayish', 'beard', '--', 'as', 'if', 'he', 'had', 'the', 'secret', ',', 'and', 'were', 'amusing', 'himself', 'by', 'holding', 'it', 'back', 'from', 'me', '.', 'That', 'exasperated', 'me', 'still', 'more', '.', 'The', 'secret?', 'Why', ',', 'I', 'had', 'a', 'secret', 'worth', 'twenty', 'of', 'his', '!', 'I', 'dashed', 'at', 'the', 'canvas', 'furiously', ',', 'and', 'tried', 'some', 'of', 'my', 'bravura', 'tricks', '.', 'But', 'they', 'failed', 'me', ',', 'they', 'crumbled', '.', 'I', 'saw', 'that', 'he', \"wasn't\", 'watching', 'the', 'showy', 'bits', '--', 'I', \"couldn't\", 'distract', 'his', 'attention', ';', 'he', 'just', 'kept', 'his', 'eyes', 'on', 'the', 'hard', 'passages', 'between', '.', 'Those', 'were', 'the', 'ones', 'I', 'had', 'always', 'shirked', ',', 'or', 'covered', 'up', 'with', 'some', 'lying', 'paint', '.', 'And', 'how', 'he', 'saw', 'through', 'my', 'lies', '!', '\"I', 'looked', 'up', 'again', ',', 'and', 'caught', 'sight', 'of', 'that', 'sketch', 'of', 'the', 'donkey', 'hanging', 'on', 'the', 'wall', 'near', 'his', 'bed', '.', 'His', 'wife', 'told', 'me', 'afterward', 'it', 'was', 'the', 'last', 'thing', 'he', 'had', 'done', '--', 'just', 'a', 'note', 'taken', 'with', 'a', 'shaking', 'hand', ',', 'when', 'he', 'was', 'down', 'in', 'Devonshire', 'recovering', 'from', 'a', 'previous', 'heart', 'attack', '.', 'Just', 'a', 'note', '!', 'But', 'it', 'tells', 'his', 'whole', 'history', '.', 'There', 'are', 'years', 'of', 'patient', 'scornful', 'persistence', 'in', 'every', 'line', '.', 'A', 'man', 'who', 'had', 'swum', 'with', 'the', 'current', 'could', 'never', 'have', 'learned', 'that', 'mighty', 'up-stream', 'stroke', '.', '.', '.', '.', '\"I', 'turned', 'back', 'to', 'my', 'work', ',', 'and', 'went', 'on', 'groping', 'and', 'muddling', ';', 'then', 'I', 'looked', 'at', 'the', 'donkey', 'again', '.', 'I', 'saw', 'that', ',', 'when', 'Stroud', 'laid', 'in', 'the', 'first', 'stroke', ',', 'he', 'knew', 'just', 'what', 'the', 'end', 'would', 'be', '.', 'He', 'had', 'possessed', 'his', 'subject', ',', 'absorbed', 'it', ',', 'recreated', 'it', '.', 'When', 'had', 'I', 'done', 'that', 'with', 'any', 'of', 'my', 'things?', 'They', \"hadn't\", 'been', 'born', 'of', 'me', '--', 'I', 'had', 'just', 'adopted', 'them', '.', '.', '.', '.', '\"Hang', 'it', ',', 'Rickham', ',', 'with', 'that', 'face', 'watching', 'me', 'I', \"couldn't\", 'do', 'another', 'stroke', '.', 'The', 'plain', 'truth', 'was', ',', 'I', \"didn't\", 'know', 'where', 'to', 'put', 'it', '--', '_I', 'had', 'never', 'known_', '.', 'Only', ',', 'with', 'my', 'sitters', 'and', 'my', 'public', ',', 'a', 'showy', 'splash', 'of', 'colour', 'covered', 'up', 'the', 'fact', '--', 'I', 'just', 'threw', 'paint', 'into', 'their', 'faces', '.', '.', '.', '.', 'Well', ',', 'paint', 'was', 'the', 'one', 'medium', 'those', 'dead', 'eyes', 'could', 'see', 'through', '--', 'see', 'straight', 'to', 'the', 'tottering', 'foundations', 'underneath', '.', \"Don't\", 'you', 'know', 'how', ',', 'in', 'talking', 'a', 'foreign', 'language', ',', 'even', 'fluently', ',', 'one', 'says', 'half', 'the', 'time', 'not', 'what', 'one', 'wants', 'to', 'but', 'what', 'one', 'can?', 'Well', '--', 'that', 'was', 'the', 'way', 'I', 'painted', ';', 'and', 'as', 'he', 'lay', 'there', 'and', 'watched', 'me', ',', 'the', 'thing', 'they', 'called', 'my', \"'technique'\", 'collapsed', 'like', 'a', 'house', 'of', 'cards', '.', 'He', \"didn't\", 'sneer', ',', 'you', 'understand', ',', 'poor', 'Stroud', '--', 'he', 'just', 'lay', 'there', 'quietly', 'watching', ',', 'and', 'on', 'his', 'lips', ',', 'through', 'the', 'gray', 'beard', ',', 'I', 'seemed', 'to', 'hear', 'the', 'question', ':', \"'Are\", 'you', 'sure', 'you', 'know', 'where', \"you're\", 'coming', \"out?'\", '\"If', 'I', 'could', 'have', 'painted', 'that', 'face', ',', 'with', 'that', 'question', 'on', 'it', ',', 'I', 'should', 'have', 'done', 'a', 'great', 'thing', '.', 'The', 'next', 'greatest', 'thing', 'was', 'to', 'see', 'that', 'I', \"couldn't\", '--', 'and', 'that', 'grace', 'was', 'given', 'me', '.', 'But', ',', 'oh', ',', 'at', 'that', 'minute', ',', 'Rickham', ',', 'was', 'there', 'anything', 'on', 'earth', 'I', \"wouldn't\", 'have', 'given', 'to', 'have', 'Stroud', 'alive', 'before', 'me', ',', 'and', 'to', 'hear', 'him', 'say', ':', \"'It's\", 'not', 'too', 'late', '--', \"I'll\", 'show', 'you', \"how'?\", '\"It', '_was_', 'too', 'late', '--', 'it', 'would', 'have', 'been', ',', 'even', 'if', \"he'd\", 'been', 'alive', '.', 'I', 'packed', 'up', 'my', 'traps', ',', 'and', 'went', 'down', 'and', 'told', 'Mrs', '.', 'Stroud', '.', 'Of', 'course', 'I', \"didn't\", 'tell', 'her', '_that_', '--', 'it', 'would', 'have', 'been', 'Greek', 'to', 'her', '.', 'I', 'simply', 'said', 'I', \"couldn't\", 'paint', 'him', ',', 'that', 'I', 'was', 'too', 'moved', '.', 'She', 'rather', 'liked', 'the', 'idea', '--', \"she's\", 'so', 'romantic', '!', 'It', 'was', 'that', 'that', 'made', 'her', 'give', 'me', 'the', 'donkey', '.', 'But', 'she', 'was', 'terribly', 'upset', 'at', 'not', 'getting', 'the', 'portrait', '--', 'she', 'did', 'so', 'want', 'him', \"'done'\", 'by', 'some', 'one', 'showy', '!', 'At', 'first', 'I', 'was', 'afraid', 'she', \"wouldn't\", 'let', 'me', 'off', '--', 'and', 'at', 'my', \"wits'\", 'end', 'I', 'suggested', 'Grindle', '.', 'Yes', ',', 'it', 'was', 'I', 'who', 'started', 'Grindle', ':', 'I', 'told', 'Mrs', '.', 'Stroud', 'he', 'was', 'the', \"'coming'\", 'man', ',', 'and', 'she', 'told', 'somebody', 'else', ',', 'and', 'so', 'it', 'got', 'to', 'be', 'true', '.', '.', '.', '.', 'And', 'he', 'painted', 'Stroud', 'without', 'wincing', ';', 'and', 'she', 'hung', 'the', 'picture', 'among', 'her', \"husband's\", 'things', '.', '.', '.', '.', '\"', 'He', 'flung', 'himself', 'down', 'in', 'the', 'arm-chair', 'near', 'mine', ',', 'laid', 'back', 'his', 'head', ',', 'and', 'clasping', 'his', 'arms', 'beneath', 'it', ',', 'looked', 'up', 'at', 'the', 'picture', 'above', 'the', 'chimney-piece', '.', '\"I', 'like', 'to', 'fancy', 'that', 'Stroud', 'himself', 'would', 'have', 'given', 'it', 'to', 'me', ',', 'if', \"he'd\", 'been', 'able', 'to', 'say', 'what', 'he', 'thought', 'that', 'day', '.', '\"', 'And', ',', 'in', 'answer', 'to', 'a', 'question', 'I', 'put', 'half-mechanically', '--', '\"Begin', 'again?\"', 'he', 'flashed', 'out', '.', '\"When', 'the', 'one', 'thing', 'that', 'brings', 'me', 'anywhere', 'near', 'him', 'is', 'that', 'I', 'knew', 'enough', 'to', 'leave', 'off?\"', 'He', 'stood', 'up', 'and', 'laid', 'his', 'hand', 'on', 'my', 'shoulder', 'with', 'a', 'laugh', '.', '\"Only', 'the', 'irony', 'of', 'it', 'is', 'that', 'I', '_am_', 'still', 'painting', '--', 'since', \"Grindle's\", 'doing', 'it', 'for', 'me', '!', 'The', 'Strouds', 'stand', 'alone', ',', 'and', 'happen', 'once', '--', 'but', \"there's\", 'no', 'exterminating', 'our', 'kind', 'of', 'art', '.', '\"']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This is a word based tokenization algorithm. This tokenize for each word.....\n",
    "\n",
    "result = [item.strip() for item in split_text if item.strip()]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ec956c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4364"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49ca9105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n"
     ]
    }
   ],
   "source": [
    "words = sorted(set(result))\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96156f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, '\"': 1, '\"Ah': 2, '\"Be': 3, '\"Begin': 4, '\"By': 5, '\"Come': 6, '\"Destroyed': 7, '\"Don\\'t': 8, '\"Gisburns\"': 9, '\"Grindles': 10, '\"Hang': 11, '\"Has': 12, '\"How': 13, '\"I': 14, '\"I\\'d': 15, '\"If': 16, '\"It': 17, '\"It\\'s': 18, '\"Jack': 19, '\"Money\\'s': 20, '\"Moon-dancers\"': 21, '\"Mr': 22, '\"Mrs': 23, '\"My': 24, '\"Never': 25, '\"Of': 26, '\"Oh': 27, '\"Once': 28, '\"Only': 29, '\"Or': 30, '\"That': 31, '\"The': 32, '\"Then': 33, '\"There': 34, '\"This': 35, '\"We': 36, '\"Well': 37, '\"What': 38, '\"When': 39, '\"Why': 40, '\"Yes': 41, '\"You': 42, '\"but': 43, '\"deadening': 44, '\"dragged': 45, '\"effects\"': 46, '\"interesting\"': 47, '\"lift': 48, '\"obituary\"': 49, '\"strongest': 50, '\"strongly\"': 51, '\"sweetly\"': 52, \"'\": 53, \"'Are\": 54, \"'It's\": 55, \"'coming'\": 56, \"'done'\": 57, \"'subject\": 58, \"'technique'\": 59, \"'way\": 60, '(': 61, ')': 62, ',': 63, '--': 64, '.': 65, ':': 66, ';': 67, 'A': 68, 'Among': 69, 'And': 70, 'Arrt': 71, 'As': 72, 'At': 73, 'Burlington': 74, 'But': 75, 'By': 76, 'Carlo': 77, 'Chicago': 78, 'Claude': 79, 'Croft': 80, 'Devonshire': 81, \"Don't\": 82, 'Dubarry_': 83, 'Emperors': 84, 'Florence': 85, 'For': 86, 'Gallery': 87, 'Gideon': 88, 'Gisburn': 89, \"Gisburn's\": 90, 'Grafton': 91, 'Greek': 92, 'Grindle': 93, \"Grindle's\": 94, 'HAD': 95, 'Had': 96, 'He': 97, 'Her': 98, 'Hermia': 99, \"Hermia's\": 100, 'His': 101, 'I': 102, \"I'd\": 103, \"I'll\": 104, \"I've\": 105, 'If': 106, 'In': 107, 'It': 108, 'Jack': 109, \"Jack's\": 110, 'Jove': 111, 'Just': 112, 'Lord': 113, 'Made': 114, 'Miss': 115, 'Monte': 116, 'Mr': 117, 'Mrs': 118, 'My': 119, 'No': 120, 'Now': 121, 'Nutley': 122, 'Of': 123, 'On': 124, 'Only': 125, 'Perhaps': 126, 'Poor': 127, 'Professional': 128, 'Renaissance': 129, 'Rickham': 130, 'Riviera': 131, 'Rome': 132, 'Russian': 133, 'Sevres': 134, 'She': 135, \"She's\": 136, 'Stroud': 137, \"Stroud's\": 138, 'Strouds': 139, 'Suddenly': 140, 'That': 141, \"That's\": 142, 'The': 143, 'Then': 144, 'There': 145, 'They': 146, 'Those': 147, 'Though': 148, 'Thwing': 149, \"Thwing's\": 150, 'Thwings': 151, 'To': 152, 'Usually': 153, 'Venetian': 154, 'Victor': 155, 'Was': 156, 'Well': 157, 'What': 158, 'When': 159, 'Why': 160, 'Yes': 161, 'You': 162, 'You?\"': 163, '_I': 164, '_am_': 165, '_famille-verte_': 166, '_felt_': 167, '_has_': 168, '_have_': 169, '_jardiniere_': 170, '_mine_': 171, '_not_': 172, '_rose': 173, '_rs_': 174, '_that_': 175, '_the_': 176, '_was_': 177, '_were_': 178, 'a': 179, 'abdication': 180, 'able': 181, 'about': 182, 'above': 183, 'abruptly': 184, 'absolute': 185, 'absorbed': 186, 'absurdity': 187, 'academic': 188, 'accuse': 189, 'accustomed': 190, 'across': 191, 'activity': 192, 'add': 193, 'added': 194, 'admirers': 195, 'adopted': 196, 'adulation': 197, 'advance': 198, 'aesthetic': 199, 'affect': 200, 'afraid': 201, 'after': 202, 'afterward': 203, 'again': 204, 'again\"?': 205, 'again?\"': 206, 'ago': 207, 'ah': 208, 'air': 209, 'alive': 210, 'all': 211, 'almost': 212, 'alone': 213, 'along': 214, 'always': 215, 'amazement': 216, 'amid': 217, 'among': 218, 'amplest': 219, 'amusing': 220, 'an': 221, 'and': 222, 'another': 223, 'answer': 224, 'answered': 225, 'any': 226, 'anything': 227, 'anywhere': 228, 'apparent': 229, 'apparently': 230, 'appearance': 231, 'appeared': 232, 'appointed': 233, 'are': 234, 'arm': 235, 'arm-chair': 236, 'arm-chairs': 237, 'arms': 238, 'art': 239, 'articles': 240, 'artist': 241, 'as': 242, 'aside': 243, 'asked': 244, 'at': 245, 'atmosphere': 246, 'atom': 247, 'attack': 248, 'attention': 249, 'attitude': 250, 'audacities': 251, 'away': 252, 'awful': 253, 'axioms': 254, 'azaleas': 255, 'back': 256, 'background': 257, 'balance': 258, 'balancing': 259, 'balustraded': 260, 'basking': 261, 'bath-rooms': 262, 'be': 263, 'beaming': 264, 'bean-stalk': 265, 'bear': 266, 'beard': 267, 'beauty': 268, 'became': 269, 'because': 270, 'becoming': 271, 'bed': 272, 'been': 273, 'before': 274, 'began': 275, 'begun': 276, 'behind': 277, 'being': 278, 'believed': 279, 'beneath': 280, 'bespoke': 281, 'better': 282, 'between': 283, 'big': 284, 'bits': 285, 'bitterness': 286, 'blocked': 287, 'born': 288, 'borne': 289, 'boudoir': 290, 'bravura': 291, 'break': 292, 'breaking': 293, 'breathing': 294, 'bric-a-brac': 295, 'briefly': 296, 'brings': 297, 'bronzes': 298, 'brought': 299, 'brown': 300, 'brush': 301, 'bull': 302, 'business': 303, 'but': 304, 'buying': 305, 'by': 306, 'called': 307, 'came': 308, 'can': 309, 'can?': 310, 'canvas': 311, 'canvases': 312, 'cards': 313, 'care': 314, 'career': 315, 'caught': 316, 'central': 317, 'chair': 318, 'chap': 319, 'characteristic': 320, 'charming': 321, 'cheap': 322, 'check': 323, 'cheeks': 324, 'chest': 325, 'chimney-piece': 326, 'chucked': 327, 'cigar': 328, 'cigarette': 329, 'cigars': 330, 'circulation': 331, 'circumstance': 332, \"circus-clown's\": 333, 'claimed': 334, 'clasping': 335, 'clear': 336, 'cleverer': 337, 'close': 338, 'clue': 339, 'coat': 340, 'collapsed': 341, 'colour': 342, 'come': 343, 'comfortable': 344, 'coming': 345, 'companion': 346, 'compared': 347, 'complex': 348, 'confident': 349, 'congesting': 350, 'conjugal': 351, 'constraint': 352, 'consummate': 353, 'contended': 354, 'continued': 355, 'corner': 356, 'corrected': 357, 'could': 358, \"couldn't\": 359, 'count': 360, 'countenance': 361, 'couple': 362, 'course': 363, 'covered': 364, 'craft': 365, 'cried': 366, 'crossed': 367, 'crowned': 368, 'crumbled': 369, 'cry': 370, 'cured': 371, 'curiosity': 372, 'curious': 373, 'current': 374, 'curtains': 375, 'dabble': 376, 'damask': 377, 'dark': 378, 'dashed': 379, 'day': 380, 'days': 381, 'dead': 382, 'dead?': 383, 'dear': 384, 'deep': 385, \"deerhound's\": 386, 'degree': 387, 'delicate': 388, 'demand': 389, 'denied': 390, 'deploring': 391, 'deprecating': 392, 'deprecatingly': 393, 'desire': 394, 'destroyed': 395, 'destruction': 396, 'desultory': 397, 'detail': 398, 'diagnosis': 399, 'did': 400, \"didn't\": 401, 'died': 402, 'dim': 403, 'dimmest': 404, 'dingy': 405, 'dining-room': 406, 'disarming': 407, 'discovery': 408, 'discrimination': 409, 'discussion': 410, 'disdain': 411, 'disdained': 412, 'disease': 413, 'disguised': 414, 'display': 415, 'dissatisfied': 416, 'distinguished': 417, 'distract': 418, 'divert': 419, 'do': 420, \"doesn't\": 421, 'doing': 422, 'domestic': 423, \"don't\": 424, 'done': 425, 'donkey': 426, 'down': 427, 'dozen': 428, 'dragged': 429, 'drawing-room': 430, 'drawing-rooms': 431, 'drawn': 432, 'dress-closets': 433, 'drew': 434, 'dropped': 435, 'each': 436, 'earth': 437, 'ease': 438, 'easel': 439, 'easy': 440, 'echoed': 441, 'economy': 442, 'effect': 443, 'efforts': 444, 'egregious': 445, 'eighteenth-century': 446, 'elbow': 447, 'elegant': 448, 'else': 449, 'embarrassed': 450, 'enabled': 451, 'end': 452, 'endless': 453, 'enjoy': 454, 'enlightenment': 455, 'enough': 456, 'ensuing': 457, 'equally': 458, 'equanimity': 459, 'escape': 460, 'established': 461, 'etching?\"': 462, 'even': 463, 'event': 464, 'ever': 465, 'everlasting': 466, 'every': 467, 'exasperated': 468, 'except': 469, 'excuse': 470, 'excusing': 471, 'existed': 472, 'expected': 473, 'exquisite': 474, 'exquisitely': 475, 'extenuation': 476, 'exterminating': 477, 'extracting': 478, 'eye': 479, 'eyebrows': 480, 'eyes': 481, 'face': 482, 'faces': 483, 'fact': 484, 'faded': 485, 'failed': 486, 'failure': 487, 'fair': 488, 'faith': 489, 'false': 490, 'familiar': 491, 'fancy': 492, 'fashionable': 493, 'fate': 494, 'feather': 495, 'feet': 496, 'fell': 497, 'fellow': 498, 'felt': 499, 'few': 500, 'fewer': 501, 'finality': 502, 'find': 503, 'fingers': 504, 'first': 505, 'fit': 506, 'fitting': 507, 'five': 508, 'flash': 509, 'flashed': 510, 'florid': 511, 'flowers': 512, 'fluently': 513, 'flung': 514, 'follow': 515, 'followed': 516, 'fond': 517, 'footstep': 518, 'for': 519, 'forced': 520, 'forcing': 521, 'forehead': 522, 'foreign': 523, 'foreseen': 524, 'forgive': 525, 'forgotten': 526, 'form': 527, 'formed': 528, 'forming': 529, 'forward': 530, 'fostered': 531, 'found': 532, 'foundations': 533, 'fragment': 534, 'fragments': 535, 'frame': 536, 'frames': 537, 'frequently': 538, \"friend's\": 539, 'from': 540, 'full': 541, 'fullest': 542, 'furiously': 543, 'furrowed': 544, 'garlanded': 545, 'garlands': 546, 'gave': 547, 'genial': 548, 'genius': 549, 'gesture': 550, 'get': 551, 'getting': 552, 'give': 553, 'given': 554, 'glad': 555, 'glanced': 556, 'glimpse': 557, 'gloried': 558, 'glory': 559, 'glory\"': 560, 'go': 561, 'going': 562, 'gone': 563, 'good': 564, 'good-breeding': 565, 'good-humoured': 566, 'got': 567, 'grace': 568, 'gradually': 569, 'gray': 570, 'grayish': 571, 'great': 572, 'greatest': 573, 'greatness': 574, 'grew': 575, 'groping': 576, 'growing': 577, 'had': 578, \"hadn't\": 579, 'hair': 580, 'half': 581, 'half-light': 582, 'half-mechanically': 583, 'hall': 584, 'hand': 585, 'hands': 586, 'handsome': 587, 'hanging': 588, 'happen': 589, 'happened': 590, 'happened?': 591, 'hard': 592, 'hardly': 593, 'have': 594, \"haven't\": 595, 'having': 596, 'he': 597, \"he'd\": 598, \"he's\": 599, 'head': 600, 'hear': 601, 'heard': 602, 'heart': 603, 'height': 604, 'her': 605, 'here': 606, 'hermit': 607, 'herself': 608, 'hesitations': 609, 'hide': 610, 'high': 611, 'him': 612, 'himself': 613, 'hint': 614, 'his': 615, 'history': 616, 'history?\"': 617, 'holding': 618, 'home': 619, 'honour': 620, 'hooded': 621, 'hostess': 622, 'hot-house': 623, 'hour': 624, 'hours': 625, 'house': 626, 'how': 627, \"how'?\": 628, 'hung': 629, 'husband': 630, \"husband's\": 631, 'idea': 632, 'idle': 633, 'idling': 634, 'if': 635, 'immediately': 636, 'in': 637, 'incense': 638, 'indifferent': 639, 'inevitable': 640, 'inevitably': 641, 'inflexible': 642, 'insensible': 643, 'insignificant': 644, 'instinctively': 645, 'instructive': 646, 'interesting': 647, 'into': 648, 'ironic': 649, 'irony': 650, 'irrelevance': 651, 'irrevocable': 652, 'is': 653, 'it': 654, \"it's\": 655, 'it?\"': 656, 'its': 657, 'itself': 658, 'jealousy': 659, 'jealousy?': 660, 'just': 661, 'keep': 662, 'kept': 663, 'kind': 664, 'knees': 665, 'knew': 666, 'knew?': 667, 'know': 668, 'known_': 669, 'laid': 670, 'lair': 671, 'landing': 672, 'language': 673, 'last': 674, 'late': 675, 'later': 676, 'latter': 677, \"latter's\": 678, 'laugh': 679, 'laughed': 680, 'lay': 681, 'leading': 682, 'lean': 683, 'learned': 684, 'least': 685, 'leathery': 686, 'leave': 687, 'led': 688, 'left': 689, 'leisure': 690, 'lends': 691, 'lent': 692, 'let': 693, 'lies': 694, 'life': 695, 'life-likeness': 696, 'lift': 697, 'lifted': 698, 'light': 699, 'lightly': 700, 'like': 701, 'liked': 702, 'line': 703, 'lines': 704, 'lingered': 705, 'lips': 706, 'lit': 707, 'little': 708, 'live': 709, 'loathing': 710, 'long': 711, 'longed': 712, 'longer': 713, 'look': 714, 'looked': 715, 'looking': 716, 'lose': 717, 'loss': 718, 'lounging': 719, 'lovely': 720, 'lucky': 721, 'lump': 722, 'luncheon-table': 723, 'luxury': 724, 'lying': 725, 'made': 726, 'make': 727, 'man': 728, 'manage': 729, 'managed': 730, 'mantel-piece': 731, 'marble': 732, 'married': 733, 'may': 734, 'me': 735, 'meant': 736, 'mediocrity\"': 737, 'medium': 738, 'mentioned': 739, 'mere': 740, 'merely': 741, 'met': 742, 'might': 743, 'mighty': 744, \"millionaire's\": 745, 'mine': 746, 'minute': 747, 'minutes': 748, 'mirrors': 749, 'modest': 750, 'modesty': 751, 'moment': 752, 'money': 753, 'monumental': 754, 'mood': 755, 'morbidly': 756, 'more': 757, 'more?\"': 758, 'most': 759, 'mourn': 760, 'mourned': 761, 'moustache': 762, 'moved': 763, 'much': 764, 'muddling': 765, 'multiplied': 766, 'murmur': 767, 'muscles': 768, 'must': 769, 'my': 770, 'myself': 771, 'mysterious': 772, 'naive': 773, 'near': 774, 'nearly': 775, 'negatived': 776, 'nervous': 777, 'nervousness': 778, 'neutral': 779, 'never': 780, 'next': 781, 'no': 782, 'none': 783, 'not': 784, 'note': 785, 'nothing': 786, 'now': 787, 'nymphs': 788, 'oak': 789, 'object': 790, 'objects': 791, 'occurred': 792, 'oddly': 793, 'of': 794, 'off': 795, 'off?\"': 796, 'often': 797, 'oh': 798, 'old': 799, 'on': 800, 'once': 801, 'one': 802, 'ones': 803, 'only': 804, 'onto': 805, 'open': 806, 'or': 807, 'other': 808, 'our': 809, 'ourselves': 810, 'out': 811, \"out?'\": 812, 'outline': 813, 'oval': 814, 'over': 815, 'own': 816, 'packed': 817, 'paid': 818, 'paint': 819, 'painted': 820, 'painter': 821, 'painting': 822, 'painting?\"': 823, 'pale': 824, 'paled': 825, 'palm-trees': 826, 'panel': 827, 'panelling': 828, 'pardonable': 829, 'pardoned': 830, 'part': 831, 'passages': 832, 'passing': 833, 'past': 834, 'pastels': 835, 'pathos': 836, 'patient': 837, 'people': 838, 'perceptible': 839, 'perfect': 840, 'persistence': 841, 'persuasively': 842, 'phrase': 843, 'picture': 844, 'pictures': 845, 'pictures?': 846, 'pines': 847, 'pink': 848, 'place': 849, 'placed': 850, 'plain': 851, 'platitudes': 852, 'pleased': 853, 'pockets': 854, 'point': 855, 'poised': 856, 'poor': 857, 'portrait': 858, 'posing': 859, 'possessed': 860, 'poverty': 861, 'predicted': 862, 'preliminary': 863, 'presenting': 864, 'prestidigitation': 865, 'pretty': 866, 'previous': 867, 'price': 868, 'pride': 869, 'princely': 870, 'prism': 871, 'problem': 872, 'proclaiming': 873, 'prodigious': 874, 'profusion': 875, 'protest': 876, 'prove': 877, 'public': 878, 'purblind': 879, 'purely': 880, 'pushed': 881, 'put': 882, 'qualities': 883, 'quality': 884, 'queerly': 885, 'question': 886, 'quickly': 887, 'quietly': 888, 'quite': 889, 'quote': 890, 'rain': 891, 'raised': 892, 'random': 893, 'rather': 894, 'real': 895, 'really': 896, 'reared': 897, 'reason': 898, 'reassurance': 899, 'recovering': 900, 'recreated': 901, 'reflected': 902, 'reflection': 903, 'regrets': 904, 'relatively': 905, 'remained': 906, 'remember': 907, 'reminded': 908, 'repeating': 909, 'represented': 910, 'reproduction': 911, 'resented': 912, 'resolve': 913, 'resources': 914, 'rest': 915, 'rich': 916, 'ridiculous': 917, 'robbed': 918, 'romantic': 919, 'room': 920, 'rose': 921, 'rule': 922, 'run': 923, 'said': 924, 'same': 925, 'satisfaction': 926, 'savour?': 927, 'saw': 928, 'say': 929, 'saying': 930, 'says': 931, 'scorn': 932, 'scornful': 933, 'secret': 934, 'secret?': 935, 'see': 936, 'seemed': 937, 'seen': 938, 'self-confident': 939, 'send': 940, 'sensation': 941, 'sensitive': 942, 'sent': 943, 'serious': 944, 'set': 945, 'sex': 946, 'shade': 947, 'shaking': 948, 'shall': 949, 'she': 950, \"she's\": 951, 'shirked': 952, 'short': 953, 'should': 954, 'shoulder': 955, 'shoulders': 956, 'show': 957, 'showed': 958, 'showy': 959, 'shrug': 960, 'shrugged': 961, 'sight': 962, 'sign': 963, 'silent': 964, 'silver': 965, 'similar': 966, 'simpleton': 967, 'simplifications': 968, 'simply': 969, 'since': 970, 'single': 971, 'sitter': 972, 'sitters': 973, 'sketch': 974, 'skill': 975, 'slight': 976, 'slightly': 977, 'slowly': 978, 'small': 979, 'smile': 980, 'smiling': 981, 'sneer': 982, 'so': 983, 'solace': 984, 'some': 985, 'somebody': 986, 'something': 987, 'spacious': 988, 'spaniel': 989, 'speaking-tubes': 990, 'speculations': 991, 'spite': 992, 'splash': 993, 'square': 994, 'stairs': 995, 'stammer': 996, 'stand': 997, 'standing': 998, 'started': 999, 'stay': 1000, 'still': 1001, 'stocked': 1002, 'stood': 1003, 'stopped': 1004, 'stopping': 1005, 'straddling': 1006, 'straight': 1007, 'strain': 1008, 'straining': 1009, 'strange': 1010, 'straw': 1011, 'stream': 1012, 'stroke': 1013, 'strokes': 1014, 'strolled': 1015, 'struck': 1016, 'studio': 1017, 'stuff': 1018, 'subject': 1019, 'substantial': 1020, 'suburban': 1021, 'such': 1022, 'suddenly': 1023, 'suffered': 1024, 'sugar': 1025, 'suggested': 1026, 'sunburn': 1027, 'sunburnt': 1028, 'sunlit': 1029, 'superb': 1030, 'sure': 1031, 'surest': 1032, 'surface': 1033, 'surprise': 1034, 'surprised': 1035, 'surrounded': 1036, 'suspected': 1037, 'sweetness': 1038, 'swelling': 1039, 'swept': 1040, 'swum': 1041, 'table': 1042, 'take': 1043, 'taken': 1044, 'talking': 1045, 'tea': 1046, 'tears': 1047, 'technicalities': 1048, 'tell': 1049, 'tells': 1050, 'tempting': 1051, 'terra-cotta': 1052, 'terrace': 1053, 'terraces': 1054, 'terribly': 1055, 'than': 1056, 'that': 1057, 'the': 1058, 'their': 1059, 'them': 1060, 'then': 1061, 'there': 1062, \"there's\": 1063, 'therefore': 1064, 'they': 1065, \"they're\": 1066, 'thin': 1067, 'thing': 1068, 'things': 1069, 'things?': 1070, 'think': 1071, 'this': 1072, 'thither': 1073, 'those': 1074, 'though': 1075, 'thought': 1076, 'three': 1077, 'threshold': 1078, 'threw': 1079, 'through': 1080, 'throwing': 1081, 'tie': 1082, 'till': 1083, 'time': 1084, 'timorously': 1085, 'tinge': 1086, 'tips': 1087, 'tired': 1088, 'to': 1089, 'told': 1090, 'tone': 1091, 'tones': 1092, 'too': 1093, 'too?': 1094, 'took': 1095, 'tottering': 1096, 'touched': 1097, 'toward': 1098, 'trace': 1099, 'trade': 1100, 'transmute': 1101, 'traps': 1102, 'travelled': 1103, 'tribute': 1104, 'tributes': 1105, 'tricks': 1106, 'tried': 1107, 'trouser-presses': 1108, 'true': 1109, 'truth': 1110, 'turned': 1111, 'twenty': 1112, 'twenty-four': 1113, 'twice': 1114, 'twirling': 1115, 'unaccountable': 1116, 'uncertain': 1117, 'under': 1118, 'underlay': 1119, 'underneath': 1120, 'understand': 1121, 'unexpected': 1122, 'untouched': 1123, 'unusual': 1124, 'up': 1125, 'up\"': 1126, 'up-stream': 1127, 'upon': 1128, 'upset': 1129, 'upstairs': 1130, 'us': 1131, 'used': 1132, 'usual': 1133, 'value': 1134, 'varnishing': 1135, 'vases': 1136, 'veins': 1137, 'velveteen': 1138, 'very': 1139, 'villa': 1140, 'vindicated': 1141, 'virtuosity': 1142, 'vista': 1143, 'vocation': 1144, 'voice': 1145, 'wall': 1146, 'wander': 1147, 'want': 1148, 'wanted': 1149, 'wants': 1150, 'was': 1151, \"wasn't\": 1152, 'watched': 1153, 'watching': 1154, 'water-colour': 1155, 'waves': 1156, 'way': 1157, 'weekly': 1158, 'weeks': 1159, \"weeks'\": 1160, 'welcome': 1161, 'went': 1162, 'were': 1163, 'what': 1164, 'when': 1165, 'whenever': 1166, 'where': 1167, 'which': 1168, 'while': 1169, 'white': 1170, 'white-panelled': 1171, 'who': 1172, 'whole': 1173, 'whom': 1174, 'why': 1175, 'wide': 1176, 'widow': 1177, 'wife': 1178, \"wife's\": 1179, 'wild': 1180, 'wincing': 1181, 'window-curtains': 1182, 'wish': 1183, 'with': 1184, 'without': 1185, \"wits'\": 1186, 'woman': 1187, 'women': 1188, \"won't\": 1189, 'wonder': 1190, 'wondered': 1191, 'word': 1192, 'work': 1193, 'working?': 1194, 'worth': 1195, 'would': 1196, \"wouldn't\": 1197, 'year': 1198, 'years': 1199, 'yellow': 1200, 'yet': 1201, 'you': 1202, \"you'd\": 1203, \"you're\": 1204, 'you?\"': 1205, 'younger': 1206, 'your': 1207, 'yourself': 1208}\n"
     ]
    }
   ],
   "source": [
    "token_id = {token:i for i,token in enumerate(words)}\n",
    "print(token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e367cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tik version =  0.12.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tik version = \", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4c8d85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# Byte Pair Encoding ---------- BPE is done using tiktoken library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74482610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[28254, 5439, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 19606, 8812, 2114, 1659, 617, 20035, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\"Helllo, do you like tea? <|endoftext|> In the sunlight terraces\"\n",
    "        \"of someUnknownPlace.\")\n",
    "\n",
    "interger = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(interger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04549de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helllo, do you like tea? <|endoftext|> In the sunlight terracesof someUnknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(interger)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9173daf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input-target pair\n",
    "with open(\"the-verdict.txt\" , 'r') as file:\n",
    "    data = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee031f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(data)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2362e0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I ------->  H\n",
      "I H -------> AD\n",
      "I HAD ------->  always\n",
      "I HAD always ------->  thought\n",
      "I HAD always thought ------->  Jack\n",
      "I HAD always thought Jack ------->  G\n",
      "I HAD always thought Jack G -------> is\n",
      "I HAD always thought Jack Gis -------> burn\n",
      "I HAD always thought Jack Gisburn ------->  rather\n",
      "I HAD always thought Jack Gisburn rather ------->  a\n",
      "I HAD always thought Jack Gisburn rather a ------->  cheap\n",
      "I HAD always thought Jack Gisburn rather a cheap ------->  genius\n",
      "I HAD always thought Jack Gisburn rather a cheap genius -------> --\n",
      "I HAD always thought Jack Gisburn rather a cheap genius-- -------> though\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though ------->  a\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a ------->  good\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good ------->  fellow\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow ------->  enough\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough -------> --\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough-- -------> so\n"
     ]
    }
   ],
   "source": [
    "sample_context = 20\n",
    "for i in range(1, sample_context+1):\n",
    "    context = enc_text[:i]\n",
    "    desire = enc_text[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"------->\", tokenizer.decode([desire]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14950503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "# Creating a input output pair\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_len, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "    # Using sliding window to chuck block into overlapping sequence of max_len \n",
    "        for i in range (0, len(token_ids)-max_len, stride):\n",
    "            input_chuck = token_ids[i : i + max_len]\n",
    "            target_chuck = token_ids[i + 1 : i + max_len + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chuck))\n",
    "            self.target_ids.append(torch.tensor(target_chuck))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "    \n",
    "\n",
    "def create_dataloader(txt, batch_size = 4, max_len = 256, stride = 128, shuffle = True, drop_last = True, num_workers = 0):\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_len, stride) #create dataset\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle = shuffle, drop_last = drop_last, num_workers = num_workers)\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "#with open(\"the-verdict.txt\" , 'r') as file:\n",
    "#    data = file.read()      Run this to get the data\n",
    "\n",
    "\n",
    "dataloader = create_dataloader(data, batch_size=1, max_len=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)\n",
    "\n",
    "# [tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])] ----->> it gives input_tensor and output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20d1da52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatiing Token Embedding\n",
    "\n",
    "input_ids = torch.tensor([2,3,5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "65d694ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  1.3337,  0.0771, -0.0522],\n",
      "        [ 0.2386,  0.1411, -1.3354,  ..., -0.0315, -1.0640,  0.9417],\n",
      "        [-1.3152, -0.0677, -0.1350,  ..., -0.3181, -1.3936,  0.5226],\n",
      "        ...,\n",
      "        [ 0.5871, -0.0572, -1.1628,  ..., -0.6887, -0.7364,  0.4479],\n",
      "        [ 0.4438,  0.7411,  1.1263,  ...,  1.2091,  0.6781,  0.3331],\n",
      "        [-0.2537,  0.1446,  0.7203,  ..., -0.2134,  0.2144,  0.3006]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "torch.manual_seed(123)\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim) # Here number of rows is equal to vocab_size\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb8a61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Id : \n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "Input Shape :  torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "max_len = 4\n",
    "dataloader = create_dataloader(data, batch_size=8, max_len=max_len, stride=max_len, shuffle=False)\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "\n",
    "print(\"Token Id : \\n\", inputs)\n",
    "print(\"Input Shape : \", inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d571b43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "token_embedding = embedding_layer(inputs)\n",
    "print(token_embedding.shape) # This will give 3 dimensional matrix which formed by embedding vector for all the 8*4 so output is ------>> 8*4*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f40a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256])\n"
     ]
    }
   ],
   "source": [
    "# Create another embedding layer for positional encoding\n",
    "# Here the number of row is context_length\n",
    "context_length = max_len\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim) # This creates the embedding layer for positional  encoding\n",
    "\n",
    "pos_embedding  = pos_embedding_layer(torch.arange(max_len))\n",
    "print(pos_embedding.shape)\n",
    "\n",
    "# This encodes the data to positional encoding to form size of 4*256\n",
    "# Why we need to encode to get 4*256? ----  Because we are encoding positional encode and we want to know which position does input present\n",
    "# So we get 4*256 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6a3983eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "input_embedding = token_embedding + pos_embedding\n",
    "print(input_embedding.shape)\n",
    "\n",
    "# As total embedding of positional embedding is addidtion of token embedding and unquie positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "07b2ca98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing simple attention mechanism\n",
    "import torch\n",
    "input_sam = torch.tensor(\n",
    "    [[0.43, 0.15, 0.89],   #your   (x^1)\n",
    "    [0.55, 0.87, 0.66],    #journey (x^2)\n",
    "    [0.57, 0.85, 0.64],    #starts  (x^3)\n",
    "    [0.22, 0.58, 0.33],    #with  (x^4)\n",
    "    [0.77, 0.25, 0.10],    #one (x^5)\n",
    "    [0.05, 0.80, 0.55]])   #step (x^6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9967a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # corresponding words\n",
    "\n",
    "# words_sam = ['Your','journey', 'starts', 'with', 'one', 'step']\n",
    "\n",
    "# # Extract x, y, z coordinates\n",
    "# x_coords = input_sam[: , 0].numpy()\n",
    "# y_coords = input_sam[: , 1].numpy()\n",
    "# z_coords = input_sam[: , 2].numpy()\n",
    "\n",
    "# # creating 3D plot\n",
    "# fig = plt.Figure()\n",
    "# ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# for x,y,z,word in zip(x_coords,y_coords, z_coords, words_sam):\n",
    "#     ax.scatter(x,y,z)\n",
    "#     ax.text(x,y,z,word, fontsize = 10)\n",
    "\n",
    "# ax.set_xlabel(\"x\")\n",
    "# ax.set_ylabel(\"y\")\n",
    "# ax.set_zlabel(\"z\")\n",
    "\n",
    "# plt.title(\"3D plot of word Embedding\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a31659a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "# Attention Score with only 2nd element\n",
    "query = input_sam[1]\n",
    "attn_score_2 = torch.empty(input_sam.shape[0])\n",
    "for i,x_i in enumerate(input_sam):\n",
    "    attn_score_2[i] = torch.dot(x_i,query) # dot product (transpose not necessary)\n",
    "\n",
    "print(attn_score_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "90012c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights :  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum :  tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "# normalization of attention score\n",
    "\n",
    "attn_weight_2_tmp = attn_score_2/attn_score_2.sum()\n",
    "\n",
    "print(\"Attention weights : \", attn_weight_2_tmp)\n",
    "print(\"Sum : \", attn_weight_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "23bc629e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights :  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Using Navie Softmax for normalization\n",
    "\n",
    "def softmax_naive(x):\n",
    "    return torch.exp(x)/torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weight_2_naive = softmax_naive(attn_score_2)\n",
    "\n",
    "print(\"Attention weights : \", attn_weight_2_naive)\n",
    "print(\"Sum : \", attn_weight_2_naive.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c632acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights :  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum :  tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Pytorch implementation of softmax\n",
    "\n",
    "attn_weight_2 = torch.softmax(attn_score_2, dim=0) # Attention weights\n",
    "print(\"Attention weights : \", attn_weight_2)\n",
    "print(\"Sum : \", attn_weight_2.sum())\n",
    "\n",
    "# Naive softmax may encounter numerical instability problems such as overflow and underflow\n",
    "# It is always advisable to use Pytorch softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d19f55e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "# context vector implementation\n",
    "query = input_sam[1]\n",
    "\n",
    "context_vector = torch.zeros(query.shape)\n",
    "\n",
    "for i, x_i in enumerate(input_sam):\n",
    "    context_vector += attn_weight_2[i]*x_i\n",
    "\n",
    "print(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "# Efficient way for attention score\n",
    "\n",
    "attn_score = input_sam @ input_sam.T # It does dot product by multiplying input*input(transpose)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "attn_weight = torch.softmax(attn_score, dim=-1) # Attention weight for all the elements\n",
    "print(attn_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "all_context_vectors = attn_weight @ input_sam # All elements context vectors\n",
    "print(all_context_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02407b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[0.2961, 0.5166],\n",
      "        [0.2517, 0.6886],\n",
      "        [0.0740, 0.8665]]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.1366, 0.1025],\n",
      "        [0.1841, 0.7264],\n",
      "        [0.3153, 0.6871]]) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.0756, 0.1966],\n",
      "        [0.3164, 0.4017],\n",
      "        [0.1186, 0.8274]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# self attention mechanism with query key and value\n",
    "\n",
    "x_2 = input_sam[1]\n",
    "d_in = input_sam.shape[1]\n",
    "d_out = 2\n",
    "\n",
    "torch.manual_seed(123)\n",
    "w_query = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "w_key = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "w_value = torch.nn.Parameter(torch.rand(d_in,d_out), requires_grad=False)\n",
    "\n",
    "print(w_query,\"\\n\")\n",
    "print(w_key,\"\\n\")\n",
    "print(w_value,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c743e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551]) \n",
      "\n",
      "tensor([0.4433, 1.1419]) \n",
      "\n",
      "tensor([0.3951, 1.0037]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Key Value for input_sam[1]\n",
    "\n",
    "query_2 = x_2 @ w_query\n",
    "key_2 = x_2 @ w_key\n",
    "value_2 = x_2 @ w_value\n",
    "\n",
    "print(query_2,\"\\n\")\n",
    "print(key_2,\"\\n\")\n",
    "print(value_2,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2401725a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2309, 1.0966],\n",
      "        [0.4306, 1.4551],\n",
      "        [0.4300, 1.4343],\n",
      "        [0.2355, 0.7990],\n",
      "        [0.2983, 0.6565],\n",
      "        [0.2568, 1.0533]]) \n",
      "\n",
      "tensor([[0.3669, 0.7646],\n",
      "        [0.4433, 1.1419],\n",
      "        [0.4361, 1.1156],\n",
      "        [0.2408, 0.6706],\n",
      "        [0.1827, 0.3292],\n",
      "        [0.3275, 0.9642]]) \n",
      "\n",
      "tensor([[0.1855, 0.8812],\n",
      "        [0.3951, 1.0037],\n",
      "        [0.3879, 0.9831],\n",
      "        [0.2393, 0.5493],\n",
      "        [0.1492, 0.3346],\n",
      "        [0.3221, 0.7863]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Query Key Value using input_sam\n",
    "\n",
    "query= input_sam @ w_query\n",
    "key = input_sam @ w_key\n",
    "value = input_sam @ w_value\n",
    "\n",
    "print(query,\"\\n\")\n",
    "print(key,\"\\n\")\n",
    "print(value,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7869964e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention score 22 \n",
      " tensor(1.8524) \n",
      "\n",
      "Attention score 2 \n",
      " tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440]) \n",
      "\n",
      "Attention score \n",
      " tensor([[0.9231, 1.3545, 1.3241, 0.7910, 0.4032, 1.1330],\n",
      "        [1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440],\n",
      "        [1.2544, 1.8284, 1.7877, 1.0654, 0.5508, 1.5238],\n",
      "        [0.6973, 1.0167, 0.9941, 0.5925, 0.3061, 0.8475],\n",
      "        [0.6114, 0.8819, 0.8626, 0.5121, 0.2707, 0.7307],\n",
      "        [0.8995, 1.3165, 1.2871, 0.7682, 0.3937, 1.0996]])\n"
     ]
    }
   ],
   "source": [
    "key_2 = key[1]\n",
    "attn_score_22 = query_2.dot(key_2)\n",
    "print(\"Attention score 22 \\n\",attn_score_22,\"\\n\")\n",
    "\n",
    "attn_score_2 = query_2 @ key.T # All attention score for given query   Here query is input_sam[2]\n",
    "print(\"Attention score 2 \\n\",attn_score_2,\"\\n\")\n",
    "\n",
    "attn_score = query @ key.T # omega\n",
    "print(\"Attention score \\n\", attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46cd736d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d_k = key.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_score_2/ d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n",
    "print(d_k) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d03f288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3069, 0.8188])\n"
     ]
    }
   ],
   "source": [
    "# sample context vector with weights, values and keys\n",
    "context_vec_2 = attn_weight_2 @ value \n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bd3290d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Modular self attention implementation for full input\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.w_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_key = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.w_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self,x):\n",
    "        key = x @ self.w_key    \n",
    "        query = x @ self.w_query\n",
    "        value = x @ self.w_value\n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ value\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)       # This is the context vector of input_sam\n",
    "print(sa_v1(input_sam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9abbd2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced version of self attention \n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self, d_in, d_out, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        attn_scores = query @ key.T\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ value\n",
    "        return context_vec\n",
    "    \n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)       # This is the context vector of input_sam in an enhanced way\n",
    "print(sa_v2(input_sam)) \n",
    "\n",
    "# version 1 and version 2 are completely different because they both use different initialization\n",
    "# version 1 ---> nn.Parameter        version 2 ---> nn.Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c0eec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = sa_v2.w_query(input_sam)\n",
    "keys = sa_v2.w_key(input_sam)\n",
    "attention_score = queries @ keys.T\n",
    "attention_weights = torch.softmax(attention_score/keys.shape[-1]**0.5, dim=1)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5cb7907d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]]) \n",
      "\n",
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "context_length = attention_score.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(context_length, context_length)) # tril gives lower triangular matrix    triu gives upper triangular matrix\n",
    "print(mask_simple,\"\\n\")\n",
    "\n",
    "masked_simple = attention_weights * mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66900df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# normalizing masked_simple to make the sum of rows is equal to 1\n",
    "row_sum = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_normalize = masked_simple/row_sum\n",
    "print(masked_simple_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846e1349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Efficiently Masked \n",
      " tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>) \n",
      "\n",
      "Efficiennt attention weight after masked \n",
      " tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Efficient masking\n",
    "mask = torch.triu(torch.ones(context_length, context_length),diagonal=1)\n",
    "masked = attention_score.masked_fill(mask.bool(), -torch.inf)\n",
    "print(\"Efficiently Masked \\n\",masked, \"\\n\")  # It gives masked matrix\n",
    "\n",
    "attention_weights_eff = torch.softmax(masked/key.shape[-1]**0.5,dim=1)\n",
    "print(\"Efficiennt attention weight after masked \\n\", attention_weights_eff) # Softmax after masked will give us normalized attention weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92286f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Dropout for reducing thw lazy neurons \n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5)\n",
    "# example = torch.ones(6,6)  example\n",
    "# print(dropout(example))\n",
    "print(dropout(attention_weights_eff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3476be59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of context vector for causal attention \n",
      " torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "# Implementation of causal mechanism \n",
    "\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self,d_in, d_out, context_length, dropout, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        attn_scores = query @ key.transpose(1,2)\n",
    "        attn_score.masked_fill(self.mask.bool() [:num_token, :num_token], -torch.inf)\n",
    "        attn_weights = torch.softmax(attn_scores / key.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        context_vector_causal = attn_weights @ value\n",
    "        return context_vector_causal\n",
    "\n",
    "batch = torch.stack((input_sam, input_sam), dim=0) # shape of batch is [2,6,3]\n",
    "torch.manual_seed(123)\n",
    "context_length = batch.shape[1]\n",
    "causalAttention = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "context_vec_causal = causalAttention(batch)\n",
    "print(\"Shape of context vector for causal attention \\n\", context_vec_causal.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73eb9fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector of Multi head Attention \n",
      " tensor([[[-0.5337, -0.1051,  0.5085,  0.3508],\n",
      "         [-0.5323, -0.1080,  0.5084,  0.3508],\n",
      "         [-0.5323, -0.1079,  0.5084,  0.3506],\n",
      "         [-0.5297, -0.1076,  0.5074,  0.3471],\n",
      "         [-0.5311, -0.1066,  0.5076,  0.3446],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.5337, -0.1051,  0.5085,  0.3508],\n",
      "         [-0.5323, -0.1080,  0.5084,  0.3508],\n",
      "         [-0.5323, -0.1079,  0.5084,  0.3506],\n",
      "         [-0.5297, -0.1076,  0.5074,  0.3471],\n",
      "         [-0.5311, -0.1066,  0.5076,  0.3446],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>) \n",
      "\n",
      "torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "# Implementation Multi head attentioon mechanism\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_head, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in,d_out,context_length,dropout,qkv_bias)\n",
    "             for _ in range (num_head)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "batch = torch.stack((input_sam, input_sam), dim=0) \n",
    "context_length = batch.shape[1]\n",
    "d_in, d_out = 3, 2\n",
    "multihead = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_head=2)\n",
    "context_vec_multihead = multihead(batch)\n",
    "print(\"Context Vector of Multi head Attention \\n\", context_vec_multihead, \"\\n\")\n",
    "print(context_vec_multihead.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "380f7966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Vector for Multihead Attention \n",
      " tensor([[[ 0.1195, -0.0484,  0.0306, -0.0639, -0.2782, -0.2564],\n",
      "         [ 0.1208, -0.0497,  0.0319, -0.0638, -0.2779, -0.2566],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]],\n",
      "\n",
      "        [[ 0.1195, -0.0484,  0.0306, -0.0639, -0.2782, -0.2564],\n",
      "         [ 0.1208, -0.0497,  0.0319, -0.0638, -0.2779, -0.2566],\n",
      "         [ 0.1196, -0.0491,  0.0318, -0.0635, -0.2788, -0.2578]]],\n",
      "       grad_fn=<ViewBackward0>) \n",
      "\n",
      "Context Vector shape \n",
      " torch.Size([2, 3, 6]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Implementation of Multi head attention mechanism with weight split\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_head, qkv_bias = False):\n",
    "        super().__init__()\n",
    "        assert(d_out % num_head == 0),\\\n",
    "            \"d_out must be divisible by num_head\"\n",
    "        self.d_out = d_out\n",
    "        self.num_head = num_head\n",
    "        self.head_dim = d_out//num_head # Reduce the production dim to match the desire output dim\n",
    "        self.w_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.w_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_in,d_out) # linear projection\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length),diagonal=1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, num_token, d_in = x.shape\n",
    "        key =  self.w_key(x)    \n",
    "        query = self.w_query(x)\n",
    "        value = self.w_value(x) \n",
    "\n",
    "        # We implicity split the matrix by adding a num_head dimension \n",
    "        # unroll last dim : (b, num_token, d_out) -> (b, num_token, num_head, head_dim)\n",
    "        key = key.view(b, num_token, self.num_head, self.head_dim)\n",
    "        value = value.view(b, num_token, self.num_head, self.head_dim)\n",
    "        query = query.view(b, num_token, self.num_head, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_token, num_head, head_dim) -> (b, num_head, num_token, head_dim)\n",
    "        key = key.transpose(1,2)\n",
    "        query = query.transpose(1,2)\n",
    "        value = value.transpose(1,2)\n",
    "\n",
    "        # Compute scaled dot product attention aka self attention with causal mask\n",
    "        attention_score = query @ key.transpose(2,3)\n",
    "\n",
    "        # original mask truncated to the number of tokens and convert to boolean \n",
    "        mask_bool = self.mask.bool()[:num_token, :num_token]\n",
    "\n",
    "        attention_score.masked_fill(mask_bool, -torch.inf) # masking the attention score\n",
    "\n",
    "        attention_weight = torch.softmax(attention_score/key.shape[-1]**0.5, dim = -1)\n",
    "        attention_weight = self.dropout(attention_weight)\n",
    "\n",
    "        context_vector_multihead = (attention_weight @ value).transpose(1,2)\n",
    "\n",
    "        # Combine head, where self.d_out = self.num_head * self.head_dim\n",
    "        context_vector_multihead = context_vector_multihead.contiguous().view(b, num_token, self.d_out)\n",
    "        context_vector_multihead = self.out_proj(context_vector_multihead) # Optional projection\n",
    "\n",
    "        return context_vector_multihead\n",
    "    \n",
    "torch.manual_seed(123)\n",
    "\n",
    "inputs = torch.tensor(\n",
    "    [\n",
    "    [0.43, 0.15, 0.89, 0.55, 0.87, 0.66],    \n",
    "    [0.57, 0.85, 0.64, 0.22, 0.58, 0.33],    \n",
    "    [0.77, 0.25, 0.10, 0.05, 0.80, 0.55]\n",
    "    ])\n",
    "batch = torch.stack((inputs, inputs), dim=0)\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 6\n",
    "multiheadAttentionComplete = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_head=2)\n",
    "context_vector = multiheadAttentionComplete(batch)\n",
    "print(\"Context Vector for Multihead Attention \\n\", context_vector,\"\\n\")\n",
    "print(\"Context Vector shape \\n\", context_vector.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0762078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90937982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79925f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd46a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf6cc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e40ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622dbf70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f15e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6443ff98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
